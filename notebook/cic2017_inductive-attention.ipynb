{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e31772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv, ResGatedGraphConv, GATv2Conv, SAGEConv, GENConv, DeepGCNLayer, PairNorm, GINConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch.nn.functional as F\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle\n",
    "from torch.nn import LayerNorm, Linear, ReLU\n",
    "from torch_scatter import scatter\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import RandomNodeSampler\n",
    "import math\n",
    "import copy\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33f2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# action参数可以设置为ignore，一位一次也不喜爱你是，once表示为只显示一次\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d10df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2acf4d40cf90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2022)\n",
    "torch.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6f1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition_layer(torch.nn.Module):\n",
    "    def __init__(self, act, norm, lin):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.lin = lin\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "        \n",
    "class DenseGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_class, num_layers=6, num_blocks=3, growth_rate=10, theta=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.blocks = num_blocks\n",
    "        self.theta = theta\n",
    "        self.growth_rate = growth_rate\n",
    "        self.base_rate = growth_rate\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = num_class\n",
    "        self.linear_layers = torch.nn.ModuleList()\n",
    "        self.transition_layers = torch.nn.ModuleList()\n",
    "        self.block_layers = torch.nn.ModuleList()\n",
    "        self.node_encoder = Linear(in_channels, in_channels)\n",
    "        self.linear_layers.append(self.node_encoder)\n",
    "        \n",
    "        for i in range(self.blocks):\n",
    "            # block\n",
    "            layers = torch.nn.ModuleList()\n",
    "            # 2^(i - 1) * k0\n",
    "            self.growth_rate = int(math.pow(2, i) * self.base_rate)\n",
    "            print(self.growth_rate)\n",
    "            for j in range(1, self.num_layers + 1):\n",
    "                conv = GATConv(in_channels + (j - 1) * self.growth_rate, self.growth_rate, aggr='max') \n",
    "                norm = LayerNorm(self.growth_rate)\n",
    "                act = ReLU()\n",
    "                layer = DeepGCNLayer(conv, norm, act, block='dense')\n",
    "                layers.append(layer)\n",
    "            self.block_layers.append(layers)\n",
    "            \n",
    "            # transition\n",
    "            hidden_channels = in_channels +  self.num_layers * self.growth_rate\n",
    "            out_channels = int(hidden_channels * self.theta)\n",
    "            transition_norm = LayerNorm(hidden_channels, elementwise_affine=True)\n",
    "            transition_act = ReLU()\n",
    "            transition_lin = Linear(hidden_channels, out_channels)\n",
    "            transitionLayer = Transition_layer(transition_act, transition_norm, transition_lin)\n",
    "            self.transition_layers.append(transitionLayer)\n",
    "            in_channels = copy.copy(out_channels)\n",
    "        \n",
    "        self.lin_last = Linear(in_channels, self.out_channels)\n",
    "        self.linear_layers.append(self.lin_last)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.blocks):\n",
    "            # block layer\n",
    "            for layer in self.block_layers[i]:\n",
    "                x = layer(x, edge_index)\n",
    "            # transition layer\n",
    "            x = self.transition_layers[i](x)\n",
    "        x = self.linear_layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb53e0e6",
   "metadata": {
    "code_folding": [
     21
    ]
   },
   "outputs": [],
   "source": [
    "class DeepGCNLayer(torch.nn.Module):\n",
    "    def __init__(self, conv=None, norm=None, act=None, block='res+',\n",
    "                 dropout=0., ckpt_grad=False):\n",
    "        super().__init__()\n",
    "        self.conv = conv\n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        self.block = block.lower()\n",
    "        assert self.block in ['res+', 'res', 'dense', 'plain']\n",
    "        self.dropout = dropout\n",
    "        self.ckpt_grad = ckpt_grad\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "        self.norm.reset_parameters()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"\"\"\"\n",
    "        args = list(args)\n",
    "        x = args.pop(0)\n",
    "        h2 = self.conv(x, *args, **kwargs)\n",
    "        if self.norm is not None:\n",
    "            h2 = self.norm(h2)\n",
    "        if self.act is not None:\n",
    "            h2 = self.act(h2)\n",
    "        h = torch.cat([x, h2], dim=-1)\n",
    "        return F.dropout(h, p=self.dropout, training=self.training), h2 # h是Dense连接后的,h2是聚合输出\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(block={self.block})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81679e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADCGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_class, num_layers=6, num_blocks=3, growth_rate=10, theta=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.blocks = num_blocks\n",
    "        self.theta = theta\n",
    "        self.growth_rate = growth_rate\n",
    "        self.base_rate = growth_rate\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = num_class\n",
    "        self.linear_layers = torch.nn.ModuleList()\n",
    "        self.transition_layers = torch.nn.ModuleList()\n",
    "        self.block_layers = torch.nn.ModuleList()\n",
    "        self.node_encoder = Linear(in_channels, in_channels)\n",
    "        self.linear_layers.append(self.node_encoder)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.proj = Linear(growth_rate, 1)\n",
    "        for j in range(1, self.num_layers + 1):\n",
    "            conv = GATConv(in_channels + (j - 1) * self.growth_rate, self.growth_rate) \n",
    "            norm = LayerNorm(self.growth_rate)\n",
    "            act = ReLU()\n",
    "            layer = DeepGCNLayer(conv, norm, act, block='dense')\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        self.lin_last = Linear(self.growth_rate , self.out_channels)\n",
    "        self.linear_layers.append(self.lin_last)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        pre = []\n",
    "        for layer in self.layers:\n",
    "            x, out = layer(x, edge_index)\n",
    "            pre.append(out)\n",
    "        preds = torch.stack(pre, 1)\n",
    "        m = torch.tanh(preds)\n",
    "        print(m.size())\n",
    "        retain_score = self.proj(m)\n",
    "        print(retain_score.size())\n",
    "        retain_score = torch.softmax(retain_score, 1)\n",
    "        print(retain_score.size())\n",
    "        out = preds * retain_score\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out = self.linear_layers[-1](out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e481d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_class, hidden):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.conv1 = GATConv(in_channels, hidden) \n",
    "        self.conv2 = GATConv(hidden, num_class) \n",
    "        self.act = ReLU()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "24142276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_class, hidden, k):\n",
    "        super().__init__()\n",
    "        self.layer_num = k\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GATConv(in_channels, hidden))\n",
    "        for i in range(k-2):\n",
    "            self.layers.append(GATConv(hidden, hidden))\n",
    "        self.layers.append(GATConv(hidden,num_class))\n",
    "        self.act = ReLU()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = self.act(x)\n",
    "            x = layer(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "42ed76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_loss = total_correct = total_examples = 0\n",
    "    start_time = time.time()\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        y = batch.y[:batch.batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(batch.x.to(device), batch.edge_index.to(device))[:batch.batch_size]\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler1.step()\n",
    "        total_loss += float(loss) * batch.batch_size\n",
    "        total_correct += int((y_hat.argmax(dim=-1) == y).sum())\n",
    "        total_examples += batch.batch_size\n",
    "    end_time = time.time()  \n",
    "    \n",
    "    return total_loss / total_examples, total_correct / total_examples\n",
    "\n",
    "def inferrence(model, subgraph_loader):\n",
    "    total_loss = total_correct = total_examples = 0\n",
    "    xs = []\n",
    "    y = []\n",
    "    pbar = tqdm(total=len(subgraph_loader))\n",
    "    for batch in subgraph_loader:\n",
    "        y_hat = model(batch.x, batch.edge_index.to(device))[:batch.batch_size]\n",
    "        xs.append(y_hat.cpu())\n",
    "        y_true = batch.y[:batch.batch_size]\n",
    "        y.append(y_true.cpu())\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    y_hat = torch.cat(xs, 0)\n",
    "    y_hat = y_hat.detach().numpy()\n",
    "    y_hat = np.argmax(y_hat, -1)\n",
    "    y = torch.cat(y, 0)\n",
    "    print(y_hat)\n",
    "    print(y)\n",
    "    cr1 = classification_report(y, y_hat,digits=4)\n",
    "    cf = confusion_matrix(y, y_hat)\n",
    "    print(cr1)\n",
    "    print(cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "94a6bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('cic2017_directed_line_graph_train_data_15label2')\n",
    "test_data = torch.load('cic2017_directed_line_graph_test_data_15label2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518a891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff1ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('cic2017_dgl_to_pyg_train_data50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0c35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.load('cic2017_dgl_to_pyg_test_data50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bea003cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = torch.load('cic2017_dgl_to_pyg_train_data50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5781a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = torch.load('cic2017_dgl_to_pyg_test_data50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8c79afac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[424589, 78], edge_index=[2, 156754252], y=[424589], num_nodes=424589),\n",
       " Data(x=[181967, 78], edge_index=[2, 31672070], y=[181967], num_nodes=181967))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3314cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二分类试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d4a654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data.y.numpy()\n",
    "test_label = test_data.y.numpy()\n",
    "train_label[train_label != 0] = 1\n",
    "test_label[test_label != 0] = 1\n",
    "train_data.y = torch.LongTensor(train_label)\n",
    "test_data.y = torch.LongTensor(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d18106aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_nodes = torch.arange(len(train_data.y))\n",
    "test_num_nodes = torch.arange(len(test_data.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7f124994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to('cpu')\n",
    "test_data = test_data.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5b31d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num_per_k = 20\n",
    "depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "52281e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dff9dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop = [node_num_per_k] * depth\n",
    "train_loader = NeighborLoader(train_data, input_nodes = train_num_nodes, num_neighbors=hop, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "03e328ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1651, 78], edge_index=[2, 664], y=[1651], num_nodes=1651, batch_size=1024)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "be2f9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_loader = NeighborLoader(test_data, input_nodes=test_num_nodes,num_neighbors=hop, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48a9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "09810cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = train_data.x.size(-1)\n",
    "num_class = 15\n",
    "model = DenseGAT(in_channels=in_channels, num_class=num_class, \n",
    "                 num_layers=depth, num_blocks=1, growth_rate=growth_rate, theta=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "243eebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OriginalGAT(in_channels, num_class, hidden=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e83942d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "94216a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a2198afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerGAT(\n",
       "  (layers): ModuleList()\n",
       "  (conv1): GATConv(78, 10, heads=1)\n",
       "  (conv2): GATConv(10, 10, heads=1)\n",
       "  (conv3): GATConv(10, 15, heads=1)\n",
       "  (act): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "32c5f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerGAT(in_channels, num_class, hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ed5fe484",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e6d33d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ea1d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 13 10:38:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    25W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    27W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    25W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f820078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1e5a4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001 loss_train: 2.3923 acc_train: 0.2854 time: 3.6652s\n",
      "epoch: 0002 loss_train: 1.7008 acc_train: 0.4786 time: 3.5770s\n",
      "epoch: 0003 loss_train: 1.2416 acc_train: 0.5736 time: 3.6401s\n",
      "epoch: 0004 loss_train: 0.9177 acc_train: 0.7083 time: 3.5382s\n",
      "epoch: 0005 loss_train: 0.7078 acc_train: 0.7710 time: 3.5872s\n",
      "epoch: 0006 loss_train: 0.5729 acc_train: 0.8350 time: 3.7166s\n",
      "epoch: 0007 loss_train: 0.4790 acc_train: 0.8982 time: 3.5740s\n",
      "epoch: 0008 loss_train: 0.4118 acc_train: 0.9114 time: 3.6365s\n",
      "epoch: 0009 loss_train: 0.3630 acc_train: 0.9169 time: 3.6028s\n",
      "epoch: 0010 loss_train: 0.3265 acc_train: 0.9199 time: 3.7190s\n",
      "epoch: 0011 loss_train: 0.2990 acc_train: 0.9217 time: 3.6162s\n",
      "epoch: 0012 loss_train: 0.2774 acc_train: 0.9232 time: 3.7302s\n",
      "epoch: 0013 loss_train: 0.2593 acc_train: 0.9246 time: 3.6012s\n",
      "epoch: 0014 loss_train: 0.2428 acc_train: 0.9261 time: 3.6647s\n",
      "epoch: 0015 loss_train: 0.2294 acc_train: 0.9278 time: 3.6334s\n",
      "epoch: 0016 loss_train: 0.2177 acc_train: 0.9297 time: 3.6690s\n",
      "epoch: 0017 loss_train: 0.2067 acc_train: 0.9360 time: 3.5864s\n",
      "epoch: 0018 loss_train: 0.1972 acc_train: 0.9418 time: 3.6537s\n",
      "epoch: 0019 loss_train: 0.1875 acc_train: 0.9433 time: 3.6950s\n",
      "epoch: 0020 loss_train: 0.1794 acc_train: 0.9505 time: 3.6191s\n",
      "epoch: 0021 loss_train: 0.1712 acc_train: 0.9549 time: 3.6609s\n",
      "epoch: 0022 loss_train: 0.1643 acc_train: 0.9557 time: 3.6834s\n",
      "epoch: 0023 loss_train: 0.1587 acc_train: 0.9568 time: 3.6852s\n",
      "epoch: 0024 loss_train: 0.1541 acc_train: 0.9574 time: 3.5715s\n",
      "epoch: 0025 loss_train: 0.1498 acc_train: 0.9575 time: 3.8163s\n",
      "epoch: 0026 loss_train: 0.1455 acc_train: 0.9577 time: 3.6719s\n",
      "epoch: 0027 loss_train: 0.1418 acc_train: 0.9593 time: 3.6771s\n",
      "epoch: 0028 loss_train: 0.1381 acc_train: 0.9649 time: 3.6669s\n",
      "epoch: 0029 loss_train: 0.1347 acc_train: 0.9681 time: 3.7049s\n",
      "epoch: 0030 loss_train: 0.1306 acc_train: 0.9689 time: 3.6534s\n",
      "epoch: 0031 loss_train: 0.1275 acc_train: 0.9693 time: 3.6390s\n",
      "epoch: 0032 loss_train: 0.1242 acc_train: 0.9696 time: 3.6461s\n",
      "epoch: 0033 loss_train: 0.1215 acc_train: 0.9699 time: 4.0009s\n",
      "epoch: 0034 loss_train: 0.1193 acc_train: 0.9700 time: 3.6272s\n",
      "epoch: 0035 loss_train: 0.1174 acc_train: 0.9702 time: 3.7369s\n",
      "epoch: 0036 loss_train: 0.1152 acc_train: 0.9706 time: 3.7146s\n",
      "epoch: 0037 loss_train: 0.1136 acc_train: 0.9709 time: 3.6739s\n",
      "epoch: 0038 loss_train: 0.1123 acc_train: 0.9711 time: 3.6601s\n",
      "epoch: 0039 loss_train: 0.1113 acc_train: 0.9713 time: 3.6635s\n",
      "epoch: 0040 loss_train: 0.1098 acc_train: 0.9715 time: 3.6688s\n",
      "epoch: 0041 loss_train: 0.1083 acc_train: 0.9718 time: 3.6366s\n",
      "epoch: 0042 loss_train: 0.1076 acc_train: 0.9721 time: 3.7397s\n",
      "epoch: 0043 loss_train: 0.1076 acc_train: 0.9720 time: 3.6146s\n",
      "epoch: 0044 loss_train: 0.1060 acc_train: 0.9723 time: 3.6808s\n",
      "epoch: 0045 loss_train: 0.1047 acc_train: 0.9726 time: 3.9714s\n",
      "epoch: 0046 loss_train: 0.1032 acc_train: 0.9729 time: 4.3210s\n",
      "epoch: 0047 loss_train: 0.1035 acc_train: 0.9730 time: 4.3570s\n",
      "epoch: 0048 loss_train: 0.1018 acc_train: 0.9732 time: 4.0248s\n",
      "epoch: 0049 loss_train: 0.1008 acc_train: 0.9733 time: 3.6668s\n",
      "epoch: 0050 loss_train: 0.1000 acc_train: 0.9735 time: 3.6895s\n",
      "epoch: 0051 loss_train: 0.0987 acc_train: 0.9737 time: 3.8465s\n",
      "epoch: 0052 loss_train: 0.0983 acc_train: 0.9738 time: 3.6952s\n",
      "epoch: 0053 loss_train: 0.0983 acc_train: 0.9735 time: 3.7200s\n",
      "epoch: 0054 loss_train: 0.0972 acc_train: 0.9738 time: 3.6781s\n",
      "epoch: 0055 loss_train: 0.0962 acc_train: 0.9739 time: 3.6836s\n",
      "epoch: 0056 loss_train: 0.0961 acc_train: 0.9740 time: 3.6820s\n",
      "epoch: 0057 loss_train: 0.0947 acc_train: 0.9741 time: 3.6383s\n",
      "epoch: 0058 loss_train: 0.0942 acc_train: 0.9742 time: 3.6470s\n",
      "epoch: 0059 loss_train: 0.0933 acc_train: 0.9744 time: 3.6864s\n",
      "epoch: 0060 loss_train: 0.0934 acc_train: 0.9743 time: 3.7068s\n",
      "epoch: 0061 loss_train: 0.0923 acc_train: 0.9745 time: 3.6824s\n",
      "epoch: 0062 loss_train: 0.0919 acc_train: 0.9747 time: 3.6566s\n",
      "epoch: 0063 loss_train: 0.0917 acc_train: 0.9749 time: 3.6442s\n",
      "epoch: 0064 loss_train: 0.0904 acc_train: 0.9751 time: 3.5865s\n",
      "epoch: 0065 loss_train: 0.0902 acc_train: 0.9754 time: 3.6205s\n",
      "epoch: 0066 loss_train: 0.0888 acc_train: 0.9755 time: 3.6700s\n",
      "epoch: 0067 loss_train: 0.0893 acc_train: 0.9755 time: 3.6844s\n",
      "epoch: 0068 loss_train: 0.0878 acc_train: 0.9758 time: 3.6169s\n",
      "epoch: 0069 loss_train: 0.0875 acc_train: 0.9759 time: 3.6587s\n",
      "epoch: 0070 loss_train: 0.0866 acc_train: 0.9762 time: 3.6267s\n",
      "epoch: 0071 loss_train: 0.0867 acc_train: 0.9764 time: 3.6594s\n",
      "epoch: 0072 loss_train: 0.0858 acc_train: 0.9766 time: 3.6887s\n",
      "epoch: 0073 loss_train: 0.0856 acc_train: 0.9767 time: 3.7020s\n",
      "epoch: 0074 loss_train: 0.0848 acc_train: 0.9770 time: 3.6356s\n",
      "epoch: 0075 loss_train: 0.0847 acc_train: 0.9771 time: 3.6938s\n",
      "epoch: 0076 loss_train: 0.0835 acc_train: 0.9776 time: 3.7506s\n",
      "epoch: 0077 loss_train: 0.0833 acc_train: 0.9780 time: 3.7177s\n",
      "epoch: 0078 loss_train: 0.0827 acc_train: 0.9783 time: 3.7952s\n",
      "epoch: 0079 loss_train: 0.0824 acc_train: 0.9786 time: 3.6830s\n",
      "epoch: 0080 loss_train: 0.0819 acc_train: 0.9786 time: 3.7021s\n",
      "epoch: 0081 loss_train: 0.0812 acc_train: 0.9787 time: 3.6769s\n",
      "epoch: 0082 loss_train: 0.0808 acc_train: 0.9789 time: 3.6827s\n",
      "epoch: 0083 loss_train: 0.0804 acc_train: 0.9790 time: 3.6490s\n",
      "epoch: 0084 loss_train: 0.0803 acc_train: 0.9789 time: 3.7629s\n",
      "epoch: 0085 loss_train: 0.0795 acc_train: 0.9792 time: 3.6914s\n",
      "epoch: 0086 loss_train: 0.0794 acc_train: 0.9792 time: 3.6766s\n",
      "epoch: 0087 loss_train: 0.0787 acc_train: 0.9792 time: 3.6926s\n",
      "epoch: 0088 loss_train: 0.0788 acc_train: 0.9792 time: 3.7235s\n",
      "epoch: 0089 loss_train: 0.0785 acc_train: 0.9795 time: 3.7750s\n",
      "epoch: 0090 loss_train: 0.0779 acc_train: 0.9796 time: 3.6325s\n",
      "epoch: 0091 loss_train: 0.0777 acc_train: 0.9802 time: 3.6727s\n",
      "epoch: 0092 loss_train: 0.0776 acc_train: 0.9804 time: 3.6769s\n",
      "epoch: 0093 loss_train: 0.0770 acc_train: 0.9805 time: 3.7889s\n",
      "epoch: 0094 loss_train: 0.0766 acc_train: 0.9806 time: 3.6367s\n",
      "epoch: 0095 loss_train: 0.0764 acc_train: 0.9808 time: 3.6888s\n",
      "epoch: 0096 loss_train: 0.0763 acc_train: 0.9807 time: 3.6012s\n",
      "epoch: 0097 loss_train: 0.0754 acc_train: 0.9809 time: 3.6112s\n",
      "epoch: 0098 loss_train: 0.0749 acc_train: 0.9810 time: 3.5602s\n",
      "epoch: 0099 loss_train: 0.0750 acc_train: 0.9811 time: 3.6077s\n",
      "epoch: 0100 loss_train: 0.0745 acc_train: 0.9811 time: 3.6461s\n",
      "epoch: 0101 loss_train: 0.0743 acc_train: 0.9814 time: 3.6300s\n",
      "epoch: 0102 loss_train: 0.0738 acc_train: 0.9813 time: 3.6860s\n",
      "epoch: 0103 loss_train: 0.0730 acc_train: 0.9814 time: 3.5886s\n",
      "epoch: 0104 loss_train: 0.0731 acc_train: 0.9815 time: 3.6508s\n",
      "epoch: 0105 loss_train: 0.0733 acc_train: 0.9814 time: 3.6623s\n",
      "epoch: 0106 loss_train: 0.0723 acc_train: 0.9815 time: 3.6588s\n",
      "epoch: 0107 loss_train: 0.0723 acc_train: 0.9815 time: 3.6501s\n",
      "epoch: 0108 loss_train: 0.0722 acc_train: 0.9815 time: 3.6338s\n",
      "epoch: 0109 loss_train: 0.0724 acc_train: 0.9815 time: 3.7442s\n",
      "epoch: 0110 loss_train: 0.0717 acc_train: 0.9816 time: 3.6817s\n",
      "epoch: 0111 loss_train: 0.0711 acc_train: 0.9818 time: 3.6313s\n",
      "epoch: 0112 loss_train: 0.0712 acc_train: 0.9818 time: 3.6780s\n",
      "epoch: 0113 loss_train: 0.0710 acc_train: 0.9818 time: 3.7119s\n",
      "epoch: 0114 loss_train: 0.0706 acc_train: 0.9819 time: 3.6642s\n",
      "epoch: 0115 loss_train: 0.0708 acc_train: 0.9817 time: 3.6725s\n",
      "epoch: 0116 loss_train: 0.0705 acc_train: 0.9817 time: 3.6851s\n",
      "epoch: 0117 loss_train: 0.0699 acc_train: 0.9818 time: 3.7629s\n",
      "epoch: 0118 loss_train: 0.0695 acc_train: 0.9819 time: 3.6242s\n",
      "epoch: 0119 loss_train: 0.0692 acc_train: 0.9820 time: 3.7072s\n",
      "epoch: 0120 loss_train: 0.0697 acc_train: 0.9819 time: 3.6366s\n",
      "epoch: 0121 loss_train: 0.0689 acc_train: 0.9821 time: 3.7110s\n",
      "epoch: 0122 loss_train: 0.0686 acc_train: 0.9821 time: 3.6492s\n",
      "epoch: 0123 loss_train: 0.0683 acc_train: 0.9821 time: 3.6823s\n",
      "epoch: 0124 loss_train: 0.0683 acc_train: 0.9822 time: 3.6372s\n",
      "epoch: 0125 loss_train: 0.0681 acc_train: 0.9822 time: 3.7119s\n",
      "epoch: 0126 loss_train: 0.0677 acc_train: 0.9822 time: 3.6478s\n",
      "epoch: 0127 loss_train: 0.0678 acc_train: 0.9821 time: 3.6910s\n",
      "epoch: 0128 loss_train: 0.0681 acc_train: 0.9823 time: 3.6030s\n",
      "epoch: 0129 loss_train: 0.0673 acc_train: 0.9824 time: 3.6551s\n",
      "epoch: 0130 loss_train: 0.0673 acc_train: 0.9824 time: 3.7352s\n",
      "epoch: 0131 loss_train: 0.0673 acc_train: 0.9824 time: 3.7390s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0132 loss_train: 0.0672 acc_train: 0.9825 time: 3.6263s\n",
      "epoch: 0133 loss_train: 0.0667 acc_train: 0.9824 time: 3.6844s\n",
      "epoch: 0134 loss_train: 0.0663 acc_train: 0.9826 time: 3.9602s\n",
      "epoch: 0135 loss_train: 0.0662 acc_train: 0.9826 time: 3.6373s\n",
      "epoch: 0136 loss_train: 0.0663 acc_train: 0.9826 time: 3.7051s\n",
      "epoch: 0137 loss_train: 0.0662 acc_train: 0.9826 time: 3.6381s\n",
      "epoch: 0138 loss_train: 0.0651 acc_train: 0.9827 time: 3.6640s\n",
      "epoch: 0139 loss_train: 0.0654 acc_train: 0.9827 time: 3.8221s\n",
      "epoch: 0140 loss_train: 0.0654 acc_train: 0.9826 time: 3.6441s\n",
      "epoch: 0141 loss_train: 0.0648 acc_train: 0.9828 time: 3.6694s\n",
      "epoch: 0142 loss_train: 0.0652 acc_train: 0.9827 time: 3.7190s\n",
      "epoch: 0143 loss_train: 0.0649 acc_train: 0.9827 time: 3.6779s\n",
      "epoch: 0144 loss_train: 0.0644 acc_train: 0.9828 time: 3.6792s\n",
      "epoch: 0145 loss_train: 0.0642 acc_train: 0.9829 time: 3.6502s\n",
      "epoch: 0146 loss_train: 0.0639 acc_train: 0.9829 time: 3.6380s\n",
      "epoch: 0147 loss_train: 0.0640 acc_train: 0.9828 time: 3.6382s\n",
      "epoch: 0148 loss_train: 0.0644 acc_train: 0.9827 time: 3.7027s\n",
      "epoch: 0149 loss_train: 0.0635 acc_train: 0.9830 time: 3.6053s\n",
      "epoch: 0150 loss_train: 0.0633 acc_train: 0.9832 time: 3.6004s\n",
      "epoch: 0151 loss_train: 0.0634 acc_train: 0.9830 time: 3.7085s\n",
      "epoch: 0152 loss_train: 0.0629 acc_train: 0.9832 time: 3.7270s\n",
      "epoch: 0153 loss_train: 0.0630 acc_train: 0.9832 time: 3.7614s\n",
      "epoch: 0154 loss_train: 0.0630 acc_train: 0.9834 time: 3.7093s\n",
      "epoch: 0155 loss_train: 0.0629 acc_train: 0.9835 time: 3.6583s\n",
      "epoch: 0156 loss_train: 0.0628 acc_train: 0.9836 time: 3.7027s\n",
      "epoch: 0157 loss_train: 0.0626 acc_train: 0.9838 time: 3.6654s\n",
      "epoch: 0158 loss_train: 0.0621 acc_train: 0.9839 time: 3.7651s\n",
      "epoch: 0159 loss_train: 0.0625 acc_train: 0.9839 time: 4.4182s\n",
      "epoch: 0160 loss_train: 0.0619 acc_train: 0.9839 time: 4.1567s\n",
      "epoch: 0161 loss_train: 0.0622 acc_train: 0.9839 time: 3.6292s\n",
      "epoch: 0162 loss_train: 0.0617 acc_train: 0.9841 time: 3.6225s\n",
      "epoch: 0163 loss_train: 0.0617 acc_train: 0.9841 time: 3.6656s\n",
      "epoch: 0164 loss_train: 0.0621 acc_train: 0.9839 time: 3.6092s\n",
      "epoch: 0165 loss_train: 0.0619 acc_train: 0.9841 time: 3.7268s\n",
      "epoch: 0166 loss_train: 0.0613 acc_train: 0.9840 time: 3.6211s\n",
      "epoch: 0167 loss_train: 0.0610 acc_train: 0.9841 time: 3.6476s\n",
      "epoch: 0168 loss_train: 0.0614 acc_train: 0.9841 time: 3.6167s\n",
      "epoch: 0169 loss_train: 0.0611 acc_train: 0.9841 time: 3.6479s\n",
      "epoch: 0170 loss_train: 0.0608 acc_train: 0.9842 time: 3.7069s\n",
      "epoch: 0171 loss_train: 0.0606 acc_train: 0.9843 time: 3.7238s\n",
      "epoch: 0172 loss_train: 0.0608 acc_train: 0.9844 time: 3.6884s\n",
      "epoch: 0173 loss_train: 0.0608 acc_train: 0.9843 time: 3.7538s\n",
      "epoch: 0174 loss_train: 0.0605 acc_train: 0.9843 time: 3.6085s\n",
      "epoch: 0175 loss_train: 0.0606 acc_train: 0.9843 time: 3.6280s\n",
      "epoch: 0176 loss_train: 0.0599 acc_train: 0.9848 time: 3.5834s\n",
      "epoch: 0177 loss_train: 0.0603 acc_train: 0.9844 time: 3.6995s\n",
      "epoch: 0178 loss_train: 0.0597 acc_train: 0.9847 time: 3.6193s\n",
      "epoch: 0179 loss_train: 0.0597 acc_train: 0.9848 time: 3.5934s\n",
      "epoch: 0180 loss_train: 0.0597 acc_train: 0.9848 time: 3.6486s\n",
      "epoch: 0181 loss_train: 0.0594 acc_train: 0.9847 time: 3.6611s\n",
      "epoch: 0182 loss_train: 0.0595 acc_train: 0.9848 time: 3.7352s\n",
      "epoch: 0183 loss_train: 0.0597 acc_train: 0.9848 time: 3.6507s\n",
      "epoch: 0184 loss_train: 0.0597 acc_train: 0.9849 time: 3.6075s\n",
      "epoch: 0185 loss_train: 0.0589 acc_train: 0.9851 time: 3.6567s\n",
      "epoch: 0186 loss_train: 0.0591 acc_train: 0.9851 time: 3.6295s\n",
      "epoch: 0187 loss_train: 0.0587 acc_train: 0.9851 time: 3.6790s\n",
      "epoch: 0188 loss_train: 0.0587 acc_train: 0.9851 time: 3.7007s\n",
      "epoch: 0189 loss_train: 0.0589 acc_train: 0.9851 time: 3.7151s\n",
      "epoch: 0190 loss_train: 0.0582 acc_train: 0.9851 time: 3.6039s\n",
      "epoch: 0191 loss_train: 0.0590 acc_train: 0.9851 time: 3.5998s\n",
      "epoch: 0192 loss_train: 0.0586 acc_train: 0.9852 time: 3.6383s\n",
      "epoch: 0193 loss_train: 0.0586 acc_train: 0.9852 time: 3.6133s\n",
      "epoch: 0194 loss_train: 0.0583 acc_train: 0.9853 time: 3.5808s\n",
      "epoch: 0195 loss_train: 0.0580 acc_train: 0.9853 time: 3.6339s\n",
      "epoch: 0196 loss_train: 0.0581 acc_train: 0.9853 time: 3.6430s\n",
      "epoch: 0197 loss_train: 0.0580 acc_train: 0.9853 time: 3.6760s\n",
      "epoch: 0198 loss_train: 0.0579 acc_train: 0.9853 time: 3.6083s\n",
      "epoch: 0199 loss_train: 0.0572 acc_train: 0.9854 time: 3.6563s\n",
      "epoch: 0200 loss_train: 0.0573 acc_train: 0.9854 time: 3.6273s\n",
      "epoch: 0201 loss_train: 0.0574 acc_train: 0.9853 time: 3.7101s\n",
      "epoch: 0202 loss_train: 0.0576 acc_train: 0.9853 time: 3.6720s\n",
      "epoch: 0203 loss_train: 0.0573 acc_train: 0.9854 time: 3.6948s\n",
      "epoch: 0204 loss_train: 0.0572 acc_train: 0.9854 time: 3.7004s\n",
      "epoch: 0205 loss_train: 0.0570 acc_train: 0.9853 time: 3.6994s\n",
      "epoch: 0206 loss_train: 0.0566 acc_train: 0.9855 time: 3.6313s\n",
      "epoch: 0207 loss_train: 0.0573 acc_train: 0.9853 time: 3.7082s\n",
      "epoch: 0208 loss_train: 0.0567 acc_train: 0.9854 time: 3.6859s\n",
      "epoch: 0209 loss_train: 0.0566 acc_train: 0.9854 time: 3.6964s\n",
      "epoch: 0210 loss_train: 0.0571 acc_train: 0.9854 time: 3.6488s\n",
      "epoch: 0211 loss_train: 0.0565 acc_train: 0.9855 time: 3.6584s\n",
      "epoch: 0212 loss_train: 0.0566 acc_train: 0.9855 time: 3.6060s\n",
      "epoch: 0213 loss_train: 0.0566 acc_train: 0.9855 time: 3.6375s\n",
      "epoch: 0214 loss_train: 0.0559 acc_train: 0.9855 time: 3.6356s\n",
      "epoch: 0215 loss_train: 0.0565 acc_train: 0.9854 time: 3.6809s\n",
      "epoch: 0216 loss_train: 0.0560 acc_train: 0.9854 time: 3.5859s\n",
      "epoch: 0217 loss_train: 0.0559 acc_train: 0.9856 time: 3.6799s\n",
      "epoch: 0218 loss_train: 0.0560 acc_train: 0.9855 time: 3.7103s\n",
      "epoch: 0219 loss_train: 0.0558 acc_train: 0.9855 time: 3.7233s\n",
      "epoch: 0220 loss_train: 0.0558 acc_train: 0.9855 time: 3.6467s\n",
      "epoch: 0221 loss_train: 0.0555 acc_train: 0.9857 time: 3.7167s\n",
      "epoch: 0222 loss_train: 0.0553 acc_train: 0.9857 time: 3.6721s\n",
      "epoch: 0223 loss_train: 0.0557 acc_train: 0.9856 time: 3.7010s\n",
      "epoch: 0224 loss_train: 0.0556 acc_train: 0.9856 time: 3.6215s\n",
      "epoch: 0225 loss_train: 0.0552 acc_train: 0.9857 time: 3.7445s\n",
      "epoch: 0226 loss_train: 0.0554 acc_train: 0.9859 time: 3.6251s\n",
      "epoch: 0227 loss_train: 0.0551 acc_train: 0.9859 time: 3.7051s\n",
      "epoch: 0228 loss_train: 0.0555 acc_train: 0.9856 time: 3.6434s\n",
      "epoch: 0229 loss_train: 0.0553 acc_train: 0.9856 time: 3.6981s\n",
      "epoch: 0230 loss_train: 0.0552 acc_train: 0.9858 time: 3.6826s\n",
      "epoch: 0231 loss_train: 0.0548 acc_train: 0.9858 time: 3.7088s\n",
      "epoch: 0232 loss_train: 0.0550 acc_train: 0.9859 time: 3.6868s\n",
      "epoch: 0233 loss_train: 0.0551 acc_train: 0.9859 time: 3.7184s\n",
      "epoch: 0234 loss_train: 0.0550 acc_train: 0.9858 time: 3.6941s\n",
      "epoch: 0235 loss_train: 0.0545 acc_train: 0.9860 time: 3.7228s\n",
      "epoch: 0236 loss_train: 0.0548 acc_train: 0.9858 time: 3.7519s\n",
      "epoch: 0237 loss_train: 0.0544 acc_train: 0.9860 time: 3.6075s\n",
      "epoch: 0238 loss_train: 0.0540 acc_train: 0.9860 time: 3.6584s\n",
      "epoch: 0239 loss_train: 0.0546 acc_train: 0.9860 time: 3.6505s\n",
      "epoch: 0240 loss_train: 0.0542 acc_train: 0.9860 time: 3.7122s\n",
      "epoch: 0241 loss_train: 0.0544 acc_train: 0.9861 time: 3.6248s\n",
      "epoch: 0242 loss_train: 0.0544 acc_train: 0.9861 time: 3.6705s\n",
      "epoch: 0243 loss_train: 0.0543 acc_train: 0.9861 time: 3.6369s\n",
      "epoch: 0244 loss_train: 0.0544 acc_train: 0.9860 time: 3.6162s\n",
      "epoch: 0245 loss_train: 0.0542 acc_train: 0.9861 time: 3.7094s\n",
      "epoch: 0246 loss_train: 0.0541 acc_train: 0.9860 time: 3.7112s\n",
      "epoch: 0247 loss_train: 0.0540 acc_train: 0.9861 time: 3.7143s\n",
      "epoch: 0248 loss_train: 0.0540 acc_train: 0.9861 time: 3.7316s\n",
      "epoch: 0249 loss_train: 0.0542 acc_train: 0.9861 time: 3.7565s\n",
      "epoch: 0250 loss_train: 0.0538 acc_train: 0.9861 time: 3.8123s\n",
      "epoch: 0251 loss_train: 0.0538 acc_train: 0.9862 time: 3.6940s\n",
      "epoch: 0252 loss_train: 0.0536 acc_train: 0.9861 time: 3.7493s\n",
      "epoch: 0253 loss_train: 0.0533 acc_train: 0.9862 time: 3.8582s\n",
      "epoch: 0254 loss_train: 0.0532 acc_train: 0.9862 time: 3.7519s\n",
      "epoch: 0255 loss_train: 0.0533 acc_train: 0.9862 time: 3.6727s\n",
      "epoch: 0256 loss_train: 0.0536 acc_train: 0.9862 time: 3.6693s\n",
      "epoch: 0257 loss_train: 0.0533 acc_train: 0.9862 time: 3.6887s\n",
      "epoch: 0258 loss_train: 0.0533 acc_train: 0.9862 time: 3.6875s\n",
      "epoch: 0259 loss_train: 0.0528 acc_train: 0.9862 time: 3.6922s\n",
      "epoch: 0260 loss_train: 0.0535 acc_train: 0.9861 time: 3.6506s\n",
      "epoch: 0261 loss_train: 0.0531 acc_train: 0.9862 time: 3.6968s\n",
      "epoch: 0262 loss_train: 0.0529 acc_train: 0.9863 time: 3.6135s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0263 loss_train: 0.0529 acc_train: 0.9862 time: 3.6761s\n",
      "epoch: 0264 loss_train: 0.0526 acc_train: 0.9861 time: 3.6590s\n",
      "epoch: 0265 loss_train: 0.0531 acc_train: 0.9861 time: 3.7029s\n",
      "epoch: 0266 loss_train: 0.0526 acc_train: 0.9862 time: 3.7167s\n",
      "epoch: 0267 loss_train: 0.0523 acc_train: 0.9863 time: 3.7814s\n",
      "epoch: 0268 loss_train: 0.0529 acc_train: 0.9862 time: 3.7169s\n",
      "epoch: 0269 loss_train: 0.0525 acc_train: 0.9863 time: 3.6781s\n",
      "epoch: 0270 loss_train: 0.0526 acc_train: 0.9862 time: 3.6806s\n",
      "epoch: 0271 loss_train: 0.0523 acc_train: 0.9864 time: 3.6459s\n",
      "epoch: 0272 loss_train: 0.0523 acc_train: 0.9862 time: 3.6613s\n",
      "epoch: 0273 loss_train: 0.0524 acc_train: 0.9863 time: 3.7079s\n",
      "epoch: 0274 loss_train: 0.0521 acc_train: 0.9864 time: 3.6519s\n",
      "epoch: 0275 loss_train: 0.0519 acc_train: 0.9862 time: 4.0283s\n",
      "epoch: 0276 loss_train: 0.0524 acc_train: 0.9862 time: 3.6306s\n",
      "epoch: 0277 loss_train: 0.0516 acc_train: 0.9863 time: 3.7060s\n",
      "epoch: 0278 loss_train: 0.0522 acc_train: 0.9862 time: 3.7149s\n",
      "epoch: 0279 loss_train: 0.0522 acc_train: 0.9863 time: 3.6531s\n",
      "epoch: 0280 loss_train: 0.0518 acc_train: 0.9863 time: 3.6336s\n",
      "epoch: 0281 loss_train: 0.0520 acc_train: 0.9863 time: 3.6793s\n",
      "epoch: 0282 loss_train: 0.0520 acc_train: 0.9863 time: 3.6625s\n",
      "epoch: 0283 loss_train: 0.0515 acc_train: 0.9864 time: 3.7206s\n",
      "epoch: 0284 loss_train: 0.0514 acc_train: 0.9863 time: 3.7355s\n",
      "epoch: 0285 loss_train: 0.0520 acc_train: 0.9864 time: 3.7158s\n",
      "epoch: 0286 loss_train: 0.0519 acc_train: 0.9861 time: 3.7771s\n",
      "epoch: 0287 loss_train: 0.0516 acc_train: 0.9864 time: 3.7652s\n",
      "epoch: 0288 loss_train: 0.0515 acc_train: 0.9865 time: 3.8098s\n",
      "epoch: 0289 loss_train: 0.0515 acc_train: 0.9865 time: 3.6736s\n",
      "epoch: 0290 loss_train: 0.0515 acc_train: 0.9862 time: 3.6402s\n",
      "epoch: 0291 loss_train: 0.0512 acc_train: 0.9865 time: 3.7418s\n",
      "epoch: 0292 loss_train: 0.0511 acc_train: 0.9864 time: 3.6945s\n",
      "epoch: 0293 loss_train: 0.0518 acc_train: 0.9863 time: 3.6690s\n",
      "epoch: 0294 loss_train: 0.0512 acc_train: 0.9864 time: 3.7179s\n",
      "epoch: 0295 loss_train: 0.0507 acc_train: 0.9864 time: 3.7829s\n",
      "epoch: 0296 loss_train: 0.0515 acc_train: 0.9863 time: 4.4433s\n",
      "epoch: 0297 loss_train: 0.0511 acc_train: 0.9865 time: 3.7320s\n",
      "epoch: 0298 loss_train: 0.0510 acc_train: 0.9866 time: 3.6910s\n",
      "epoch: 0299 loss_train: 0.0508 acc_train: 0.9864 time: 3.6512s\n",
      "epoch: 0300 loss_train: 0.0514 acc_train: 0.9865 time: 3.6510s\n",
      "epoch: 0301 loss_train: 0.0511 acc_train: 0.9865 time: 3.7630s\n",
      "epoch: 0302 loss_train: 0.0505 acc_train: 0.9865 time: 3.6958s\n",
      "epoch: 0303 loss_train: 0.0509 acc_train: 0.9864 time: 3.7364s\n",
      "epoch: 0304 loss_train: 0.0508 acc_train: 0.9866 time: 3.6591s\n",
      "epoch: 0305 loss_train: 0.0507 acc_train: 0.9866 time: 3.6809s\n",
      "epoch: 0306 loss_train: 0.0505 acc_train: 0.9865 time: 3.6136s\n",
      "epoch: 0307 loss_train: 0.0505 acc_train: 0.9865 time: 3.6957s\n",
      "epoch: 0308 loss_train: 0.0508 acc_train: 0.9865 time: 3.6444s\n",
      "epoch: 0309 loss_train: 0.0504 acc_train: 0.9866 time: 3.6629s\n",
      "epoch: 0310 loss_train: 0.0505 acc_train: 0.9866 time: 3.7261s\n",
      "epoch: 0311 loss_train: 0.0507 acc_train: 0.9865 time: 3.7916s\n",
      "epoch: 0312 loss_train: 0.0504 acc_train: 0.9866 time: 3.7099s\n",
      "epoch: 0313 loss_train: 0.0506 acc_train: 0.9866 time: 3.7121s\n",
      "epoch: 0314 loss_train: 0.0502 acc_train: 0.9866 time: 3.7869s\n",
      "epoch: 0315 loss_train: 0.0504 acc_train: 0.9868 time: 3.6538s\n",
      "epoch: 0316 loss_train: 0.0500 acc_train: 0.9867 time: 3.7117s\n",
      "epoch: 0317 loss_train: 0.0506 acc_train: 0.9866 time: 3.6107s\n",
      "epoch: 0318 loss_train: 0.0505 acc_train: 0.9866 time: 3.6897s\n",
      "epoch: 0319 loss_train: 0.0502 acc_train: 0.9865 time: 3.6380s\n",
      "epoch: 0320 loss_train: 0.0501 acc_train: 0.9867 time: 3.7378s\n",
      "epoch: 0321 loss_train: 0.0499 acc_train: 0.9868 time: 3.6835s\n",
      "epoch: 0322 loss_train: 0.0503 acc_train: 0.9868 time: 3.6462s\n",
      "epoch: 0323 loss_train: 0.0499 acc_train: 0.9867 time: 3.6701s\n",
      "epoch: 0324 loss_train: 0.0500 acc_train: 0.9866 time: 3.7425s\n",
      "epoch: 0325 loss_train: 0.0504 acc_train: 0.9867 time: 3.8577s\n",
      "epoch: 0326 loss_train: 0.0502 acc_train: 0.9868 time: 3.6556s\n",
      "epoch: 0327 loss_train: 0.0499 acc_train: 0.9868 time: 3.7343s\n",
      "epoch: 0328 loss_train: 0.0498 acc_train: 0.9869 time: 3.6797s\n",
      "epoch: 0329 loss_train: 0.0497 acc_train: 0.9869 time: 3.7035s\n",
      "epoch: 0330 loss_train: 0.0497 acc_train: 0.9868 time: 3.8751s\n",
      "epoch: 0331 loss_train: 0.0498 acc_train: 0.9868 time: 4.0117s\n",
      "epoch: 0332 loss_train: 0.0497 acc_train: 0.9868 time: 3.6721s\n",
      "epoch: 0333 loss_train: 0.0498 acc_train: 0.9868 time: 3.6515s\n",
      "epoch: 0334 loss_train: 0.0494 acc_train: 0.9869 time: 3.6339s\n",
      "epoch: 0335 loss_train: 0.0497 acc_train: 0.9868 time: 3.6252s\n",
      "epoch: 0336 loss_train: 0.0498 acc_train: 0.9868 time: 3.6416s\n",
      "epoch: 0337 loss_train: 0.0497 acc_train: 0.9868 time: 3.7754s\n",
      "epoch: 0338 loss_train: 0.0501 acc_train: 0.9868 time: 3.5996s\n",
      "epoch: 0339 loss_train: 0.0493 acc_train: 0.9869 time: 3.6252s\n",
      "epoch: 0340 loss_train: 0.0496 acc_train: 0.9868 time: 3.6481s\n",
      "epoch: 0341 loss_train: 0.0496 acc_train: 0.9869 time: 3.6605s\n",
      "epoch: 0342 loss_train: 0.0497 acc_train: 0.9869 time: 3.7037s\n",
      "epoch: 0343 loss_train: 0.0495 acc_train: 0.9869 time: 3.7641s\n",
      "epoch: 0344 loss_train: 0.0493 acc_train: 0.9870 time: 3.7774s\n",
      "epoch: 0345 loss_train: 0.0490 acc_train: 0.9869 time: 3.7061s\n",
      "epoch: 0346 loss_train: 0.0499 acc_train: 0.9869 time: 3.5931s\n",
      "epoch: 0347 loss_train: 0.0493 acc_train: 0.9869 time: 3.6386s\n",
      "epoch: 0348 loss_train: 0.0496 acc_train: 0.9868 time: 3.6790s\n",
      "epoch: 0349 loss_train: 0.0490 acc_train: 0.9870 time: 3.6045s\n",
      "epoch: 0350 loss_train: 0.0490 acc_train: 0.9869 time: 3.6248s\n",
      "epoch: 0351 loss_train: 0.0493 acc_train: 0.9869 time: 3.6448s\n",
      "epoch: 0352 loss_train: 0.0495 acc_train: 0.9869 time: 3.7173s\n",
      "epoch: 0353 loss_train: 0.0492 acc_train: 0.9869 time: 3.6349s\n",
      "epoch: 0354 loss_train: 0.0489 acc_train: 0.9869 time: 3.7316s\n",
      "epoch: 0355 loss_train: 0.0486 acc_train: 0.9870 time: 3.6972s\n",
      "epoch: 0356 loss_train: 0.0490 acc_train: 0.9870 time: 3.6832s\n",
      "epoch: 0357 loss_train: 0.0490 acc_train: 0.9870 time: 3.7812s\n",
      "epoch: 0358 loss_train: 0.0492 acc_train: 0.9869 time: 3.7082s\n",
      "epoch: 0359 loss_train: 0.0488 acc_train: 0.9870 time: 3.6154s\n",
      "epoch: 0360 loss_train: 0.0487 acc_train: 0.9872 time: 3.7526s\n",
      "epoch: 0361 loss_train: 0.0487 acc_train: 0.9871 time: 3.6889s\n",
      "epoch: 0362 loss_train: 0.0489 acc_train: 0.9870 time: 3.7454s\n",
      "epoch: 0363 loss_train: 0.0488 acc_train: 0.9869 time: 3.7114s\n",
      "epoch: 0364 loss_train: 0.0488 acc_train: 0.9870 time: 3.6159s\n",
      "epoch: 0365 loss_train: 0.0492 acc_train: 0.9869 time: 3.6799s\n",
      "epoch: 0366 loss_train: 0.0490 acc_train: 0.9869 time: 3.6096s\n",
      "epoch: 0367 loss_train: 0.0486 acc_train: 0.9870 time: 3.7196s\n",
      "epoch: 0368 loss_train: 0.0488 acc_train: 0.9870 time: 3.6579s\n",
      "epoch: 0369 loss_train: 0.0490 acc_train: 0.9870 time: 3.6939s\n",
      "epoch: 0370 loss_train: 0.0484 acc_train: 0.9870 time: 3.7113s\n",
      "epoch: 0371 loss_train: 0.0480 acc_train: 0.9870 time: 3.6742s\n",
      "epoch: 0372 loss_train: 0.0485 acc_train: 0.9871 time: 3.6187s\n",
      "epoch: 0373 loss_train: 0.0486 acc_train: 0.9871 time: 3.6902s\n",
      "epoch: 0374 loss_train: 0.0486 acc_train: 0.9870 time: 3.6100s\n",
      "epoch: 0375 loss_train: 0.0485 acc_train: 0.9870 time: 3.6326s\n",
      "epoch: 0376 loss_train: 0.0482 acc_train: 0.9871 time: 3.7130s\n",
      "epoch: 0377 loss_train: 0.0485 acc_train: 0.9870 time: 3.6870s\n",
      "epoch: 0378 loss_train: 0.0483 acc_train: 0.9871 time: 3.6673s\n",
      "epoch: 0379 loss_train: 0.0483 acc_train: 0.9871 time: 3.6641s\n",
      "epoch: 0380 loss_train: 0.0485 acc_train: 0.9870 time: 3.6977s\n",
      "epoch: 0381 loss_train: 0.0487 acc_train: 0.9870 time: 3.6695s\n",
      "epoch: 0382 loss_train: 0.0487 acc_train: 0.9869 time: 3.6862s\n",
      "epoch: 0383 loss_train: 0.0483 acc_train: 0.9870 time: 3.6580s\n",
      "epoch: 0384 loss_train: 0.0484 acc_train: 0.9870 time: 3.7844s\n",
      "epoch: 0385 loss_train: 0.0481 acc_train: 0.9870 time: 3.6415s\n",
      "epoch: 0386 loss_train: 0.0480 acc_train: 0.9871 time: 3.6817s\n",
      "epoch: 0387 loss_train: 0.0484 acc_train: 0.9870 time: 3.5986s\n",
      "epoch: 0388 loss_train: 0.0480 acc_train: 0.9871 time: 4.1210s\n",
      "epoch: 0389 loss_train: 0.0481 acc_train: 0.9870 time: 4.2705s\n",
      "epoch: 0390 loss_train: 0.0480 acc_train: 0.9870 time: 3.8963s\n",
      "epoch: 0391 loss_train: 0.0482 acc_train: 0.9870 time: 3.7235s\n",
      "epoch: 0392 loss_train: 0.0485 acc_train: 0.9869 time: 3.6550s\n",
      "epoch: 0393 loss_train: 0.0480 acc_train: 0.9870 time: 3.6659s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0394 loss_train: 0.0484 acc_train: 0.9869 time: 3.7036s\n",
      "epoch: 0395 loss_train: 0.0481 acc_train: 0.9871 time: 3.6518s\n",
      "epoch: 0396 loss_train: 0.0486 acc_train: 0.9869 time: 3.7202s\n",
      "epoch: 0397 loss_train: 0.0479 acc_train: 0.9871 time: 3.6323s\n",
      "epoch: 0398 loss_train: 0.0479 acc_train: 0.9871 time: 3.6270s\n",
      "epoch: 0399 loss_train: 0.0480 acc_train: 0.9871 time: 3.7262s\n",
      "epoch: 0400 loss_train: 0.0476 acc_train: 0.9871 time: 3.6185s\n",
      "epoch: 0401 loss_train: 0.0483 acc_train: 0.9871 time: 3.6481s\n",
      "epoch: 0402 loss_train: 0.0475 acc_train: 0.9871 time: 3.7257s\n",
      "epoch: 0403 loss_train: 0.0478 acc_train: 0.9872 time: 3.6603s\n",
      "epoch: 0404 loss_train: 0.0476 acc_train: 0.9872 time: 3.6872s\n",
      "epoch: 0405 loss_train: 0.0481 acc_train: 0.9871 time: 3.6574s\n",
      "epoch: 0406 loss_train: 0.0479 acc_train: 0.9871 time: 3.6793s\n",
      "epoch: 0407 loss_train: 0.0479 acc_train: 0.9871 time: 3.7018s\n",
      "epoch: 0408 loss_train: 0.0477 acc_train: 0.9871 time: 3.6784s\n",
      "epoch: 0409 loss_train: 0.0477 acc_train: 0.9872 time: 3.7159s\n",
      "epoch: 0410 loss_train: 0.0479 acc_train: 0.9870 time: 3.7323s\n",
      "epoch: 0411 loss_train: 0.0473 acc_train: 0.9872 time: 4.0563s\n",
      "epoch: 0412 loss_train: 0.0476 acc_train: 0.9871 time: 3.7539s\n",
      "epoch: 0413 loss_train: 0.0471 acc_train: 0.9872 time: 3.6427s\n",
      "epoch: 0414 loss_train: 0.0475 acc_train: 0.9871 time: 3.6538s\n",
      "epoch: 0415 loss_train: 0.0476 acc_train: 0.9871 time: 3.6921s\n",
      "epoch: 0416 loss_train: 0.0475 acc_train: 0.9871 time: 3.8501s\n",
      "epoch: 0417 loss_train: 0.0476 acc_train: 0.9871 time: 3.6193s\n",
      "epoch: 0418 loss_train: 0.0478 acc_train: 0.9870 time: 3.6675s\n",
      "epoch: 0419 loss_train: 0.0472 acc_train: 0.9872 time: 3.6543s\n",
      "epoch: 0420 loss_train: 0.0470 acc_train: 0.9873 time: 3.6310s\n",
      "epoch: 0421 loss_train: 0.0475 acc_train: 0.9871 time: 3.6117s\n",
      "epoch: 0422 loss_train: 0.0475 acc_train: 0.9871 time: 3.6533s\n",
      "epoch: 0423 loss_train: 0.0471 acc_train: 0.9872 time: 3.6876s\n",
      "epoch: 0424 loss_train: 0.0481 acc_train: 0.9871 time: 3.6879s\n",
      "epoch: 0425 loss_train: 0.0473 acc_train: 0.9871 time: 3.6453s\n",
      "epoch: 0426 loss_train: 0.0471 acc_train: 0.9872 time: 3.7206s\n",
      "epoch: 0427 loss_train: 0.0474 acc_train: 0.9871 time: 3.6807s\n",
      "epoch: 0428 loss_train: 0.0471 acc_train: 0.9872 time: 3.6779s\n",
      "epoch: 0429 loss_train: 0.0469 acc_train: 0.9872 time: 3.6431s\n",
      "epoch: 0430 loss_train: 0.0474 acc_train: 0.9872 time: 3.6823s\n",
      "epoch: 0431 loss_train: 0.0473 acc_train: 0.9871 time: 3.6695s\n",
      "epoch: 0432 loss_train: 0.0472 acc_train: 0.9872 time: 3.7102s\n",
      "epoch: 0433 loss_train: 0.0470 acc_train: 0.9872 time: 3.6843s\n",
      "epoch: 0434 loss_train: 0.0467 acc_train: 0.9872 time: 3.7683s\n",
      "epoch: 0435 loss_train: 0.0469 acc_train: 0.9874 time: 3.6791s\n",
      "epoch: 0436 loss_train: 0.0469 acc_train: 0.9872 time: 3.6460s\n",
      "epoch: 0437 loss_train: 0.0469 acc_train: 0.9873 time: 3.6923s\n",
      "epoch: 0438 loss_train: 0.0471 acc_train: 0.9872 time: 4.2988s\n",
      "epoch: 0439 loss_train: 0.0471 acc_train: 0.9873 time: 3.9620s\n",
      "epoch: 0440 loss_train: 0.0468 acc_train: 0.9873 time: 3.7166s\n",
      "epoch: 0441 loss_train: 0.0467 acc_train: 0.9872 time: 3.6341s\n",
      "epoch: 0442 loss_train: 0.0467 acc_train: 0.9873 time: 3.7753s\n",
      "epoch: 0443 loss_train: 0.0468 acc_train: 0.9872 time: 3.7041s\n",
      "epoch: 0444 loss_train: 0.0464 acc_train: 0.9873 time: 3.7643s\n",
      "epoch: 0445 loss_train: 0.0466 acc_train: 0.9873 time: 3.5892s\n",
      "epoch: 0446 loss_train: 0.0468 acc_train: 0.9873 time: 3.6091s\n",
      "epoch: 0447 loss_train: 0.0465 acc_train: 0.9873 time: 3.6878s\n",
      "epoch: 0448 loss_train: 0.0465 acc_train: 0.9873 time: 3.6917s\n",
      "epoch: 0449 loss_train: 0.0474 acc_train: 0.9871 time: 3.6582s\n",
      "epoch: 0450 loss_train: 0.0466 acc_train: 0.9873 time: 3.6609s\n",
      "epoch: 0451 loss_train: 0.0467 acc_train: 0.9873 time: 3.6555s\n",
      "epoch: 0452 loss_train: 0.0465 acc_train: 0.9873 time: 3.7217s\n",
      "epoch: 0453 loss_train: 0.0462 acc_train: 0.9873 time: 3.6937s\n",
      "epoch: 0454 loss_train: 0.0463 acc_train: 0.9874 time: 3.6631s\n",
      "epoch: 0455 loss_train: 0.0468 acc_train: 0.9873 time: 3.7082s\n",
      "epoch: 0456 loss_train: 0.0468 acc_train: 0.9872 time: 3.7006s\n",
      "epoch: 0457 loss_train: 0.0467 acc_train: 0.9872 time: 3.8029s\n",
      "epoch: 0458 loss_train: 0.0464 acc_train: 0.9873 time: 3.6569s\n",
      "epoch: 0459 loss_train: 0.0467 acc_train: 0.9873 time: 3.7153s\n",
      "epoch: 0460 loss_train: 0.0465 acc_train: 0.9872 time: 3.7402s\n",
      "epoch: 0461 loss_train: 0.0464 acc_train: 0.9874 time: 3.6910s\n",
      "epoch: 0462 loss_train: 0.0470 acc_train: 0.9872 time: 3.6374s\n",
      "epoch: 0463 loss_train: 0.0463 acc_train: 0.9872 time: 3.6424s\n",
      "epoch: 0464 loss_train: 0.0462 acc_train: 0.9875 time: 3.7171s\n",
      "epoch: 0465 loss_train: 0.0459 acc_train: 0.9873 time: 3.6379s\n",
      "epoch: 0466 loss_train: 0.0464 acc_train: 0.9873 time: 3.6808s\n",
      "epoch: 0467 loss_train: 0.0466 acc_train: 0.9874 time: 3.7211s\n",
      "epoch: 0468 loss_train: 0.0462 acc_train: 0.9873 time: 3.5898s\n",
      "epoch: 0469 loss_train: 0.0462 acc_train: 0.9874 time: 3.6362s\n",
      "epoch: 0470 loss_train: 0.0462 acc_train: 0.9873 time: 3.5971s\n",
      "epoch: 0471 loss_train: 0.0463 acc_train: 0.9873 time: 3.6154s\n",
      "epoch: 0472 loss_train: 0.0464 acc_train: 0.9873 time: 3.8569s\n",
      "epoch: 0473 loss_train: 0.0464 acc_train: 0.9873 time: 4.4206s\n",
      "epoch: 0474 loss_train: 0.0461 acc_train: 0.9874 time: 4.1856s\n",
      "epoch: 0475 loss_train: 0.0462 acc_train: 0.9873 time: 3.6696s\n",
      "epoch: 0476 loss_train: 0.0463 acc_train: 0.9873 time: 3.6352s\n",
      "epoch: 0477 loss_train: 0.0460 acc_train: 0.9875 time: 3.6753s\n",
      "epoch: 0478 loss_train: 0.0466 acc_train: 0.9873 time: 3.6376s\n",
      "epoch: 0479 loss_train: 0.0462 acc_train: 0.9874 time: 3.6725s\n",
      "epoch: 0480 loss_train: 0.0461 acc_train: 0.9872 time: 3.7285s\n",
      "epoch: 0481 loss_train: 0.0465 acc_train: 0.9874 time: 3.6912s\n",
      "epoch: 0482 loss_train: 0.0463 acc_train: 0.9873 time: 3.6924s\n",
      "epoch: 0483 loss_train: 0.0463 acc_train: 0.9873 time: 3.7353s\n",
      "epoch: 0484 loss_train: 0.0464 acc_train: 0.9873 time: 3.6038s\n",
      "epoch: 0485 loss_train: 0.0460 acc_train: 0.9873 time: 3.6825s\n",
      "epoch: 0486 loss_train: 0.0461 acc_train: 0.9873 time: 3.6855s\n",
      "epoch: 0487 loss_train: 0.0462 acc_train: 0.9873 time: 3.6141s\n",
      "epoch: 0488 loss_train: 0.0458 acc_train: 0.9874 time: 3.7123s\n",
      "epoch: 0489 loss_train: 0.0457 acc_train: 0.9875 time: 3.6229s\n",
      "epoch: 0490 loss_train: 0.0458 acc_train: 0.9874 time: 3.7203s\n",
      "epoch: 0491 loss_train: 0.0458 acc_train: 0.9874 time: 3.7464s\n",
      "epoch: 0492 loss_train: 0.0459 acc_train: 0.9873 time: 3.6280s\n",
      "epoch: 0493 loss_train: 0.0458 acc_train: 0.9873 time: 3.6417s\n",
      "epoch: 0494 loss_train: 0.0459 acc_train: 0.9873 time: 3.6857s\n",
      "epoch: 0495 loss_train: 0.0458 acc_train: 0.9874 time: 3.6188s\n",
      "epoch: 0496 loss_train: 0.0462 acc_train: 0.9872 time: 3.6418s\n",
      "epoch: 0497 loss_train: 0.0453 acc_train: 0.9874 time: 3.6106s\n",
      "epoch: 0498 loss_train: 0.0456 acc_train: 0.9874 time: 3.6847s\n",
      "epoch: 0499 loss_train: 0.0456 acc_train: 0.9875 time: 3.6694s\n",
      "epoch: 0500 loss_train: 0.0457 acc_train: 0.9874 time: 3.6438s\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 500\n",
    "for i in range(1, 1 + epochs):\n",
    "#     train_loader = NeighborLoader(train_data, input_nodes = train_num_nodes, num_neighbors=hop, batch_size=1024, shuffle=True)\n",
    "    start_time = time.time()\n",
    "    loss, acc = train()\n",
    "    end_time = time.time()\n",
    "    if i % 10 == 0:\n",
    "        torch.save(model.state_dict(), 'MultiLayer3GAT' + str(i))\n",
    "    print('epoch: {:04d}'.format(i),\n",
    "          'loss_train: {:.4f}'.format(loss),\n",
    "          'acc_train: {:.4f}'.format(acc),\n",
    "          'time: {:.4f}s'.format(end_time - start_time))\n",
    "    loss_all.append(loss)\n",
    "    acc_all.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "09306312",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MultiLayer3GAT_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e09dc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "db005a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GAT3_Layer', np.array(loss_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cf67aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GAT3_Layer_acc', np.array(acc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d131f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all2 = np.load('GAT2_Layer.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "90529acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all5 = np.load('GAT5_Layer.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "200eff79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ad86c0f0cf8>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_all5[300:])\n",
    "plt.plot(loss_all[800:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f1f45f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ad86c0c3c88>]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_all[500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cfa4a38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2add3a3b4d30>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAF5dJREFUeJzt3WuQZPV53/Hv0zOzwy67sMCOYQ27rBRhJb4gg7awKF2KcqwEYRW8MI7BKWM5cm2VIlWkWKmUcKqQ7RdxnFRUsYxKGEvEliPrEkmRNxiVjGy5JLmKy4K4I6yVgswiECtYbtqF3Zl58uKcnnNmpmend6Znus/h+6nq6nPr08/M9v7Of55zujsyE0lSu3SGXYAkafAMd0lqIcNdklrIcJekFjLcJamFDHdJaiHDXZJaaNlwj4gdEfHViHgoIh6MiPf22OaSiHguIu4pb9etTbmSpH6M97HNNPD+zLw7IrYAd0XErZn50ILtvp6Zbx98iZKkE7VsuGfmE8AT5fQLEfEwcDawMNxPyLZt23LXrl2r2YUkveLcddddP8zMqeW262fkPicidgEXALf3WH1xRNwLfB/4D5n5YI/H7wH2AOzcuZN9+/adyNNL0iteRHyvn+36PqEaEZuBzwPvy8znF6y+Gzg3M18H/BHwxV77yMwbM3N3Zu6emlr2wCNJWqG+wj0iJiiC/ZOZ+YWF6zPz+cx8sZy+BZiIiG0DrVSS1Ld+rpYJ4OPAw5n5oSW2Oavcjoi4qNzv04MsVJLUv3567m8Efg24PyLuKZf9NrATIDNvAK4E3hUR08AR4Kr0s4QlaWj6uVrmG0Ass831wPWDKkqStDq+Q1WSWshwl6QWMtxVmZ2Bu/+8uJfUaIa7Kvtugr3vgTv+ZNiVSFolw12VI4eK+8M/HG4dklbNcFdNeVGUV7FKjWe4q9K94DVnh1qGpNUz3FWJ7svBkbvUdIa7arptGUfuUtMZ7qqEPXepLQx31cw13YdahaTVM9xV6fbcHblLjWe4q2JbRmoNw101tmWktjDcVbEtI7WG4a5KeCmk1BaGuyq+iUlqDcNdNY7cpbYw3FXxahmpNQx3Vey5S61huKvGSyGltjDcVbEtI7WG4a4aR+5SWxjuqvgmJqk1DHdVbMtIrWG4q+KbmKTWMNxV46WQUlsY7qrYlpFaw3BXxbaM1BqGu2psy0htYbir4qWQUmsY7qr42TJSaywb7hGxIyK+GhEPRcSDEfHeHttERHw4IvZHxH0RceHalKv14chdarrxPraZBt6fmXdHxBbgroi4NTMfqm3zNuC88vZzwEfLezWJV8tIrbHsyD0zn8jMu8vpF4CHgbMXbHYF8Iks3AZsjYjtA69Wa8zPlpHa4oR67hGxC7gAuH3BqrOBx2rzB1h8ANCo84Sq1Bp9h3tEbAY+D7wvM59fyZNFxJ6I2BcR+w4ePLiSXWgteUJVao2+wj0iJiiC/ZOZ+YUemzwO7KjNn1Mumyczb8zM3Zm5e2pqaiX1SpL60M/VMgF8HHg4Mz+0xGZ7gWvKq2beADyXmU8MsE6th247xpG71Hj9XC3zRuDXgPsj4p5y2W8DOwEy8wbgFuAyYD9wGPiNwZeqdWPPXWq8ZcM9M79BdRnFUtsk8O5BFaVhceQutYXvUFVlbsTuyF1qOsNdi9mWkRrPcFeNbRmpLQx39eDIXWo6w12VuUshDXep6Qx39WC4S01nuGsxe+5S4xnuWsy2jNR4hrsqhrrUGoa7arwUUmoLw12LOYKXGs9wV8VPhZRaw3BXD47cpaYz3FXjyF1qC8Ndi9lzlxrPcFfFj/yVWsNwV42fLSO1heGuHgx3qekMd1W8FFJqDcNdi9mWkRrPcFeNI3epLQx3LebIXWo8w12VuZ77zHDrkLRqhrtqbMtIbWG4azHDXWo8w10VL4WUWsNw12Kz9tylpjPcVePHD0htYbhrMdsyUuMZ7qp4KaTUGoa7ajyhKrWF4a7FPKEqNZ7hroqXQkqtsWy4R8RNEfFURDywxPpLIuK5iLinvF03+DK1ruy5S4033sc2fwpcD3ziONt8PTPfPpCKNEReCim1xbIj98z8GvDMOtSiUWFbRmq8QfXcL46IeyPiSxHxUwPap9abPXepNfppyyznbuDczHwxIi4Dvgic12vDiNgD7AHYuXPnAJ5aa8Jwlxpv1SP3zHw+M18sp28BJiJi2xLb3piZuzNz99TU1GqfWmvFSyGlxlt1uEfEWRER5fRF5T6fXu1+NQS2ZaTWWLYtExGfAi4BtkXEAeCDwARAZt4AXAm8KyKmgSPAVZlebtFoXgopNd6y4Z6ZVy+z/nqKSyXVeB6TpbbwHaqS1EKGuyp206TWMNwlqYUMd9U4cpfawnBXxbaM1BqGuyS1kOGumtrIfdY3MklNZrirN9/IJDWa4a5Kvefu58tIjWa4qzdH7lKjGe6qceQutYXhrkq9LePIXWo0w129ebWM1GiGu2ocuUttYbirN3vuUqMZ7qrM67nblpGazHBXb7PTw65A0ioY7qqx5y61heGuiu9QlVrDcFdvtmWkRjPcVVMfuRvuUpMZ7urNcJcazXBXxZ671BqGu3pz5C41muGuGnvuUlsY7qrUvx/bcJcazXBXbzOGu9RkhrtqbMtIbWG4qzfDXWo0w12VdOQutYXhrt4Md6nRDHfV+CYmqS0Md1Vsy0itsWy4R8RNEfFURDywxPqIiA9HxP6IuC8iLhx8mVp3hrvUaP2M3P8UuPQ4698GnFfe9gAfXX1ZGg5H7lJbLBvumfk14JnjbHIF8Iks3AZsjYjtgypQQ2LPXWq0QfTczwYeq80fKJepaey5S62xridUI2JPROyLiH0HDx5cz6fWiTLcpUYbRLg/DuyozZ9TLlskM2/MzN2ZuXtqamoAT63Bqo/cjw2vDEmrNohw3wtcU1418wbgucx8YgD71Xrzyzqk1hhfboOI+BRwCbAtIg4AHwQmADLzBuAW4DJgP3AY+I21KlbrIDqQs7ZlpIZbNtwz8+pl1ifw7oFVpCFKIKAzbrhLDec7VDVfGO5SGxjuqmRt5O6XdUiNZrhrsc64V8tIDWe4qyaLtsz4JMwcHXYxklbBcNdiYxtgxpG71GSGuyrdnvvYBEy/POxqJK2C4a6a8k1MY7ZlpKYz3DVflCN3w11qNMNdlbm2zAbDXWo4w12LjU96QlVqOMNdNVm1ZTyhKjWa4a7FbMtIjWe4qzKv525bRmoyw12LjW2AGdsyUpMZ7povvFpGagPDXZVuW2bctozUdIa7Fhvb4NUyUsMZ7qrpXgrpde5S0xnuWmxswhOqUsMZ7qos/PiBzGFXJGmFDHfVlGE+sbG4n35peKVIWhXDXfMFMLGpmD52ZKilSFo5w12VblumO3I33KXGMty1mCN3qfEMd9WUl0JOnFTMHjs83HIkrZjhrsVsy0iNZ7irMtdz77ZlHLlLTWW4q2bBpZCO3KXGMtw1X9RG7tOGu9RUhrsqc58K2T2harhLTWW4a7HuyP2oPXepqQx31ZSXQk5uLmaPvjjcciStmOGuxcZPgs44vPzCsCuRtEJ9hXtEXBoRj0TE/oj4QI/174iIgxFxT3n7zcGXqjXX7blHwOQWw11qsPHlNoiIMeAjwFuBA8CdEbE3Mx9asOlnMvM9a1Cj1k3tI34nTzHcpQbrZ+R+EbA/M7+bmUeBTwNXrG1ZGpqI4n7yFHj5+eHWImnF+gn3s4HHavMHymUL/VJE3BcRn4uIHQOpTuur25YB2zJSww3qhOr/BXZl5vnArcCf9dooIvZExL6I2Hfw4MEBPbXWxOQWR+5Sg/UT7o8D9ZH4OeWyOZn5dGZ2v3TzY8Dre+0oM2/MzN2ZuXtqamol9WpNZa0tswVeMtylpuon3O8EzouIV0XEBuAqYG99g4jYXpu9HHh4cCVqKDadDkcODbsKSSu07NUymTkdEe8BvgyMATdl5oMR8XvAvszcC/y7iLgcmAaeAd6xhjVrrdR77pvOgJeehZlpGFv2ZSJpxPT1vzYzbwFuWbDsutr0tcC1gy1NQ7XpjOL+yCHYbAtNahrfoaqaWs9942nF/ZFnhleOpBUz3FVJmNeWATj89LCqkbQKhrt6O7lsxbz4g+HWIWlFDHfV1Noyp55T3D/72NKbSxpZhrt627i1+AiC5wx3qYkMd1Xql0ICbN3pyF1qKMNdSzt1hyN3qaEMd9XkvIE7W3fAs/84tGokrZzhrsrCtsypO4oPDzvy7NBKkrQyhruWdtqu4v6Z7wy1DEknznBXTe1SSIDtryvuv//N4ZQjacUMdy1t607YtA0ev3vYlUg6QYa7Kgt77hFw9uvh8buGVpKklTHcdXzn7IaDj8BhP0BMahLDXTULeu4A/+Tni+X7vzKUiiStjOGuysK2DMCPXwgn/xg88qWhlCRpZQx3HV+nA6+9FL59K7z84rCrkdQnw101PdoyABdcA0dfgPs/u/4lSVoRw13LO2c3nHU+3HYDzM4MuxpJfTDcVenVc4diNP/m34IfPgIPfH7dy5J04gx39eefXQFn/gz83e/D9NFhVyNpGYa7apbouUNxYvUXfgee+S587b+tZ1GSVsBwV2WptkzXeb8Ar7savv7f4R9vW7eyJJ04w10n5tLfh9POhf91JfzDXw+7GklLMNxVc5y2TNfG0+AdfwWn74K/+GX4yu/ag5dGkOGuE3fKj8M7b4ULr4FvfAj++C3w2B3DrkpSjeGuynI997qJjXD5H8GvfhZefgE+/la46VK482Pw1LdgdnZNS5V0fOPDLkAN9xP/Et59G9zxJ3DPJ+Gv3l8sn9gEZ/40bD+/+NKPM84revWbzyquvJG0pgx31fTRc+9lckvxJqc3/Xt4+jvw2G3w5APw2O1w76eL0XzX2GTxJSCnnVvcb90Jm8+EM14DW86CTWfAhpMH9yNJr1CGuyon0pbpJQK2vaa4dc3OFtfGH3oUnn0UDn2vnP5e8Q1PR3p8TvxJp8KW7bD5x4pPpNx0RnEAmdwMk6fAptNh4+nFyd2TTi1uk6f4F4FUY7hrbXU6iwO/7vAzcPjp4gDwwpNF2D93oJj+0UF4fB8cPlR8cFker48fxXmAiY1FS2hiU3EQ6IwXB4ENJ8PYBIxtKG7d7cZPgvHJ8nZSsXzTGcU20am2HZ8s5ic2VvvojK/sLx1pHTQu3B/6+72M/91/hgiSICOAIOkU99356JDE3HZEsX6WIKJYXzyu2k+W++l0gk5njIju4zvlf+JivrsvojP3fER9m061XQRR276+nOgQc8uBGJu7L3bVKR5bPj66jynrr8/3vO90yM5EsY/OWHGLMTrMELPTdMrnGIti39ue+T5jM8nkev6Dbjq9uG077/jbZcKxI/DSc3DkUHEQOHKomO/ejh0utjl2BI6+WBw4Zo7BwW/B9EvF9MxRmH652Pa4B4s+RKc4IHSDfmyiuO+MQac7PQ5j40U7amJjbf148fj6fPnv03ub4z2uu2ysfC0t8Zi512H5Oh2fLH4HnfHiZ5idLg6COVt7ndZfu91br9f7wnW1GqH4wLnu76v+f6n7l+K8/18eMAehceE+e/hljsxMQBXLwCzBdDldRHgnu1Fe3HeXx3HuO8wWbWdmyUw6teeo3zrk3PPP36bYR/0+Fsx3Itf5N3Zi7h/7KX5m2EX0EgEbNhW3U7avfn+ZVdBPv1yE//TLcOxHxV8SM9NF2M0eq9bPzhQHjpmjxYFi+qXyoHG03Ha69rjysbMzxfT0S8VVRTlTLpupTU/X5qerZTlb21e5/hWnFvbRKQ9+5UGwfjCZOVab7zHIqh9M5i1j/r7mPefx7mv1QfFv1R0sdA+yS9UQHTj/X8Huf7Omv7m+wj0iLgX+EBgDPpaZ/2XB+kngE8DrgaeBX8nMRwdbauGnDyV88BtrsesVS2A2OtV9QEaHmQhmo4j12bGx4q+JTqfYZrxTzI91mI0gO8V0sSyYjYBOh9lOh+yUf0GMlYeITsxfHkF2ittsp/hLoXg8MFbUQqeYz05xWJrJsp4onmu20ykOUFt3DfeXuV4iqnZMU2TWAr97AFh4gChDZt7ych3l45Ni+tiRIohmZ4oDVAQcPVwsy6xtP1s9d3d63roF68lyn8fK7Won6qdfqj1+7gerLcull9V/rlzw/J3xWk25zDTzl+VMFczznrNXDeX83HS5rxgrWpCZ1b9D93Fz07UaYu3PDy0b7hExBnwEeCtwALgzIvZm5kO1zd4JHMrM10TEVcAfAL+yFgXz5jfDzTeXv7QsTtgtnO51fyLrOrWj+OwsTE8Xt4XKx0QmY/X99Hubrb2g6rdey5ZaPujH75ga/L+ZBiOiGrlKy+hn5H4RsD8zvwsQEZ8GrgDq4X4F8Dvl9OeA6yMiMucObYOzfTv84i8OfLeS1Cb9/G1wNvBYbf5AuaznNpk5DTwHnDGIAiVJJ25dLwyOiD0RsS8i9h08eHA9n1qSXlH6CffHgR21+XPKZT23iYhx4FSKE6vzZOaNmbk7M3dPTdnblaS10k+43wmcFxGviogNwFXA3gXb7AV+vZy+EvjbNem3S5L6suwJ1cycjoj3AF+muBTypsx8MCJ+D9iXmXuBjwN/HhH7gWcoDgCSpCHp6zr3zLwFuGXBsutq0y8BvzzY0iRJK+UnLUlSCxnuktRCMazznhFxEPjeCh++DfjhAMtZa9a7dppUKzSr3ibVCs2qdzW1npuZy15uOLRwX42I2JeZu4ddR7+sd+00qVZoVr1NqhWaVe961GpbRpJayHCXpBZqarjfOOwCTpD1rp0m1QrNqrdJtUKz6l3zWhvZc5ckHV9TR+6SpONoXLhHxKUR8UhE7I+IDwy7HoCIuCkinoqIB2rLTo+IWyPi2+X9aeXyiIgPl/XfFxEXrnOtOyLiqxHxUEQ8GBHvHdV6I+KkiLgjIu4ta/3dcvmrIuL2sqbPlJ95RERMlvP7y/W71qvWBXWPRcQ3I+LmUa83Ih6NiPsj4p6I2FcuG7nXQvn8WyPicxHxrYh4OCIuHuFaX1v+Tru35yPifetab2Y25kbx2TbfAV4NbADuBX5yBOp6C3Ah8EBt2X8FPlBOfwD4g3L6MuBLFF+++Abg9nWudTtwYTm9BfgH4CdHsd7yOTeX0xPA7WUNnwWuKpffALyrnP63wA3l9FXAZ4b0evgt4C+Am8v5ka0XeBTYtmDZyL0Wyuf/M+A3y+kNwNZRrXVB3WPAk8C561nvUH7YVfySLga+XJu/Frh22HWVtexaEO6PANvL6e3AI+X0HwNX99puSHX/JcVXKI50vcAm4G7g5yje/DG+8DVB8eF2F5fT4+V2sc51ngP8DfDzwM3lf9ZRrrdXuI/ca4HiY8T/38LfzyjW2qP2fwH8/XrX27S2TD/fCjUqzszMJ8rpJ4Ezy+mR+RnKNsAFFCPikay3bHHcAzwF3Erxl9uzWXzj18J6RuEbwf4H8B+B8gtyOYPRrjeBv46IuyJiT7lsFF8LrwIOAv+zbHl9LCJOHtFaF7oK+FQ5vW71Ni3cGymLQ/FIXZYUEZuBzwPvy8zn6+tGqd7MnMnMn6UYEV8E/NMhl7SkiHg78FRm3jXsWk7AmzLzQuBtwLsj4i31lSP0WhinaH1+NDMvAH5E0daYM0K1zinPr1wO/O+F69a63qaFez/fCjUqfhAR2wHK+6fK5UP/GSJigiLYP5mZXygXj2y9AJn5LPBVirbG1ii+8WthPX19I9gaeiNweUQ8CnyaojXzhyNcL5n5eHn/FPB/KA6go/haOAAcyMzby/nPUYT9KNZa9zbg7sz8QTm/bvU2Ldz7+VaoUVH/dqpfp+htd5dfU54dfwPwXO3PtDUXEUHx5SoPZ+aHRrneiJiKiK3l9EaKcwMPU4T8lUvUOrRvBMvMazPznMzcRfHa/NvM/NejWm9EnBwRW7rTFL3hBxjB10JmPgk8FhGvLRf9c+ChUax1gaupWjLdutan3mGcYFjlyYnLKK7w+A7wn4ZdT1nTp4AngGMUI4x3UvRO/wb4NvAV4PRy2wA+UtZ/P7B7nWt9E8WfgvcB95S3y0axXuB84JtlrQ8A15XLXw3cAeyn+HN3slx+Ujm/v1z/6iG+Ji6hulpmJOst67q3vD3Y/f80iq+F8vl/FthXvh6+CJw2qrWWNZxM8ZfYqbVl61av71CVpBZqWltGktQHw12SWshwl6QWMtwlqYUMd0lqIcNdklrIcJekFjLcJamF/j+oK7T1KCR6ZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_all2[300:],color='red')\n",
    "plt.plot(loss_all5[300:])\n",
    "plt.plot(loss_all[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95ea6eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2add3a336ac8>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_all[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb246854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dd2611f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b59cd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(test_data.x.to(device), test_data.edge_index.to(device))\n",
    "y_hat = y_hat.detach().numpy()\n",
    "y_hat = np.argmax(y_hat, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e00e2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_data.y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e5521108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.983720</td>\n",
       "      <td>0.922467</td>\n",
       "      <td>0.952109</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993444</td>\n",
       "      <td>0.998099</td>\n",
       "      <td>0.995766</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995793</td>\n",
       "      <td>0.998636</td>\n",
       "      <td>0.997212</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.809735</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.867984</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.957770</td>\n",
       "      <td>0.981513</td>\n",
       "      <td>0.969496</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.879592</td>\n",
       "      <td>0.974562</td>\n",
       "      <td>0.924645</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.970280</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.963821</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.964755</td>\n",
       "      <td>0.978788</td>\n",
       "      <td>0.971721</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.992306</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>0.994944</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.988581</td>\n",
       "      <td>0.981218</td>\n",
       "      <td>0.984885</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.602671</td>\n",
       "      <td>0.798673</td>\n",
       "      <td>0.686965</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.988635</td>\n",
       "      <td>0.988635</td>\n",
       "      <td>0.988635</td>\n",
       "      <td>0.988635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.675910</td>\n",
       "      <td>0.701617</td>\n",
       "      <td>0.687303</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.987946</td>\n",
       "      <td>0.988635</td>\n",
       "      <td>0.988130</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.983720  0.922467  0.952109   15000.000000\n",
       "1              0.993444  0.998099  0.995766   38408.000000\n",
       "2              0.995793  0.998636  0.997212   47641.000000\n",
       "3              0.809735  0.935264  0.867984     587.000000\n",
       "4              0.000000  0.000000  0.000000      11.000000\n",
       "5              0.957770  0.981513  0.969496    2380.000000\n",
       "6              0.879592  0.974562  0.924645    1769.000000\n",
       "7              0.970280  0.957447  0.963821    1739.000000\n",
       "8              0.964755  0.978788  0.971721    1650.000000\n",
       "9              0.992306  0.997595  0.994944   69037.000000\n",
       "10             0.988581  0.981218  0.984885    3088.000000\n",
       "11             0.000000  0.000000  0.000000       3.000000\n",
       "12             0.602671  0.798673  0.686965     452.000000\n",
       "13             0.000000  0.000000  0.000000     196.000000\n",
       "14             0.000000  0.000000  0.000000       6.000000\n",
       "accuracy       0.988635  0.988635  0.988635       0.988635\n",
       "macro avg      0.675910  0.701617  0.687303  181967.000000\n",
       "weighted avg   0.987946  0.988635  0.988130  181967.000000"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a652cb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.869778</td>\n",
       "      <td>0.936867</td>\n",
       "      <td>0.902077</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.974326</td>\n",
       "      <td>0.998959</td>\n",
       "      <td>0.986489</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999154</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.995396</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.032368</td>\n",
       "      <td>0.058193</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.942370</td>\n",
       "      <td>0.487815</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.864906</td>\n",
       "      <td>0.962691</td>\n",
       "      <td>0.911182</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.941756</td>\n",
       "      <td>0.660150</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.723935</td>\n",
       "      <td>0.967879</td>\n",
       "      <td>0.828320</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.993821</td>\n",
       "      <td>0.990121</td>\n",
       "      <td>0.991968</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.980819</td>\n",
       "      <td>0.977008</td>\n",
       "      <td>0.978910</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.602671</td>\n",
       "      <td>0.798673</td>\n",
       "      <td>0.686965</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.972847</td>\n",
       "      <td>0.972847</td>\n",
       "      <td>0.972847</td>\n",
       "      <td>0.972847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.612094</td>\n",
       "      <td>0.586946</td>\n",
       "      <td>0.583904</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.971357</td>\n",
       "      <td>0.972847</td>\n",
       "      <td>0.970233</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.869778  0.936867  0.902077   15000.000000\n",
       "1              0.974326  0.998959  0.986489   38408.000000\n",
       "2              0.999154  0.991667  0.995396   47641.000000\n",
       "3              0.287879  0.032368  0.058193     587.000000\n",
       "4              0.000000  0.000000  0.000000      11.000000\n",
       "5              0.942370  0.487815  0.642857    2380.000000\n",
       "6              0.864906  0.962691  0.911182    1769.000000\n",
       "7              0.941756  0.660150  0.776200    1739.000000\n",
       "8              0.723935  0.967879  0.828320    1650.000000\n",
       "9              0.993821  0.990121  0.991968   69037.000000\n",
       "10             0.980819  0.977008  0.978910    3088.000000\n",
       "11             0.000000  0.000000  0.000000       3.000000\n",
       "12             0.602671  0.798673  0.686965     452.000000\n",
       "13             0.000000  0.000000  0.000000     196.000000\n",
       "14             0.000000  0.000000  0.000000       6.000000\n",
       "accuracy       0.972847  0.972847  0.972847       0.972847\n",
       "macro avg      0.612094  0.586946  0.583904  181967.000000\n",
       "weighted avg   0.971357  0.972847  0.970233  181967.000000"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c59a627c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992756</td>\n",
       "      <td>0.972173</td>\n",
       "      <td>0.982357</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.997995</td>\n",
       "      <td>0.998060</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.982493</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.990054</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.383305</td>\n",
       "      <td>0.550122</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.976401</td>\n",
       "      <td>0.973529</td>\n",
       "      <td>0.974963</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.726964</td>\n",
       "      <td>0.967778</td>\n",
       "      <td>0.830262</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.925347</td>\n",
       "      <td>0.919494</td>\n",
       "      <td>0.922411</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.937576</td>\n",
       "      <td>0.929129</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.966387</td>\n",
       "      <td>0.997393</td>\n",
       "      <td>0.981645</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.953174</td>\n",
       "      <td>0.982189</td>\n",
       "      <td>0.967464</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.431211</td>\n",
       "      <td>0.464602</td>\n",
       "      <td>0.447284</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.981777</td>\n",
       "      <td>0.981777</td>\n",
       "      <td>0.981777</td>\n",
       "      <td>0.981777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.667626</td>\n",
       "      <td>0.661807</td>\n",
       "      <td>0.653065</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.981874</td>\n",
       "      <td>0.981777</td>\n",
       "      <td>0.981375</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.992756  0.972173  0.982357  150000.000000\n",
       "1              0.998125  0.997995  0.998060   38408.000000\n",
       "2              0.982493  0.997733  0.990054   47641.000000\n",
       "3              0.974026  0.383305  0.550122     587.000000\n",
       "4              0.000000  0.000000  0.000000      11.000000\n",
       "5              0.976401  0.973529  0.974963    2380.000000\n",
       "6              0.726964  0.967778  0.830262    1769.000000\n",
       "7              0.925347  0.919494  0.922411    1739.000000\n",
       "8              0.920833  0.937576  0.929129    1650.000000\n",
       "9              0.966387  0.997393  0.981645   69037.000000\n",
       "10             0.953174  0.982189  0.967464    3088.000000\n",
       "11             0.166667  0.333333  0.222222       3.000000\n",
       "12             0.431211  0.464602  0.447284     452.000000\n",
       "13             0.000000  0.000000  0.000000     196.000000\n",
       "14             0.000000  0.000000  0.000000       6.000000\n",
       "accuracy       0.981777  0.981777  0.981777       0.981777\n",
       "macro avg      0.667626  0.661807  0.653065  316967.000000\n",
       "weighted avg   0.981874  0.981777  0.981375  316967.000000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1161673c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993700</td>\n",
       "      <td>0.974847</td>\n",
       "      <td>0.984183</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999271</td>\n",
       "      <td>0.998828</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.985460</td>\n",
       "      <td>0.998720</td>\n",
       "      <td>0.992046</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.659284</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.986975</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.789220</td>\n",
       "      <td>0.927077</td>\n",
       "      <td>0.852612</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.970132</td>\n",
       "      <td>0.971248</td>\n",
       "      <td>0.970690</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.959644</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.969715</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.967860</td>\n",
       "      <td>0.998450</td>\n",
       "      <td>0.982917</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.986270</td>\n",
       "      <td>0.977008</td>\n",
       "      <td>0.981617</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.439080</td>\n",
       "      <td>0.422566</td>\n",
       "      <td>0.430665</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.984317</td>\n",
       "      <td>0.984317</td>\n",
       "      <td>0.984317</td>\n",
       "      <td>0.984317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.811182</td>\n",
       "      <td>0.722633</td>\n",
       "      <td>0.730966</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.984608</td>\n",
       "      <td>0.984317</td>\n",
       "      <td>0.984017</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.993700  0.974847  0.984183  150000.000000\n",
       "1              0.999271  0.998828  0.999049   38408.000000\n",
       "2              0.985460  0.998720  0.992046   47641.000000\n",
       "3              0.767857  0.659284  0.709441     587.000000\n",
       "4              0.333333  0.272727  0.300000      11.000000\n",
       "5              0.975904  0.986975  0.981408    2380.000000\n",
       "6              0.789220  0.927077  0.852612    1769.000000\n",
       "7              0.970132  0.971248  0.970690    1739.000000\n",
       "8              0.959644  0.980000  0.969715    1650.000000\n",
       "9              0.967860  0.998450  0.982917   69037.000000\n",
       "10             0.986270  0.977008  0.981617    3088.000000\n",
       "11             1.000000  0.666667  0.800000       3.000000\n",
       "12             0.439080  0.422566  0.430665     452.000000\n",
       "13             1.000000  0.005102  0.010152     196.000000\n",
       "14             0.000000  0.000000  0.000000       6.000000\n",
       "accuracy       0.984317  0.984317  0.984317       0.984317\n",
       "macro avg      0.811182  0.722633  0.730966  316967.000000\n",
       "weighted avg   0.984608  0.984317  0.984017  316967.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0e5411d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966215</td>\n",
       "      <td>0.992753</td>\n",
       "      <td>0.979304</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999713</td>\n",
       "      <td>0.999297</td>\n",
       "      <td>0.999505</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.987681</td>\n",
       "      <td>0.994605</td>\n",
       "      <td>0.991131</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.971963</td>\n",
       "      <td>0.708688</td>\n",
       "      <td>0.819704</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.997467</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>0.995157</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.967412</td>\n",
       "      <td>0.587337</td>\n",
       "      <td>0.730918</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.985690</td>\n",
       "      <td>0.990224</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.971839</td>\n",
       "      <td>0.983030</td>\n",
       "      <td>0.977403</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995700</td>\n",
       "      <td>0.945855</td>\n",
       "      <td>0.970138</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.994139</td>\n",
       "      <td>0.988666</td>\n",
       "      <td>0.991395</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.581858</td>\n",
       "      <td>0.552521</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>0.159292</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.979531</td>\n",
       "      <td>0.979531</td>\n",
       "      <td>0.979531</td>\n",
       "      <td>0.979531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.859493</td>\n",
       "      <td>0.730871</td>\n",
       "      <td>0.769184</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.979701</td>\n",
       "      <td>0.979531</td>\n",
       "      <td>0.978972</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.966215  0.992753  0.979304  150000.000000\n",
       "1              0.999713  0.999297  0.999505   38408.000000\n",
       "2              0.987681  0.994605  0.991131   47641.000000\n",
       "3              0.971963  0.708688  0.819704     587.000000\n",
       "4              0.428571  0.272727  0.333333      11.000000\n",
       "5              0.997467  0.992857  0.995157    2380.000000\n",
       "6              0.967412  0.587337  0.730918    1769.000000\n",
       "7              0.985690  0.990224  0.987952    1739.000000\n",
       "8              0.971839  0.983030  0.977403    1650.000000\n",
       "9              0.995700  0.945855  0.970138   69037.000000\n",
       "10             0.994139  0.988666  0.991395    3088.000000\n",
       "11             1.000000  0.666667  0.800000       3.000000\n",
       "12             0.526000  0.581858  0.552521     452.000000\n",
       "13             0.600000  0.091837  0.159292     196.000000\n",
       "14             0.500000  0.166667  0.250000       6.000000\n",
       "accuracy       0.979531  0.979531  0.979531       0.979531\n",
       "macro avg      0.859493  0.730871  0.769184  316967.000000\n",
       "weighted avg   0.979701  0.979531  0.978972  316967.000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ce4619d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973378</td>\n",
       "      <td>0.990373</td>\n",
       "      <td>0.981802</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.999427</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.987626</td>\n",
       "      <td>0.995130</td>\n",
       "      <td>0.991364</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.911654</td>\n",
       "      <td>0.826235</td>\n",
       "      <td>0.866845</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.996629</td>\n",
       "      <td>0.993697</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.936669</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.883513</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.986819</td>\n",
       "      <td>0.990224</td>\n",
       "      <td>0.988519</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.971856</td>\n",
       "      <td>0.983636</td>\n",
       "      <td>0.977711</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.993705</td>\n",
       "      <td>0.951157</td>\n",
       "      <td>0.971965</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.994139</td>\n",
       "      <td>0.988666</td>\n",
       "      <td>0.991395</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.472515</td>\n",
       "      <td>0.893805</td>\n",
       "      <td>0.618210</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.086735</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.981711</td>\n",
       "      <td>0.981711</td>\n",
       "      <td>0.981711</td>\n",
       "      <td>0.981711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.835523</td>\n",
       "      <td>0.776081</td>\n",
       "      <td>0.783455</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.982296</td>\n",
       "      <td>0.981711</td>\n",
       "      <td>0.981624</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.973378  0.990373  0.981802  150000.000000\n",
       "1              0.999661  0.999427  0.999544   38408.000000\n",
       "2              0.987626  0.995130  0.991364   47641.000000\n",
       "3              0.911654  0.826235  0.866845     587.000000\n",
       "4              0.428571  0.272727  0.333333      11.000000\n",
       "5              0.996629  0.993697  0.995161    2380.000000\n",
       "6              0.936669  0.836066  0.883513    1769.000000\n",
       "7              0.986819  0.990224  0.988519    1739.000000\n",
       "8              0.971856  0.983636  0.977711    1650.000000\n",
       "9              0.993705  0.951157  0.971965   69037.000000\n",
       "10             0.994139  0.988666  0.991395    3088.000000\n",
       "11             1.000000  0.666667  0.800000       3.000000\n",
       "12             0.472515  0.893805  0.618210     452.000000\n",
       "13             0.629630  0.086735  0.152466     196.000000\n",
       "14             0.250000  0.166667  0.200000       6.000000\n",
       "accuracy       0.981711  0.981711  0.981711       0.981711\n",
       "macro avg      0.835523  0.776081  0.783455  316967.000000\n",
       "weighted avg   0.982296  0.981711  0.981624  316967.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbdda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5948465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.974369</td>\n",
       "      <td>0.988887</td>\n",
       "      <td>0.981574</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999740</td>\n",
       "      <td>0.999245</td>\n",
       "      <td>0.999492</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.987328</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>0.993261</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.946309</td>\n",
       "      <td>0.720613</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.997890</td>\n",
       "      <td>0.993697</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.874652</td>\n",
       "      <td>0.887507</td>\n",
       "      <td>0.881033</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.988466</td>\n",
       "      <td>0.985624</td>\n",
       "      <td>0.987043</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.969406</td>\n",
       "      <td>0.979394</td>\n",
       "      <td>0.974374</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.993071</td>\n",
       "      <td>0.950809</td>\n",
       "      <td>0.971481</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.990263</td>\n",
       "      <td>0.988018</td>\n",
       "      <td>0.989139</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.442371</td>\n",
       "      <td>0.891593</td>\n",
       "      <td>0.591343</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.078049</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.981534</td>\n",
       "      <td>0.981534</td>\n",
       "      <td>0.981534</td>\n",
       "      <td>0.981534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.823517</td>\n",
       "      <td>0.757658</td>\n",
       "      <td>0.756432</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.982386</td>\n",
       "      <td>0.981534</td>\n",
       "      <td>0.981453</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.974369  0.988887  0.981574  150000.000000\n",
       "1              0.999740  0.999245  0.999492   38408.000000\n",
       "2              0.987328  0.999265  0.993261   47641.000000\n",
       "3              0.946309  0.720613  0.818182     587.000000\n",
       "4              0.300000  0.272727  0.285714      11.000000\n",
       "5              0.997890  0.993697  0.995789    2380.000000\n",
       "6              0.874652  0.887507  0.881033    1769.000000\n",
       "7              0.988466  0.985624  0.987043    1739.000000\n",
       "8              0.969406  0.979394  0.974374    1650.000000\n",
       "9              0.993071  0.950809  0.971481   69037.000000\n",
       "10             0.990263  0.988018  0.989139    3088.000000\n",
       "11             1.000000  0.666667  0.800000       3.000000\n",
       "12             0.442371  0.891593  0.591343     452.000000\n",
       "13             0.888889  0.040816  0.078049     196.000000\n",
       "14             0.000000  0.000000  0.000000       6.000000\n",
       "accuracy       0.981534  0.981534  0.981534       0.981534\n",
       "macro avg      0.823517  0.757658  0.756432  316967.000000\n",
       "weighted avg   0.982386  0.981534  0.981453  316967.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1c2e474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995819</td>\n",
       "      <td>0.974953</td>\n",
       "      <td>0.985276</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999297</td>\n",
       "      <td>0.999141</td>\n",
       "      <td>0.999219</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.986998</td>\n",
       "      <td>0.999097</td>\n",
       "      <td>0.993011</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.907285</td>\n",
       "      <td>0.933560</td>\n",
       "      <td>0.920235</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.965531</td>\n",
       "      <td>0.988655</td>\n",
       "      <td>0.976957</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.756506</td>\n",
       "      <td>0.969474</td>\n",
       "      <td>0.849851</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.980496</td>\n",
       "      <td>0.953997</td>\n",
       "      <td>0.967065</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.938799</td>\n",
       "      <td>0.985455</td>\n",
       "      <td>0.961561</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.967318</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.983290</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.989279</td>\n",
       "      <td>0.986075</td>\n",
       "      <td>0.987674</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.455128</td>\n",
       "      <td>0.314159</td>\n",
       "      <td>0.371728</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.985377</td>\n",
       "      <td>0.985377</td>\n",
       "      <td>0.985377</td>\n",
       "      <td>0.985377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.779497</td>\n",
       "      <td>0.736251</td>\n",
       "      <td>0.746391</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.985121</td>\n",
       "      <td>0.985377</td>\n",
       "      <td>0.985032</td>\n",
       "      <td>316967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.995819  0.974953  0.985276  150000.000000\n",
       "1              0.999297  0.999141  0.999219   38408.000000\n",
       "2              0.986998  0.999097  0.993011   47641.000000\n",
       "3              0.907285  0.933560  0.920235     587.000000\n",
       "4              0.750000  0.272727  0.400000      11.000000\n",
       "5              0.965531  0.988655  0.976957    2380.000000\n",
       "6              0.756506  0.969474  0.849851    1769.000000\n",
       "7              0.980496  0.953997  0.967065    1739.000000\n",
       "8              0.938799  0.985455  0.961561    1650.000000\n",
       "9              0.967318  0.999797  0.983290   69037.000000\n",
       "10             0.989279  0.986075  0.987674    3088.000000\n",
       "11             1.000000  0.666667  0.800000       3.000000\n",
       "12             0.455128  0.314159  0.371728     452.000000\n",
       "13             0.000000  0.000000  0.000000     196.000000\n",
       "14             0.000000  0.000000  0.000000       6.000000\n",
       "accuracy       0.985377  0.985377  0.985377       0.985377\n",
       "macro avg      0.779497  0.736251  0.746391  316967.000000\n",
       "weighted avg   0.985121  0.985377  0.985032  316967.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7114ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.945449</td>\n",
       "      <td>0.976333</td>\n",
       "      <td>0.960643</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997868</td>\n",
       "      <td>0.994939</td>\n",
       "      <td>0.996401</td>\n",
       "      <td>166967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.993405</td>\n",
       "      <td>0.993405</td>\n",
       "      <td>0.993405</td>\n",
       "      <td>0.993405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.971658</td>\n",
       "      <td>0.985636</td>\n",
       "      <td>0.978522</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.993547</td>\n",
       "      <td>0.993405</td>\n",
       "      <td>0.993454</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.945449  0.976333  0.960643   15000.000000\n",
       "1              0.997868  0.994939  0.996401  166967.000000\n",
       "accuracy       0.993405  0.993405  0.993405       0.993405\n",
       "macro avg      0.971658  0.985636  0.978522  181967.000000\n",
       "weighted avg   0.993547  0.993405  0.993454  181967.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22885e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966578</td>\n",
       "      <td>0.973667</td>\n",
       "      <td>0.970110</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997633</td>\n",
       "      <td>0.996975</td>\n",
       "      <td>0.997304</td>\n",
       "      <td>166967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.995054</td>\n",
       "      <td>0.995054</td>\n",
       "      <td>0.995054</td>\n",
       "      <td>0.995054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.982106</td>\n",
       "      <td>0.985321</td>\n",
       "      <td>0.983707</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.995073</td>\n",
       "      <td>0.995054</td>\n",
       "      <td>0.995062</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.966578  0.973667  0.970110   15000.000000\n",
       "1              0.997633  0.996975  0.997304  166967.000000\n",
       "accuracy       0.995054  0.995054  0.995054       0.995054\n",
       "macro avg      0.982106  0.985321  0.983707  181967.000000\n",
       "weighted avg   0.995073  0.995054  0.995062  181967.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db95867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdaa6d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.810716</td>\n",
       "      <td>0.973400</td>\n",
       "      <td>0.884641</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999792</td>\n",
       "      <td>0.999427</td>\n",
       "      <td>0.999609</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999685</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.825561</td>\n",
       "      <td>0.814310</td>\n",
       "      <td>0.819897</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994558</td>\n",
       "      <td>0.998319</td>\n",
       "      <td>0.996435</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.920703</td>\n",
       "      <td>0.977954</td>\n",
       "      <td>0.948465</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.993031</td>\n",
       "      <td>0.983324</td>\n",
       "      <td>0.988154</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.988540</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.990931</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.998029</td>\n",
       "      <td>0.953605</td>\n",
       "      <td>0.975311</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.992208</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.990921</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.913717</td>\n",
       "      <td>0.748867</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.025510</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.977337</td>\n",
       "      <td>0.977337</td>\n",
       "      <td>0.977337</td>\n",
       "      <td>0.977337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.893816</td>\n",
       "      <td>0.827311</td>\n",
       "      <td>0.827869</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.980480</td>\n",
       "      <td>0.977337</td>\n",
       "      <td>0.977730</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.810716  0.973400  0.884641   15000.000000\n",
       "1              0.999792  0.999427  0.999609   38408.000000\n",
       "2              0.999685  0.999244  0.999465   47641.000000\n",
       "3              0.825561  0.814310  0.819897     587.000000\n",
       "4              0.625000  0.454545  0.526316      11.000000\n",
       "5              0.994558  0.998319  0.996435    2380.000000\n",
       "6              0.920703  0.977954  0.948465    1769.000000\n",
       "7              0.993031  0.983324  0.988154    1739.000000\n",
       "8              0.988540  0.993333  0.990931    1650.000000\n",
       "9              0.998029  0.953605  0.975311   69037.000000\n",
       "10             0.992208  0.989637  0.990921    3088.000000\n",
       "11             1.000000  1.000000  1.000000       3.000000\n",
       "12             0.634409  0.913717  0.748867     452.000000\n",
       "13             0.625000  0.025510  0.049020     196.000000\n",
       "14             1.000000  0.333333  0.500000       6.000000\n",
       "accuracy       0.977337  0.977337  0.977337       0.977337\n",
       "macro avg      0.893816  0.827311  0.827869  181967.000000\n",
       "weighted avg   0.980480  0.977337  0.977730  181967.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b0449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5282abb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990507</td>\n",
       "      <td>0.966867</td>\n",
       "      <td>0.978544</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999818</td>\n",
       "      <td>0.999427</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999601</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.999423</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.954003</td>\n",
       "      <td>0.899598</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.995802</td>\n",
       "      <td>0.996639</td>\n",
       "      <td>0.996220</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.913730</td>\n",
       "      <td>0.981911</td>\n",
       "      <td>0.946594</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.991917</td>\n",
       "      <td>0.987924</td>\n",
       "      <td>0.989916</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.987923</td>\n",
       "      <td>0.991515</td>\n",
       "      <td>0.989716</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.996736</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.998192</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.994146</td>\n",
       "      <td>0.989961</td>\n",
       "      <td>0.992049</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.636503</td>\n",
       "      <td>0.918142</td>\n",
       "      <td>0.751812</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.994752</td>\n",
       "      <td>0.994752</td>\n",
       "      <td>0.994752</td>\n",
       "      <td>0.994752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.904802</td>\n",
       "      <td>0.827440</td>\n",
       "      <td>0.826204</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.994717</td>\n",
       "      <td>0.994752</td>\n",
       "      <td>0.994364</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.990507  0.966867  0.978544   15000.000000\n",
       "1              0.999818  0.999427  0.999622   38408.000000\n",
       "2              0.999601  0.999244  0.999423   47641.000000\n",
       "3              0.851064  0.954003  0.899598     587.000000\n",
       "4              0.714286  0.454545  0.555556      11.000000\n",
       "5              0.995802  0.996639  0.996220    2380.000000\n",
       "6              0.913730  0.981911  0.946594    1769.000000\n",
       "7              0.991917  0.987924  0.989916    1739.000000\n",
       "8              0.987923  0.991515  0.989716    1650.000000\n",
       "9              0.996736  0.999652  0.998192   69037.000000\n",
       "10             0.994146  0.989961  0.992049    3088.000000\n",
       "11             1.000000  1.000000  1.000000       3.000000\n",
       "12             0.636503  0.918142  0.751812     452.000000\n",
       "13             0.500000  0.005102  0.010101     196.000000\n",
       "14             1.000000  0.166667  0.285714       6.000000\n",
       "accuracy       0.994752  0.994752  0.994752       0.994752\n",
       "macro avg      0.904802  0.827440  0.826204  181967.000000\n",
       "weighted avg   0.994717  0.994752  0.994364  181967.000000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d402fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2975557a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.991852</td>\n",
       "      <td>0.965733</td>\n",
       "      <td>0.978618</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999870</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.999223</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850227</td>\n",
       "      <td>0.957411</td>\n",
       "      <td>0.900641</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994966</td>\n",
       "      <td>0.996639</td>\n",
       "      <td>0.995802</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.911904</td>\n",
       "      <td>0.983041</td>\n",
       "      <td>0.946137</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.991917</td>\n",
       "      <td>0.987924</td>\n",
       "      <td>0.989916</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.987326</td>\n",
       "      <td>0.991515</td>\n",
       "      <td>0.989416</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.996492</td>\n",
       "      <td>0.999942</td>\n",
       "      <td>0.998214</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.994793</td>\n",
       "      <td>0.989961</td>\n",
       "      <td>0.992371</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.640867</td>\n",
       "      <td>0.915929</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.994768</td>\n",
       "      <td>0.994768</td>\n",
       "      <td>0.994768</td>\n",
       "      <td>0.994768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.904939</td>\n",
       "      <td>0.827534</td>\n",
       "      <td>0.826374</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.994726</td>\n",
       "      <td>0.994768</td>\n",
       "      <td>0.994375</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.991852  0.965733  0.978618   15000.000000\n",
       "1              0.999870  0.999375  0.999622   38408.000000\n",
       "2              0.999580  0.999223  0.999402   47641.000000\n",
       "3              0.850227  0.957411  0.900641     587.000000\n",
       "4              0.714286  0.454545  0.555556      11.000000\n",
       "5              0.994966  0.996639  0.995802    2380.000000\n",
       "6              0.911904  0.983041  0.946137    1769.000000\n",
       "7              0.991917  0.987924  0.989916    1739.000000\n",
       "8              0.987326  0.991515  0.989416    1650.000000\n",
       "9              0.996492  0.999942  0.998214   69037.000000\n",
       "10             0.994793  0.989961  0.992371    3088.000000\n",
       "11             1.000000  1.000000  1.000000       3.000000\n",
       "12             0.640867  0.915929  0.754098     452.000000\n",
       "13             0.500000  0.005102  0.010101     196.000000\n",
       "14             1.000000  0.166667  0.285714       6.000000\n",
       "accuracy       0.994768  0.994768  0.994768       0.994768\n",
       "macro avg      0.904939  0.827534  0.826374  181967.000000\n",
       "weighted avg   0.994726  0.994768  0.994375  181967.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e60411d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.987666</td>\n",
       "      <td>0.939533</td>\n",
       "      <td>0.962998</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.999141</td>\n",
       "      <td>0.999310</td>\n",
       "      <td>38408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995668</td>\n",
       "      <td>0.998594</td>\n",
       "      <td>0.997129</td>\n",
       "      <td>47641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.893401</td>\n",
       "      <td>0.899489</td>\n",
       "      <td>0.896435</td>\n",
       "      <td>587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.985816</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>0.989324</td>\n",
       "      <td>2380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.975127</td>\n",
       "      <td>0.901725</td>\n",
       "      <td>1769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.969248</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.973963</td>\n",
       "      <td>1739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.979616</td>\n",
       "      <td>0.990303</td>\n",
       "      <td>0.984931</td>\n",
       "      <td>1650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995670</td>\n",
       "      <td>0.999218</td>\n",
       "      <td>0.997441</td>\n",
       "      <td>69037.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.976122</td>\n",
       "      <td>0.992876</td>\n",
       "      <td>0.984428</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.608347</td>\n",
       "      <td>0.838496</td>\n",
       "      <td>0.705116</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.991526</td>\n",
       "      <td>0.991526</td>\n",
       "      <td>0.991526</td>\n",
       "      <td>0.991526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.770864</td>\n",
       "      <td>0.751742</td>\n",
       "      <td>0.746857</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.991330</td>\n",
       "      <td>0.991526</td>\n",
       "      <td>0.991107</td>\n",
       "      <td>181967.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "0              0.987666  0.939533  0.962998   15000.000000\n",
       "1              0.999479  0.999141  0.999310   38408.000000\n",
       "2              0.995668  0.998594  0.997129   47641.000000\n",
       "3              0.893401  0.899489  0.896435     587.000000\n",
       "4              0.000000  0.000000  0.000000      11.000000\n",
       "5              0.985816  0.992857  0.989324    2380.000000\n",
       "6              0.838600  0.975127  0.901725    1769.000000\n",
       "7              0.969248  0.978723  0.973963    1739.000000\n",
       "8              0.979616  0.990303  0.984931    1650.000000\n",
       "9              0.995670  0.999218  0.997441   69037.000000\n",
       "10             0.976122  0.992876  0.984428    3088.000000\n",
       "11             1.000000  0.666667  0.800000       3.000000\n",
       "12             0.608347  0.838496  0.705116     452.000000\n",
       "13             0.333333  0.005102  0.010050     196.000000\n",
       "14             0.000000  0.000000  0.000000       6.000000\n",
       "accuracy       0.991526  0.991526  0.991526       0.991526\n",
       "macro avg      0.770864  0.751742  0.746857  181967.000000\n",
       "weighted avg   0.991330  0.991526  0.991107  181967.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a3b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ff30c3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995258</td>\n",
       "      <td>0.963852</td>\n",
       "      <td>0.979303</td>\n",
       "      <td>13500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968244</td>\n",
       "      <td>0.995850</td>\n",
       "      <td>0.981853</td>\n",
       "      <td>14941.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.980662</td>\n",
       "      <td>0.980662</td>\n",
       "      <td>0.980662</td>\n",
       "      <td>0.980662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.981751</td>\n",
       "      <td>0.979851</td>\n",
       "      <td>0.980578</td>\n",
       "      <td>28441.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.981066</td>\n",
       "      <td>0.980662</td>\n",
       "      <td>0.980643</td>\n",
       "      <td>28441.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.995258  0.963852  0.979303  13500.000000\n",
       "1              0.968244  0.995850  0.981853  14941.000000\n",
       "accuracy       0.980662  0.980662  0.980662      0.980662\n",
       "macro avg      0.981751  0.979851  0.980578  28441.000000\n",
       "weighted avg   0.981066  0.980662  0.980643  28441.000000"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6ecf2f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.902622</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.942775</td>\n",
       "      <td>13500.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.998166</td>\n",
       "      <td>3000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995684</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.997671</td>\n",
       "      <td>3000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.886525</td>\n",
       "      <td>0.426621</td>\n",
       "      <td>0.576037</td>\n",
       "      <td>293.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>5.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.989916</td>\n",
       "      <td>0.994093</td>\n",
       "      <td>1190.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.993435</td>\n",
       "      <td>0.512994</td>\n",
       "      <td>0.676602</td>\n",
       "      <td>885.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.989619</td>\n",
       "      <td>870.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.979641</td>\n",
       "      <td>0.991515</td>\n",
       "      <td>0.985542</td>\n",
       "      <td>825.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.948210</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.885176</td>\n",
       "      <td>3000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.996741</td>\n",
       "      <td>0.990285</td>\n",
       "      <td>0.993502</td>\n",
       "      <td>1544.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.061947</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>226.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.112150</td>\n",
       "      <td>98.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.941950</td>\n",
       "      <td>0.941950</td>\n",
       "      <td>0.941950</td>\n",
       "      <td>0.94195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.873071</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.740671</td>\n",
       "      <td>28441.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.941880</td>\n",
       "      <td>0.941950</td>\n",
       "      <td>0.934260</td>\n",
       "      <td>28441.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.902622  0.986667  0.942775  13500.00000\n",
       "1              0.998333  0.998000  0.998166   3000.00000\n",
       "2              0.995684  0.999667  0.997671   3000.00000\n",
       "3              0.886525  0.426621  0.576037    293.00000\n",
       "4              0.500000  0.400000  0.444444      5.00000\n",
       "5              0.998305  0.989916  0.994093   1190.00000\n",
       "6              0.993435  0.512994  0.676602    885.00000\n",
       "7              0.993056  0.986207  0.989619    870.00000\n",
       "8              0.979641  0.991515  0.985542    825.00000\n",
       "9              0.948210  0.830000  0.885176   3000.00000\n",
       "10             0.996741  0.990285  0.993502   1544.00000\n",
       "11             1.000000  1.000000  1.000000      2.00000\n",
       "12             0.736842  0.061947  0.114286    226.00000\n",
       "13             0.666667  0.061224  0.112150     98.00000\n",
       "14             0.500000  0.333333  0.400000      3.00000\n",
       "accuracy       0.941950  0.941950  0.941950      0.94195\n",
       "macro avg      0.873071  0.704558  0.740671  28441.00000\n",
       "weighted avg   0.941880  0.941950  0.934260  28441.00000"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4b0f4531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.983694</td>\n",
       "      <td>0.974148</td>\n",
       "      <td>0.978898</td>\n",
       "      <td>13500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998334</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.996013</td>\n",
       "      <td>0.999333</td>\n",
       "      <td>0.997671</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.724868</td>\n",
       "      <td>0.935154</td>\n",
       "      <td>0.816692</td>\n",
       "      <td>293.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997067</td>\n",
       "      <td>1190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.869177</td>\n",
       "      <td>0.990960</td>\n",
       "      <td>0.926082</td>\n",
       "      <td>885.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.983908</td>\n",
       "      <td>0.987313</td>\n",
       "      <td>870.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.990303</td>\n",
       "      <td>0.983745</td>\n",
       "      <td>825.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.982261</td>\n",
       "      <td>0.941333</td>\n",
       "      <td>0.961362</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.993536</td>\n",
       "      <td>0.995466</td>\n",
       "      <td>0.994500</td>\n",
       "      <td>1544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.623907</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.752197</td>\n",
       "      <td>226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.975563</td>\n",
       "      <td>0.975563</td>\n",
       "      <td>0.975563</td>\n",
       "      <td>0.975563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.942264</td>\n",
       "      <td>0.850049</td>\n",
       "      <td>0.850627</td>\n",
       "      <td>28441.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.978361</td>\n",
       "      <td>0.975563</td>\n",
       "      <td>0.974920</td>\n",
       "      <td>28441.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.983694  0.974148  0.978898  13500.000000\n",
       "1              0.998334  0.998667  0.998500   3000.000000\n",
       "2              0.996013  0.999333  0.997671   3000.000000\n",
       "3              0.724868  0.935154  0.816692    293.000000\n",
       "4              1.000000  0.600000  0.750000      5.000000\n",
       "5              0.994152  1.000000  0.997067   1190.000000\n",
       "6              0.869177  0.990960  0.926082    885.000000\n",
       "7              0.990741  0.983908  0.987313    870.000000\n",
       "8              0.977273  0.990303  0.983745    825.000000\n",
       "9              0.982261  0.941333  0.961362   3000.000000\n",
       "10             0.993536  0.995466  0.994500   1544.000000\n",
       "11             1.000000  1.000000  1.000000      2.000000\n",
       "12             0.623907  0.946903  0.752197    226.000000\n",
       "13             1.000000  0.061224  0.115385     98.000000\n",
       "14             1.000000  0.333333  0.500000      3.000000\n",
       "accuracy       0.975563  0.975563  0.975563      0.975563\n",
       "macro avg      0.942264  0.850049  0.850627  28441.000000\n",
       "weighted avg   0.978361  0.975563  0.974920  28441.000000"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59d860d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993342</td>\n",
       "      <td>0.950370</td>\n",
       "      <td>0.971381</td>\n",
       "      <td>13500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993369</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.996011</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995346</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.996671</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.686717</td>\n",
       "      <td>0.935154</td>\n",
       "      <td>0.791908</td>\n",
       "      <td>293.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.983471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.849462</td>\n",
       "      <td>0.981921</td>\n",
       "      <td>0.910901</td>\n",
       "      <td>885.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.984813</td>\n",
       "      <td>0.968966</td>\n",
       "      <td>0.976825</td>\n",
       "      <td>870.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.972586</td>\n",
       "      <td>0.989091</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>825.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.908123</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.949578</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.983964</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.988721</td>\n",
       "      <td>1544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.057522</td>\n",
       "      <td>0.107884</td>\n",
       "      <td>226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.867347</td>\n",
       "      <td>0.420792</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.964629</td>\n",
       "      <td>0.964629</td>\n",
       "      <td>0.964629</td>\n",
       "      <td>0.964629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.883042</td>\n",
       "      <td>0.844593</td>\n",
       "      <td>0.816652</td>\n",
       "      <td>28441.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.971632</td>\n",
       "      <td>0.964629</td>\n",
       "      <td>0.963985</td>\n",
       "      <td>28441.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.993342  0.950370  0.971381  13500.000000\n",
       "1              0.993369  0.998667  0.996011   3000.000000\n",
       "2              0.995346  0.998000  0.996671   3000.000000\n",
       "3              0.686717  0.935154  0.791908    293.000000\n",
       "4              0.750000  0.600000  0.666667      5.000000\n",
       "5              0.983471  1.000000  0.991667   1190.000000\n",
       "6              0.849462  0.981921  0.910901    885.000000\n",
       "7              0.984813  0.968966  0.976825    870.000000\n",
       "8              0.972586  0.989091  0.980769    825.000000\n",
       "9              0.908123  0.995000  0.949578   3000.000000\n",
       "10             0.983964  0.993523  0.988721   1544.000000\n",
       "11             1.000000  1.000000  1.000000      2.000000\n",
       "12             0.866667  0.057522  0.107884    226.000000\n",
       "13             0.277778  0.867347  0.420792     98.000000\n",
       "14             1.000000  0.333333  0.500000      3.000000\n",
       "accuracy       0.964629  0.964629  0.964629      0.964629\n",
       "macro avg      0.883042  0.844593  0.816652  28441.000000\n",
       "weighted avg   0.971632  0.964629  0.963985  28441.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1c85da15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.899637</td>\n",
       "      <td>0.918296</td>\n",
       "      <td>0.908871</td>\n",
       "      <td>13500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.924766</td>\n",
       "      <td>0.907436</td>\n",
       "      <td>0.916019</td>\n",
       "      <td>14941.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.912591</td>\n",
       "      <td>0.912591</td>\n",
       "      <td>0.912591</td>\n",
       "      <td>0.912591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.912202</td>\n",
       "      <td>0.912866</td>\n",
       "      <td>0.912445</td>\n",
       "      <td>28441.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.912838</td>\n",
       "      <td>0.912591</td>\n",
       "      <td>0.912626</td>\n",
       "      <td>28441.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.899637  0.918296  0.908871  13500.000000\n",
       "1              0.924766  0.907436  0.916019  14941.000000\n",
       "accuracy       0.912591  0.912591  0.912591      0.912591\n",
       "macro avg      0.912202  0.912866  0.912445  28441.000000\n",
       "weighted avg   0.912838  0.912591  0.912626  28441.000000"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39d1440f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.523932</td>\n",
       "      <td>0.952333</td>\n",
       "      <td>0.675973</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.889341</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.953615</td>\n",
       "      <td>0.993667</td>\n",
       "      <td>0.973229</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.358362</td>\n",
       "      <td>0.523691</td>\n",
       "      <td>293.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998724</td>\n",
       "      <td>0.657983</td>\n",
       "      <td>0.793313</td>\n",
       "      <td>1190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>885.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.974114</td>\n",
       "      <td>0.821839</td>\n",
       "      <td>0.891521</td>\n",
       "      <td>870.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.985384</td>\n",
       "      <td>0.980606</td>\n",
       "      <td>0.982989</td>\n",
       "      <td>825.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.989789</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.962671</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.868773</td>\n",
       "      <td>0.926166</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>1544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.039823</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.830779</td>\n",
       "      <td>0.830779</td>\n",
       "      <td>0.830779</td>\n",
       "      <td>0.830779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.645520</td>\n",
       "      <td>0.564585</td>\n",
       "      <td>0.564327</td>\n",
       "      <td>17941.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.838679</td>\n",
       "      <td>0.830779</td>\n",
       "      <td>0.813259</td>\n",
       "      <td>17941.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.523932  0.952333  0.675973   3000.000000\n",
       "1              0.999584  0.801000  0.889341   3000.000000\n",
       "2              0.953615  0.993667  0.973229   3000.000000\n",
       "3              0.972222  0.358362  0.523691    293.000000\n",
       "4              0.000000  0.000000  0.000000      5.000000\n",
       "5              0.998724  0.657983  0.793313   1190.000000\n",
       "6              0.000000  0.000000  0.000000    885.000000\n",
       "7              0.974114  0.821839  0.891521    870.000000\n",
       "8              0.985384  0.980606  0.982989    825.000000\n",
       "9              0.989789  0.937000  0.962671   3000.000000\n",
       "10             0.868773  0.926166  0.896552   1544.000000\n",
       "11             0.666667  1.000000  0.800000      2.000000\n",
       "12             0.750000  0.039823  0.075630    226.000000\n",
       "13             0.000000  0.000000  0.000000     98.000000\n",
       "14             0.000000  0.000000  0.000000      3.000000\n",
       "accuracy       0.830779  0.830779  0.830779      0.830779\n",
       "macro avg      0.645520  0.564585  0.564327  17941.000000\n",
       "weighted avg   0.838679  0.830779  0.813259  17941.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr = classification_report(test_y, y_hat, output_dict=True)\n",
    "df = pd.DataFrame(cr).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af2cbf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 34.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 0 ... 0 0 0]\n",
      "tensor([2, 0, 0,  ..., 0, 0, 0])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8596    0.9609    0.9075     13500\n",
      "           1     0.9931    0.9647    0.9787      3000\n",
      "           2     0.9226    0.9927    0.9563      3000\n",
      "           3     0.6329    0.9590    0.7626       293\n",
      "           4     1.0000    0.2000    0.3333         5\n",
      "           5     0.9838    0.5109    0.6726      1190\n",
      "           6     0.6667    0.0045    0.0090       885\n",
      "           7     0.9647    0.9747    0.9697       870\n",
      "           8     0.9794    0.9818    0.9806       825\n",
      "           9     0.9849    0.8673    0.9224      3000\n",
      "          10     0.9980    0.9462    0.9714      1544\n",
      "          11     0.0000    0.0000    0.0000         2\n",
      "          12     0.5710    0.8186    0.6727       226\n",
      "          13     0.0000    0.0000    0.0000        98\n",
      "          14     0.0000    0.0000    0.0000         3\n",
      "\n",
      "    accuracy                         0.9017     28441\n",
      "   macro avg     0.7038    0.6121    0.6091     28441\n",
      "weighted avg     0.8992    0.9017    0.8847     28441\n",
      "\n",
      "[[12972     5   246   159     0    10     1    11     4    39     3     0\n",
      "     46     4     0]\n",
      " [  105  2894     0     0     0     0     0     0     0     1     0     0\n",
      "      0     0     0]\n",
      " [    2    13  2978     4     0     0     0     2     0     0     0     0\n",
      "      1     0     0]\n",
      " [   12     0     0   281     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    4     0     0     0     1     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [  576     0     0     0     0   608     1     5     0     0     0     0\n",
      "      0     0     0]\n",
      " [  877     0     3     0     0     0     4     1     0     0     0     0\n",
      "      0     0     0]\n",
      " [   10     0     0     0     0     0     0   848    11     0     0     0\n",
      "      1     0     0]\n",
      " [    3     0     0     0     0     0     0    12   810     0     0     0\n",
      "      0     0     0]\n",
      " [  396     2     0     0     0     0     0     0     0  2602     0     0\n",
      "      0     0     0]\n",
      " [   81     0     0     0     0     0     0     0     2     0  1461     0\n",
      "      0     0     0]\n",
      " [    2     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [   41     0     0     0     0     0     0     0     0     0     0     0\n",
      "    185     0     0]\n",
      " [    6     0     1     0     0     0     0     0     0     0     0     0\n",
      "     91     0     0]\n",
      " [    3     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "inferrence(model, subgraph_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "98ba4a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      " 18%|█▊        | 4/22 [00:00<00:00, 31.91it/s]\u001b[A\n",
      " 36%|███▋      | 8/22 [00:00<00:00, 31.96it/s]\u001b[A\n",
      " 55%|█████▍    | 12/22 [00:00<00:00, 32.22it/s]\u001b[A\n",
      " 73%|███████▎  | 16/22 [00:00<00:00, 32.57it/s]\u001b[A\n",
      "100%|██████████| 22/22 [00:00<00:00, 32.48it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 1 0 2]\n",
      "tensor([0, 1, 2,  ..., 1, 0, 2])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9778    0.9412    0.9592      7500\n",
      "           1     0.9793    0.9893    0.9843      9240\n",
      "           2     0.9652    0.9890    0.9770      3000\n",
      "           3     0.8594    0.9354    0.8958       294\n",
      "           4     1.0000    0.4000    0.5714         5\n",
      "           5     0.9448    0.9889    0.9663      2075\n",
      "           6     0.9156    0.8960    0.9057       327\n",
      "\n",
      "    accuracy                         0.9709     22441\n",
      "   macro avg     0.9489    0.8771    0.8942     22441\n",
      "weighted avg     0.9712    0.9709    0.9709     22441\n",
      "\n",
      "[[7059  173  104   45    0   94   25]\n",
      " [  95 9141    0    0    0    3    1]\n",
      " [  25    7 2967    0    0    0    1]\n",
      " [  19    0    0  275    0    0    0]\n",
      " [   2    1    0    0    2    0    0]\n",
      " [  15    6    2    0    0 2052    0]\n",
      " [   4    6    1    0    0   23  293]]\n"
     ]
    }
   ],
   "source": [
    "inferrence(model, subgraph_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ec35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e0d88f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:01<00:00, 17.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 0]\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8286    0.9610    0.8899     13500\n",
      "           1     0.9589    0.8204    0.8842     14941\n",
      "\n",
      "    accuracy                         0.8871     28441\n",
      "   macro avg     0.8937    0.8907    0.8871     28441\n",
      "weighted avg     0.8970    0.8871    0.8869     28441\n",
      "\n",
      "[[12974   526]\n",
      " [ 2684 12257]]\n"
     ]
    }
   ],
   "source": [
    "inferrence(model, subgraph_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "80534aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:05<00:00,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 1 0 2]\n",
      "tensor([0, 1, 2,  ..., 1, 0, 2])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9142    0.9667    0.9397      7500\n",
      "           1     0.9942    0.9661    0.9800      9240\n",
      "           2     0.9657    0.9753    0.9705      3000\n",
      "           3     0.8132    0.9626    0.8816       294\n",
      "           4     1.0000    0.6000    0.7500         5\n",
      "           5     0.9780    0.8578    0.9140      2075\n",
      "           6     0.9366    0.9480    0.9422       327\n",
      "\n",
      "    accuracy                         0.9571     22441\n",
      "   macro avg     0.9431    0.8967    0.9111     22441\n",
      "weighted avg     0.9590    0.9571    0.9573     22441\n",
      "\n",
      "[[7250   45  104   65    0   17   19]\n",
      " [ 312 8927    0    0    0    0    1]\n",
      " [  51    1 2926    0    0   21    1]\n",
      " [  11    0    0  283    0    0    0]\n",
      " [   1    1    0    0    3    0    0]\n",
      " [ 292    3    0    0    0 1780    0]\n",
      " [  13    2    0    0    0    2  310]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inferrence(model, subgraph_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "7e71bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 1 0 2]\n",
      "tensor([0, 1, 2,  ..., 1, 0, 2])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9908    0.9437    0.9667      7500\n",
      "           1     0.9826    0.9963    0.9894      9240\n",
      "           2     0.9646    0.9890    0.9766      3000\n",
      "           3     0.8304    0.9660    0.8931       294\n",
      "           4     1.0000    0.6000    0.7500         5\n",
      "           5     0.9462    0.9908    0.9680      2075\n",
      "           6     0.9162    0.9358    0.9259       327\n",
      "\n",
      "    accuracy                         0.9759     22441\n",
      "   macro avg     0.9472    0.9174    0.9242     22441\n",
      "weighted avg     0.9766    0.9759    0.9759     22441\n",
      "\n",
      "[[7078  148  105   58    0   86   25]\n",
      " [  18 9206    0    0    0   15    1]\n",
      " [  20    7 2967    0    0    5    1]\n",
      " [  10    0    0  284    0    0    0]\n",
      " [   1    1    0    0    3    0    0]\n",
      " [  11    4    3    0    0 2056    1]\n",
      " [   6    3    1    0    0   11  306]]\n"
     ]
    }
   ],
   "source": [
    "inferrence(model, subgraph_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "f8805c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:00, 24.53it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:00, 28.59it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:00<00:00, 29.74it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:00<00:00, 30.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 6 ... 1 1 7]\n",
      "tensor([2, 1, 6,  ..., 1, 1, 7])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7709    0.9249    0.8409      2998\n",
      "           1     0.9997    0.9750    0.9872      3000\n",
      "           2     0.9800    0.9987    0.9893      2998\n",
      "           3     0.7153    0.7201    0.7177       293\n",
      "           4     0.0769    0.4000    0.1290         5\n",
      "           5     0.7298    0.1748    0.2820      1190\n",
      "           6     0.3454    0.5186    0.4146       885\n",
      "           7     0.9564    0.9069    0.9310       870\n",
      "           8     0.9768    0.9697    0.9732       825\n",
      "           9     0.9665    0.9454    0.9558      2987\n",
      "          10     0.9980    0.9812    0.9895      1544\n",
      "          11     0.0000    0.0000    0.0000         2\n",
      "\n",
      "    accuracy                         0.8808     17597\n",
      "   macro avg     0.7096    0.7096    0.6842     17597\n",
      "weighted avg     0.8921    0.8808    0.8727     17597\n",
      "\n",
      "[[2773    1   60   83   24    7   13    5    6   24    2    0]\n",
      " [   2 2925    0    0    0    0    0    0    0   73    0    0]\n",
      " [   4    0 2994    0    0    0    0    0    0    0    0    0]\n",
      " [  82    0    0  211    0    0    0    0    0    0    0    0]\n",
      " [   2    0    0    1    2    0    0    0    0    0    0    0]\n",
      " [ 125    0    0    0    0  208  857    0    0    0    0    0]\n",
      " [ 424    0    0    0    0    1  459    0    1    0    0    0]\n",
      " [   3    0    1    0    0   69    0  789    7    0    1    0]\n",
      " [   4    0    0    0    0    0    0   21  800    0    0    0]\n",
      " [ 163    0    0    0    0    0    0    0    0 2824    0    0]\n",
      " [  13    0    0    0    0    0    0   10    5    1 1515    0]\n",
      " [   2    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inferrence(model, subgraph_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "0b3d6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(test_data.x.to(device), test_data.edge_index.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "4bfa0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = y_hat.detach().numpy()\n",
    "y_hat = np.argmax(y_hat, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "1a15f817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22441"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "61fe6ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0     0.9189    0.9665    0.9421      7500\\n           1     0.9941    0.9661    0.9799      9240\\n           2     0.9649    0.9907    0.9776      3000\\n           3     0.8109    0.9626    0.8802       294\\n           4     1.0000    0.6000    0.7500         5\\n           5     0.9873    0.8607    0.9197      2075\\n           6     0.9366    0.9480    0.9422       327\\n\\n    accuracy                         0.9594     22441\\n   macro avg     0.9447    0.8992    0.9131     22441\\nweighted avg     0.9612    0.9594    0.9595     22441\\n'"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(test_y, y_hat, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3245257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/186 [46:34<?, ?it/s]\n",
      " 16%|█▌        | 9/57 [45:49<4:04:26, 305.55s/it]\n",
      "  0%|          | 0/186 [46:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 8703520240 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-89075c8c16d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-408e37ca4035>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# block layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;31m# transition layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_geometric/nn/models/deepgcn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor, alpha: OptPairTensor, edge_attr: OptTensor)  # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         out = self.propagate(edge_index, x=x, alpha=alpha, edge_attr=edge_attr,\n\u001b[0;32m--> 242\u001b[0;31m                              size=size)\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                         \u001b[0mmsg_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, x_j, alpha_j, alpha_i, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m  \u001b[0;31m# Save for later use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 8703520240 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "y_hat = model(all_data.x.to(device), all_data.edge_index.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621507c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = y_hat.detach().numpy()\n",
    "y_hat = np.argmax(y_hat, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "06ab3312",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 171153 is out of bounds for axis 0 with size 28441",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-fb1895e79195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 171153 is out of bounds for axis 0 with size 28441"
     ]
    }
   ],
   "source": [
    "classification_report(test_y, y_hat[all_data.test_mask], digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c217d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50868b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3fbba2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "43a1197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_loader2 = NeighborLoader(copy.copy(all_data), input_nodes=None,num_neighbors=[-1], batch_size=1024, shuffle=False)\n",
    "subgraph_loader2.data.num_nodes = all_data.num_nodes\n",
    "subgraph_loader2.data.n_id = torch.arange(all_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bed2801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferrence2(model, subgraph_loader):\n",
    "    pbar = tqdm(total=len(subgraph_loader))\n",
    "    for i, batch in enumerate(subgraph_loader):\n",
    "        batch = batch.to(device)\n",
    "        y_hat = model(batch.x.to(device), batch.edge_index.to(device))[:batch.batch_size]\n",
    "        np.save('unsw_yhat/y_hat_batch' + str(i), y_hat.detach().numpy())\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d7112f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, loader):\n",
    "    xs = []\n",
    "    for i in range(len(loader)):\n",
    "        y_hat = np.load('unsw_yhat/y_hat_batch' + str(i) + '.npy')\n",
    "        xs.append(y_hat)\n",
    "    y_hat = np.concatenate(xs, 0)\n",
    "    y_hat = np.argmax(y_hat, -1)\n",
    "    y = data.y.to(device)\n",
    "    test_mask = data.test_mask \n",
    "    f1 = f1_score(y[test_mask], y_hat[test_mask], average='weighted')\n",
    "    pr = precision_score(y[test_mask], y_hat[test_mask], average='weighted')\n",
    "    cr1 = classification_report(y[test_mask], y_hat[test_mask], digits=4)\n",
    "    cf = confusion_matrix(y[test_mask], y_hat[test_mask])\n",
    "    print(\"测试集结果：\")\n",
    "    print(cr1)\n",
    "    print(cf)\n",
    "    \n",
    "    train_mask = data.train_mask\n",
    "    f1 = f1_score(y[train_mask], y_hat[train_mask], average='weighted')\n",
    "    pr = precision_score(y[train_mask], y_hat[train_mask], average='weighted')\n",
    "    cr1 = classification_report(y[train_mask], y_hat[train_mask], digits=4)\n",
    "    cf = confusion_matrix(y[train_mask], y_hat[train_mask])\n",
    "    print(\"训练集结果：\")\n",
    "    print(cr1)\n",
    "    print(cf)\n",
    "    return f1, pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "30bd95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fc91dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0a49c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 102/186 [08:03<17:47, 12.71s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-4a87b21a712f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minferrence2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraph_loader2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-139-432df5853983>\u001b[0m in \u001b[0;36minferrence2\u001b[0;34m(model, subgraph_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubgraph_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unsw_yhat/y_hat_batch'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-408e37ca4035>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# block layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;31m# transition layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_geometric/nn/models/deepgcn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor, alpha: OptPairTensor, edge_attr: OptTensor)  # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         out = self.propagate(edge_index, x=x, alpha=alpha, edge_attr=edge_attr,\n\u001b[0;32m--> 242\u001b[0;31m                              size=size)\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                         \u001b[0maggr_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size,\n\u001b[0;32m--> 426\u001b[0;31m                            reduce=self.aggr)\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmessage_and_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/densegat2/lib/python3.6/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter_max\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         dim_size: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_scatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inferrence2(model, subgraph_loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68367c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(all_data, subgraph_loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222a09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3ba6d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_label = all_data.y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eecc3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_label[all_data_label != 0] = 1\n",
    "all_data.y = torch.LongTensor(all_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25f5f10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[189603, 78], edge_index=[2, 108609698], y=[189603], num_nodes=189603, train_mask=[161162], test_mask=[28441])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ade40f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_loader2 = NeighborLoader(all_data, input_nodes=all_data.test_mask,\n",
    "                                 num_neighbors=hop, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1f004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ab479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eee73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5419f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to('cpu')\n",
    "test_data = test_data.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ff233d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.x.numpy()\n",
    "y_train = train_data.y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "808b469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_data.x.numpy()\n",
    "y_test = test_data.y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "721cbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ecb99a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._forest import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f458dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc = CatBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bc670e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.4805458\ttotal: 55ms\tremaining: 5.45s\n",
      "1:\tlearn: 0.3225481\ttotal: 87.9ms\tremaining: 4.31s\n",
      "2:\tlearn: 0.2275914\ttotal: 121ms\tremaining: 3.91s\n",
      "3:\tlearn: 0.1780775\ttotal: 154ms\tremaining: 3.69s\n",
      "4:\tlearn: 0.1261871\ttotal: 190ms\tremaining: 3.61s\n",
      "5:\tlearn: 0.1100746\ttotal: 220ms\tremaining: 3.45s\n",
      "6:\tlearn: 0.0897102\ttotal: 252ms\tremaining: 3.34s\n",
      "7:\tlearn: 0.0745446\ttotal: 287ms\tremaining: 3.3s\n",
      "8:\tlearn: 0.0665954\ttotal: 321ms\tremaining: 3.24s\n",
      "9:\tlearn: 0.0586550\ttotal: 357ms\tremaining: 3.21s\n",
      "10:\tlearn: 0.0536122\ttotal: 387ms\tremaining: 3.13s\n",
      "11:\tlearn: 0.0494401\ttotal: 428ms\tremaining: 3.14s\n",
      "12:\tlearn: 0.0446124\ttotal: 465ms\tremaining: 3.11s\n",
      "13:\tlearn: 0.0434772\ttotal: 492ms\tremaining: 3.02s\n",
      "14:\tlearn: 0.0411160\ttotal: 526ms\tremaining: 2.98s\n",
      "15:\tlearn: 0.0390528\ttotal: 555ms\tremaining: 2.91s\n",
      "16:\tlearn: 0.0379762\ttotal: 585ms\tremaining: 2.86s\n",
      "17:\tlearn: 0.0357098\ttotal: 614ms\tremaining: 2.79s\n",
      "18:\tlearn: 0.0334713\ttotal: 646ms\tremaining: 2.75s\n",
      "19:\tlearn: 0.0319717\ttotal: 678ms\tremaining: 2.71s\n",
      "20:\tlearn: 0.0300995\ttotal: 711ms\tremaining: 2.67s\n",
      "21:\tlearn: 0.0296231\ttotal: 740ms\tremaining: 2.62s\n",
      "22:\tlearn: 0.0285910\ttotal: 773ms\tremaining: 2.59s\n",
      "23:\tlearn: 0.0277417\ttotal: 800ms\tremaining: 2.53s\n",
      "24:\tlearn: 0.0269406\ttotal: 828ms\tremaining: 2.48s\n",
      "25:\tlearn: 0.0262116\ttotal: 860ms\tremaining: 2.45s\n",
      "26:\tlearn: 0.0256581\ttotal: 891ms\tremaining: 2.41s\n",
      "27:\tlearn: 0.0250848\ttotal: 919ms\tremaining: 2.36s\n",
      "28:\tlearn: 0.0235606\ttotal: 951ms\tremaining: 2.33s\n",
      "29:\tlearn: 0.0233724\ttotal: 978ms\tremaining: 2.28s\n",
      "30:\tlearn: 0.0228936\ttotal: 1.01s\tremaining: 2.24s\n",
      "31:\tlearn: 0.0219604\ttotal: 1.04s\tremaining: 2.21s\n",
      "32:\tlearn: 0.0209287\ttotal: 1.08s\tremaining: 2.19s\n",
      "33:\tlearn: 0.0202780\ttotal: 1.11s\tremaining: 2.15s\n",
      "34:\tlearn: 0.0200038\ttotal: 1.14s\tremaining: 2.11s\n",
      "35:\tlearn: 0.0192852\ttotal: 1.17s\tremaining: 2.08s\n",
      "36:\tlearn: 0.0191385\ttotal: 1.19s\tremaining: 2.03s\n",
      "37:\tlearn: 0.0186846\ttotal: 1.23s\tremaining: 2s\n",
      "38:\tlearn: 0.0183574\ttotal: 1.25s\tremaining: 1.96s\n",
      "39:\tlearn: 0.0179263\ttotal: 1.28s\tremaining: 1.93s\n",
      "40:\tlearn: 0.0176212\ttotal: 1.31s\tremaining: 1.89s\n",
      "41:\tlearn: 0.0171803\ttotal: 1.35s\tremaining: 1.86s\n",
      "42:\tlearn: 0.0168918\ttotal: 1.38s\tremaining: 1.83s\n",
      "43:\tlearn: 0.0161302\ttotal: 1.41s\tremaining: 1.8s\n",
      "44:\tlearn: 0.0156559\ttotal: 1.44s\tremaining: 1.76s\n",
      "45:\tlearn: 0.0154449\ttotal: 1.47s\tremaining: 1.73s\n",
      "46:\tlearn: 0.0152719\ttotal: 1.5s\tremaining: 1.69s\n",
      "47:\tlearn: 0.0148754\ttotal: 1.53s\tremaining: 1.66s\n",
      "48:\tlearn: 0.0145870\ttotal: 1.56s\tremaining: 1.63s\n",
      "49:\tlearn: 0.0145323\ttotal: 1.59s\tremaining: 1.59s\n",
      "50:\tlearn: 0.0144368\ttotal: 1.62s\tremaining: 1.56s\n",
      "51:\tlearn: 0.0143171\ttotal: 1.65s\tremaining: 1.52s\n",
      "52:\tlearn: 0.0139345\ttotal: 1.69s\tremaining: 1.49s\n",
      "53:\tlearn: 0.0138489\ttotal: 1.71s\tremaining: 1.46s\n",
      "54:\tlearn: 0.0136396\ttotal: 1.75s\tremaining: 1.43s\n",
      "55:\tlearn: 0.0134880\ttotal: 1.78s\tremaining: 1.4s\n",
      "56:\tlearn: 0.0134157\ttotal: 1.8s\tremaining: 1.36s\n",
      "57:\tlearn: 0.0133244\ttotal: 1.83s\tremaining: 1.33s\n",
      "58:\tlearn: 0.0131619\ttotal: 1.86s\tremaining: 1.29s\n",
      "59:\tlearn: 0.0130113\ttotal: 1.89s\tremaining: 1.26s\n",
      "60:\tlearn: 0.0128069\ttotal: 1.93s\tremaining: 1.23s\n",
      "61:\tlearn: 0.0124870\ttotal: 1.97s\tremaining: 1.21s\n",
      "62:\tlearn: 0.0123165\ttotal: 2s\tremaining: 1.18s\n",
      "63:\tlearn: 0.0122843\ttotal: 2.03s\tremaining: 1.14s\n",
      "64:\tlearn: 0.0121027\ttotal: 2.06s\tremaining: 1.11s\n",
      "65:\tlearn: 0.0119516\ttotal: 2.09s\tremaining: 1.08s\n",
      "66:\tlearn: 0.0118390\ttotal: 2.12s\tremaining: 1.04s\n",
      "67:\tlearn: 0.0118207\ttotal: 2.15s\tremaining: 1.01s\n",
      "68:\tlearn: 0.0116767\ttotal: 2.18s\tremaining: 979ms\n",
      "69:\tlearn: 0.0116590\ttotal: 2.21s\tremaining: 946ms\n",
      "70:\tlearn: 0.0113757\ttotal: 2.24s\tremaining: 914ms\n",
      "71:\tlearn: 0.0113256\ttotal: 2.27s\tremaining: 882ms\n",
      "72:\tlearn: 0.0113149\ttotal: 2.29s\tremaining: 848ms\n",
      "73:\tlearn: 0.0112841\ttotal: 2.32s\tremaining: 815ms\n",
      "74:\tlearn: 0.0112085\ttotal: 2.35s\tremaining: 783ms\n",
      "75:\tlearn: 0.0111667\ttotal: 2.38s\tremaining: 751ms\n",
      "76:\tlearn: 0.0109876\ttotal: 2.41s\tremaining: 720ms\n",
      "77:\tlearn: 0.0107851\ttotal: 2.44s\tremaining: 689ms\n",
      "78:\tlearn: 0.0106332\ttotal: 2.47s\tremaining: 657ms\n",
      "79:\tlearn: 0.0106308\ttotal: 2.5s\tremaining: 624ms\n",
      "80:\tlearn: 0.0105445\ttotal: 2.53s\tremaining: 593ms\n",
      "81:\tlearn: 0.0104527\ttotal: 2.56s\tremaining: 561ms\n",
      "82:\tlearn: 0.0103518\ttotal: 2.59s\tremaining: 530ms\n",
      "83:\tlearn: 0.0102941\ttotal: 2.62s\tremaining: 499ms\n",
      "84:\tlearn: 0.0102909\ttotal: 2.65s\tremaining: 467ms\n",
      "85:\tlearn: 0.0102352\ttotal: 2.68s\tremaining: 436ms\n",
      "86:\tlearn: 0.0100710\ttotal: 2.71s\tremaining: 405ms\n",
      "87:\tlearn: 0.0100154\ttotal: 2.74s\tremaining: 374ms\n",
      "88:\tlearn: 0.0099523\ttotal: 2.77s\tremaining: 343ms\n",
      "89:\tlearn: 0.0098880\ttotal: 2.8s\tremaining: 311ms\n",
      "90:\tlearn: 0.0098451\ttotal: 2.83s\tremaining: 280ms\n",
      "91:\tlearn: 0.0098081\ttotal: 2.86s\tremaining: 249ms\n",
      "92:\tlearn: 0.0097478\ttotal: 2.89s\tremaining: 218ms\n",
      "93:\tlearn: 0.0096590\ttotal: 2.93s\tremaining: 187ms\n",
      "94:\tlearn: 0.0096097\ttotal: 2.96s\tremaining: 156ms\n",
      "95:\tlearn: 0.0095211\ttotal: 2.99s\tremaining: 125ms\n",
      "96:\tlearn: 0.0093684\ttotal: 3.02s\tremaining: 93.6ms\n",
      "97:\tlearn: 0.0093298\ttotal: 3.05s\tremaining: 62.3ms\n",
      "98:\tlearn: 0.0092736\ttotal: 3.08s\tremaining: 31.2ms\n",
      "99:\tlearn: 0.0092526\ttotal: 3.11s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2ba20754df28>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75c8d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cbc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb7153db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0     0.4211    0.9788    0.5889      7500\\n           1     0.9154    0.4870    0.6358      9240\\n           2     1.0000    0.0007    0.0013      3000\\n           3     0.8409    0.2517    0.3874       294\\n           4     1.0000    0.4000    0.5714         5\\n           5     0.0000    0.0000    0.0000      2075\\n           6     0.0000    0.0000    0.0000       327\\n\\n    accuracy                         0.5311     22441\\n   macro avg     0.5968    0.3026    0.3121     22441\\nweighted avg     0.6626    0.5311    0.4640     22441\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00552205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
