{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn as dglnn\n",
    "from dgl import from_networkx\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.data.utils import load_graphs\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [0, 1, 2]\n",
    "v = [1, 2, 3]\n",
    "g = dgl.graph((u, v))\n",
    "g = dgl.to_bidirected(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['h'] = torch.ones((g.num_nodes(), 55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edata['h'] = torch.ones((g.num_edges(), 55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=4, num_edges=6,\n",
       "      ndata_schemes={'h': Scheme(shape=(55,), dtype=torch.float32)}\n",
       "      edata_schemes={'h': Scheme(shape=(55,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.randint(0, 2, (6, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edata['y'] = edge_label = th.randint(0, 2, (6, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "g2 = copy.copy(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim , out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        self.W = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.attn_fc = nn.Linear(3 * out_dim, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.W.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
    "    def edge_attention(self, edges):\n",
    "        z = torch.cat([edges.src['z'], edges.dst['z'], edges.data['z']], -1)\n",
    "        a = self.attn_fc(z)\n",
    "        alpha = F.leaky_relu(a)\n",
    "        return {'e': alpha}\n",
    "    def message_func(self, edges):\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "    def reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1) # 归一化每一条入边的注意力系数\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'] * nodes.mailbox['e'], dim=1)\n",
    "        return {'h': h}\n",
    "    def forward(self, hfeat, efeat):\n",
    "        z1 = self.W(hfeat)\n",
    "        z2 = self.W(efeat)\n",
    "        self.g.ndata['z'] = z1 # 每个节点的特征\n",
    "        self.g.edata['z'] = z2 # 每条边的特征\n",
    "        self.g.edata['h'] = z2 # 每条边的特征\n",
    "        self.g.apply_edges(self.edge_attention) # 为每一条边获得其注意力系数\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata['h'], self.g.edata['z']\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = GATLayer(g, in_dim, hidden_dim)\n",
    "        self.layer2 = GATLayer(g, hidden_dim, out_dim)\n",
    "        self.pred = MLPPredictor(out_dim, 2)\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        nfeats, efeats = self.layer1(nfeats, efeats)\n",
    "        nfeats, efeats = self.layer2(nfeats, efeats)\n",
    "        return self.pred(g, nfeats)\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(th.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 55\n",
    "hidden_dim = 30\n",
    "out_dim = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(in_dim, hidden_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = th.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = g.ndata['h']\n",
    "edge_features = g.edata['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=4, num_edges=6,\n",
       "      ndata_schemes={'h': Scheme(shape=(55,), dtype=torch.float32)}\n",
       "      edata_schemes={'h': Scheme(shape=(55,), dtype=torch.float32), 'y': Scheme(shape=(), dtype=torch.int64)})"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=4, num_edges=6,\n",
       "      ndata_schemes={'h': Scheme(shape=(55,), dtype=torch.float32)}\n",
       "      edata_schemes={'h': Scheme(shape=(55,), dtype=torch.float32), 'y': Scheme(shape=(), dtype=torch.int64)})"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6941, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,100):\n",
    "    pred = model(g2, node_features, edge_features)\n",
    "    loss = criterion(pred, g.edata['y'])\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dgl.functional'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a7a75afb2f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0medge_softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIdentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dgl.functional'"
     ]
    }
   ],
   "source": [
    "\"\"\"Torch modules for graph attention networks(GAT).\"\"\"\n",
    "# pylint: disable= no-member, arguments-differ, invalid-name\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "from dgl import function as fn\n",
    "from dgl.functional import edge_softmax\n",
    "from dgl.base import DGLError\n",
    "from dgl.utils import Identity\n",
    "from dgl.utils import expand_as_pair\n",
    "\n",
    "# pylint: enable=W0235\n",
    "\n",
    "class GATConv(nn.Module):\n",
    "    r\"\"\"Graph attention layer from `Graph Attention Network\n",
    "    <https://arxiv.org/pdf/1710.10903.pdf>`__\n",
    "\n",
    "    .. math::\n",
    "        h_i^{(l+1)} = \\sum_{j\\in \\mathcal{N}(i)} \\alpha_{i,j} W^{(l)} h_j^{(l)}\n",
    "\n",
    "    where :math:`\\alpha_{ij}` is the attention score bewteen node :math:`i` and\n",
    "    node :math:`j`:\n",
    "\n",
    "    .. math::\n",
    "        \\alpha_{ij}^{l} &= \\mathrm{softmax_i} (e_{ij}^{l})\n",
    "\n",
    "        e_{ij}^{l} &= \\mathrm{LeakyReLU}\\left(\\vec{a}^T [W h_{i} \\| W h_{j}]\\right)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feats : int, or pair of ints\n",
    "        Input feature size; i.e, the number of dimensions of :math:`h_i^{(l)}`.\n",
    "        GATConv can be applied on homogeneous graph and unidirectional\n",
    "        `bipartite graph <https://docs.dgl.ai/generated/dgl.bipartite.html?highlight=bipartite>`__.\n",
    "        If the layer is to be applied to a unidirectional bipartite graph, ``in_feats``\n",
    "        specifies the input feature size on both the source and destination nodes.  If\n",
    "        a scalar is given, the source and destination node feature size would take the\n",
    "        same value.\n",
    "    out_feats : int\n",
    "        Output feature size; i.e, the number of dimensions of :math:`h_i^{(l+1)}`.\n",
    "    num_heads : int\n",
    "        Number of heads in Multi-Head Attention.\n",
    "    feat_drop : float, optional\n",
    "        Dropout rate on feature. Defaults: ``0``.\n",
    "    attn_drop : float, optional\n",
    "        Dropout rate on attention weight. Defaults: ``0``.\n",
    "    negative_slope : float, optional\n",
    "        LeakyReLU angle of negative slope. Defaults: ``0.2``.\n",
    "    residual : bool, optional\n",
    "        If True, use residual connection. Defaults: ``False``.\n",
    "    activation : callable activation function/layer or None, optional.\n",
    "        If not None, applies an activation function to the updated node features.\n",
    "        Default: ``None``.\n",
    "    allow_zero_in_degree : bool, optional\n",
    "        If there are 0-in-degree nodes in the graph, output for those nodes will be invalid\n",
    "        since no message will be passed to those nodes. This is harmful for some applications\n",
    "        causing silent performance regression. This module will raise a DGLError if it detects\n",
    "        0-in-degree nodes in input graph. By setting ``True``, it will suppress the check\n",
    "        and let the users handle it by themselves. Defaults: ``False``.\n",
    "    bias : bool, optional\n",
    "        If True, learns a bias term. Defaults: ``True``.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    Zero in-degree nodes will lead to invalid output value. This is because no message\n",
    "    will be passed to those nodes, the aggregation function will be appied on empty input.\n",
    "    A common practice to avoid this is to add a self-loop for each node in the graph if\n",
    "    it is homogeneous, which can be achieved by:\n",
    "\n",
    "    >>> g = ... # a DGLGraph\n",
    "    >>> g = dgl.add_self_loop(g)\n",
    "\n",
    "    Calling ``add_self_loop`` will not work for some graphs, for example, heterogeneous graph\n",
    "    since the edge type can not be decided for self_loop edges. Set ``allow_zero_in_degree``\n",
    "    to ``True`` for those cases to unblock the code and handle zero-in-degree nodes manually.\n",
    "    A common practise to handle this is to filter out the nodes with zero-in-degree when use\n",
    "    after conv.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import dgl\n",
    "    >>> import numpy as np\n",
    "    >>> import torch as th\n",
    "    >>> from dgl.nn import GATConv\n",
    "\n",
    "    >>> # Case 1: Homogeneous graph\n",
    "    >>> g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))\n",
    "    >>> g = dgl.add_self_loop(g)\n",
    "    >>> feat = th.ones(6, 10)\n",
    "    >>> gatconv = GATConv(10, 2, num_heads=3)\n",
    "    >>> res = gatconv(g, feat)\n",
    "    >>> res\n",
    "    tensor([[[ 3.4570,  1.8634],\n",
    "            [ 1.3805, -0.0762],\n",
    "            [ 1.0390, -1.1479]],\n",
    "            [[ 3.4570,  1.8634],\n",
    "            [ 1.3805, -0.0762],\n",
    "            [ 1.0390, -1.1479]],\n",
    "            [[ 3.4570,  1.8634],\n",
    "            [ 1.3805, -0.0762],\n",
    "            [ 1.0390, -1.1479]],\n",
    "            [[ 3.4570,  1.8634],\n",
    "            [ 1.3805, -0.0762],\n",
    "            [ 1.0390, -1.1479]],\n",
    "            [[ 3.4570,  1.8634],\n",
    "            [ 1.3805, -0.0762],\n",
    "            [ 1.0390, -1.1479]],\n",
    "            [[ 3.4570,  1.8634],\n",
    "            [ 1.3805, -0.0762],\n",
    "            [ 1.0390, -1.1479]]], grad_fn=<BinaryReduceBackward>)\n",
    "\n",
    "    >>> # Case 2: Unidirectional bipartite graph\n",
    "    >>> u = [0, 1, 0, 0, 1]\n",
    "    >>> v = [0, 1, 2, 3, 2]\n",
    "    >>> g = dgl.heterograph({('A', 'r', 'B'): (u, v)})\n",
    "    >>> u_feat = th.tensor(np.random.rand(2, 5).astype(np.float32))\n",
    "    >>> v_feat = th.tensor(np.random.rand(4, 10).astype(np.float32))\n",
    "    >>> gatconv = GATConv((5,10), 2, 3)\n",
    "    >>> res = gatconv(g, (u_feat, v_feat))\n",
    "    >>> res\n",
    "    tensor([[[-0.6066,  1.0268],\n",
    "            [-0.5945, -0.4801],\n",
    "            [ 0.1594,  0.3825]],\n",
    "            [[ 0.0268,  1.0783],\n",
    "            [ 0.5041, -1.3025],\n",
    "            [ 0.6568,  0.7048]],\n",
    "            [[-0.2688,  1.0543],\n",
    "            [-0.0315, -0.9016],\n",
    "            [ 0.3943,  0.5347]],\n",
    "            [[-0.6066,  1.0268],\n",
    "            [-0.5945, -0.4801],\n",
    "            [ 0.1594,  0.3825]]], grad_fn=<BinaryReduceBackward>)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 feat_drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 negative_slope=0.2,\n",
    "                 residual=False,\n",
    "                 activation=None,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 bias=True):\n",
    "        super(GATConv, self).__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        if isinstance(in_feats, tuple):\n",
    "            self.fc_src = nn.Linear(\n",
    "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
    "            self.fc_dst = nn.Linear(\n",
    "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
    "        else:\n",
    "            self.fc = nn.Linear(\n",
    "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
    "        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n",
    "        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(th.FloatTensor(size=(num_heads * out_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        if residual:\n",
    "            if self._in_dst_feats != out_feats * num_heads:\n",
    "                self.res_fc = nn.Linear(\n",
    "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
    "            else:\n",
    "                self.res_fc = Identity()\n",
    "        else:\n",
    "            self.register_buffer('res_fc', None)\n",
    "        self.reset_parameters()\n",
    "        self.activation = activation\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Description\n",
    "        -----------\n",
    "        Reinitialize learnable parameters.\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        The fc weights :math:`W^{(l)}` are initialized using Glorot uniform initialization.\n",
    "        The attention weights are using xavier initialization method.\n",
    "        \"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        if hasattr(self, 'fc'):\n",
    "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
    "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "        if isinstance(self.res_fc, nn.Linear):\n",
    "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
    "\n",
    "\n",
    "    def set_allow_zero_in_degree(self, set_value):\n",
    "        r\"\"\"\n",
    "\n",
    "        Description\n",
    "        -----------\n",
    "        Set allow_zero_in_degree flag.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        set_value : bool\n",
    "            The value to be set to the flag.\n",
    "        \"\"\"\n",
    "        self._allow_zero_in_degree = set_value\n",
    "\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        r\"\"\"\n",
    "\n",
    "        Description\n",
    "        -----------\n",
    "        Compute graph attention network layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : DGLGraph\n",
    "            The graph.\n",
    "        feat : torch.Tensor or pair of torch.Tensor\n",
    "            If a torch.Tensor is given, the input feature of shape :math:`(N, *, D_{in})` where\n",
    "            :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.\n",
    "            If a pair of torch.Tensor is given, the pair must contain two tensors of shape\n",
    "            :math:`(N_{in}, *, D_{in_{src}})` and :math:`(N_{out}, *, D_{in_{dst}})`.\n",
    "        get_attention : bool, optional\n",
    "            Whether to return the attention values. Default to False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output feature of shape :math:`(N, *, H, D_{out})` where :math:`H`\n",
    "            is the number of heads, and :math:`D_{out}` is size of output feature.\n",
    "        torch.Tensor, optional\n",
    "            The attention values of shape :math:`(E, *, H, 1)`, where :math:`E` is the number of\n",
    "            edges. This is returned only when :attr:`get_attention` is ``True``.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        DGLError\n",
    "            If there are 0-in-degree nodes in the input graph, it will raise DGLError\n",
    "            since no message will be passed to those nodes. This will cause invalid output.\n",
    "            The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.\n",
    "        \"\"\"\n",
    "        with graph.local_scope():\n",
    "            if not self._allow_zero_in_degree:\n",
    "                if (graph.in_degrees() == 0).any():\n",
    "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "                                   'output for those nodes will be invalid. '\n",
    "                                   'This is harmful for some applications, '\n",
    "                                   'causing silent performance regression. '\n",
    "                                   'Adding self-loop on the input graph by '\n",
    "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                                   'to be `True` when constructing this module will '\n",
    "                                   'suppress the check and let the code run.')\n",
    "\n",
    "            if isinstance(feat, tuple):\n",
    "                src_prefix_shape = feat[0].shape[:-1]\n",
    "                dst_prefix_shape = feat[1].shape[:-1]\n",
    "                h_src = self.feat_drop(feat[0])\n",
    "                h_dst = self.feat_drop(feat[1])\n",
    "                if not hasattr(self, 'fc_src'):\n",
    "                    feat_src = self.fc(h_src).view(\n",
    "                        *src_prefix_shape, self._num_heads, self._out_feats)\n",
    "                    feat_dst = self.fc(h_dst).view(\n",
    "                        *dst_prefix_shape, self._num_heads, self._out_feats)\n",
    "                else:\n",
    "                    feat_src = self.fc_src(h_src).view(\n",
    "                        *src_prefix_shape, self._num_heads, self._out_feats)\n",
    "                    feat_dst = self.fc_dst(h_dst).view(\n",
    "                        *dst_prefix_shape, self._num_heads, self._out_feats)\n",
    "            else:\n",
    "                src_prefix_shape = dst_prefix_shape = feat.shape[:-1]\n",
    "                h_src = h_dst = self.feat_drop(feat)\n",
    "                feat_src = feat_dst = self.fc(h_src).view(\n",
    "                    *src_prefix_shape, self._num_heads, self._out_feats)\n",
    "                if graph.is_block:\n",
    "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
    "                    h_dst = h_dst[:graph.number_of_dst_nodes()]\n",
    "                    dst_prefix_shape = (graph.number_of_dst_nodes(),) + dst_prefix_shape[1:]\n",
    "            # NOTE: GAT paper uses \"first concatenation then linear projection\"\n",
    "            # to compute attention scores, while ours is \"first projection then\n",
    "            # addition\", the two approaches are mathematically equivalent:\n",
    "            # We decompose the weight vector a mentioned in the paper into\n",
    "            # [a_l || a_r], then\n",
    "            # a^T [Wh_i || Wh_j] = a_l Wh_i + a_r Wh_j\n",
    "            # Our implementation is much efficient because we do not need to\n",
    "            # save [Wh_i || Wh_j] on edges, which is not memory-efficient. Plus,\n",
    "            # addition could be optimized with DGL's built-in function u_add_v,\n",
    "            # which further speeds up computation and saves memory footprint.\n",
    "            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n",
    "            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.srcdata.update({'ft': feat_src, 'el': el})\n",
    "            graph.dstdata.update({'er': er})\n",
    "            # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\n",
    "            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
    "            e = self.leaky_relu(graph.edata.pop('e'))\n",
    "            # compute softmax\n",
    "            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
    "            # message passing\n",
    "            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n",
    "                             fn.sum('m', 'ft'))\n",
    "            rst = graph.dstdata['ft']\n",
    "            # residual\n",
    "            if self.res_fc is not None:\n",
    "                # Use -1 rather than self._num_heads to handle broadcasting\n",
    "                resval = self.res_fc(h_dst).view(*dst_prefix_shape, -1, self._out_feats)\n",
    "                rst = rst + resval\n",
    "            # bias\n",
    "            if self.bias is not None:\n",
    "                rst = rst + self.bias.view(\n",
    "                    *((1,) * len(dst_prefix_shape)), self._num_heads, self._out_feats)\n",
    "            # activation\n",
    "            if self.activation:\n",
    "                rst = self.activation(rst)\n",
    "\n",
    "            if get_attention:\n",
    "                return rst, graph.edata['a']\n",
    "            else:\n",
    "                return rst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
