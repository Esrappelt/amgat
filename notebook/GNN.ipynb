{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c29003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c32be729",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "device = 'cuda:0'\n",
    "data_class = {\"UNSW-NB15\": 10,\n",
    "              \"Darknet\": 9,\n",
    "              \"CSE-CIC\": 7,\n",
    "              \"ToN-IoT\": 10}\n",
    "data_lr = {\"UNSW-NB15\": 0.007,\n",
    "           \"Darknet\": 0.003,\n",
    "           \"CSE-CIC\": 0.003,\n",
    "           \"ToN-IoT\": 0.01}\n",
    "test_size = {\"UNSW-NB15\": 210000,\n",
    "             \"Darknet\": 45000,\n",
    "             \"CSE-CIC\": 75000,\n",
    "             \"ToN-IoT\": 140000}\n",
    "from torch_geometric.nn import GATv2Conv, GATConv, ResGatedGraphConv,ClusterGCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "874dd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gat(path, binary):\n",
    "    # feature\n",
    "    edge_feat = np.load(path + \"edge_feat_scaled.npy\")  # （n,f)\n",
    "    edge_feat = torch.tensor(edge_feat, dtype=torch.float, device=device)\n",
    "\n",
    "    # label\n",
    "    if binary:\n",
    "        label = np.load(path + \"label_bi.npy\", allow_pickle=True)  # (n,1)\n",
    "    else:\n",
    "        label = np.load(path + \"label_mul.npy\", allow_pickle=True)\n",
    "    label = torch.tensor(label, dtype=torch.long, device=device)  # Cross entropy expects a long int\n",
    "\n",
    "    # adjacency\n",
    "    adj = np.load(path + \"adj_random.npy\", allow_pickle=True)\n",
    "    with open(path + 'adj_random_list.dict', 'rb') as file:\n",
    "        adj_lists = pickle.load(file)\n",
    "\n",
    "    return edge_feat, label, adj, adj_lists\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, edge_feat, adj, adj_lists):\n",
    "        self.device = device\n",
    "        self.edge_feat = edge_feat\n",
    "        self.adj = adj\n",
    "        self.adj_lists = adj_lists\n",
    "\n",
    "    def get_data(self, edge_idx):\n",
    "        source_nodes_ids, target_nodes_ids = [], []\n",
    "        seen_edges = set()\n",
    "        edges = set(edge_idx)\n",
    "        edges_neigh = set(edge_idx)\n",
    "        k = 2  # k-hop\n",
    "        for i in range(k):\n",
    "            source_nodes_ids, target_nodes_ids, seen_edges, edges_neigh = self.build_edge_index(source_nodes_ids,\n",
    "                                                                                                target_nodes_ids,\n",
    "                                                                                                seen_edges,\n",
    "                                                                                                edges_neigh)\n",
    "            edges = edges.union(edges_neigh)\n",
    "\n",
    "        # Step 2: Construct Batch in_nodes_features\n",
    "        in_nodes_features = self.edge_feat[list(edges)]\n",
    "        # Step 3: Construct new mapping; unique_map converts the edges to (0, len(edges))\n",
    "        unique_map = {}\n",
    "        for idx, edge in enumerate(edges):\n",
    "            unique_map[edge] = idx\n",
    "        source_nodes_ids = [unique_map[ids] for ids in source_nodes_ids]\n",
    "        target_nodes_ids = [unique_map[ids] for ids in target_nodes_ids]\n",
    "        # Step 4: Build edge_index; shape = (2, E), where E is the number of edges in the graph\n",
    "        edge_index = np.row_stack((source_nodes_ids, target_nodes_ids))\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.int64, device=self.device)\n",
    "\n",
    "        # Step 5: Mapped edge_idx\n",
    "        map_edge_idx = [unique_map[ids] for ids in edge_idx]\n",
    "        map_edge_idx = torch.tensor(map_edge_idx, dtype=torch.int64, device=self.device)\n",
    "        # 如果用残差 就需要这一步\n",
    "        # data = (in_nodes_features, in_nodes_features, edge_index, map_edge_idx) # 第一个是用于残差使用的，第二个是原始的节点特征\n",
    "        # 没有残差返回这个\n",
    "        data = (in_nodes_features, edge_index, map_edge_idx)\n",
    "        return data\n",
    "\n",
    "    def build_edge_index(self, source_nodes_ids, target_nodes_ids, seen_edges, edges_neigh):\n",
    "        new_neigh = set()\n",
    "        for edge in edges_neigh:\n",
    "            nodes = self.adj[edge]\n",
    "            for node in nodes:\n",
    "                neigh = self.adj_lists.get(node)\n",
    "                new_neigh = new_neigh.union(neigh)\n",
    "                for edge_neigh in neigh:\n",
    "                    if (edge, edge_neigh) not in seen_edges and \\\n",
    "                            (edge_neigh, edge) not in seen_edges:\n",
    "                        # and \\\n",
    "                        # edge != edge_neigh:\n",
    "                        source_nodes_ids.append(edge)\n",
    "                        target_nodes_ids.append(edge_neigh)\n",
    "                        seen_edges.add((edge, edge_neigh))\n",
    "\n",
    "        return source_nodes_ids, target_nodes_ids, seen_edges, new_neigh\n",
    "\n",
    "\n",
    "class MyGNN(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MyGNN, self).__init__()\n",
    "        self.hidden_size1 = 50\n",
    "        self.hidden_size2 = 30\n",
    "        self.hidden_size3 = 20\n",
    "        self.hidden_size4 = 10\n",
    "        self.conv1 = ClusterGCNConv(in_features, self.hidden_size1)\n",
    "        self.conv2 = ClusterGCNConv(self.hidden_size1, self.hidden_size2)\n",
    "        self.conv3 = ClusterGCNConv(self.hidden_size2, self.hidden_size3)\n",
    "        self.conv4 = ClusterGCNConv(self.hidden_size3, self.hidden_size4)\n",
    "        self.conv5 = ClusterGCNConv(self.hidden_size4, out_features)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        z = self.conv1(x, edge_index)\n",
    "        z = self.conv2(z, edge_index)\n",
    "        z = self.conv3(z, edge_index)\n",
    "        z = self.conv4(z, edge_index)\n",
    "        z = self.conv5(z, edge_index)\n",
    "        output = F.log_softmax(z, 1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GNNTrain:\n",
    "    def __init__(self, model, train_idx, val_idx, dataset, label, epochs):\n",
    "        self.loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "        self.model = model\n",
    "        self.train_idx = train_idx\n",
    "        self.val_idx = val_idx\n",
    "        self.dataset = dataset\n",
    "        self.epochs = epochs\n",
    "        self.label = label\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            random.shuffle(self.train_idx)\n",
    "            print(\"epoch: \", epoch)\n",
    "            for batch in range(int(len(self.train_idx) / 500)):\n",
    "                start_time = time.time()\n",
    "                batch_edges = self.train_idx[500 * batch:500 * (batch + 1)]  # 500 records per batch\n",
    "                x, edge_index, map_edge_idx = self.dataset.get_data(batch_edges)\n",
    "                data = Data(x=x, edge_index=edge_index, y=self.label[batch_edges]).to(device)\n",
    "                self.model.train()\n",
    "                output = self.model(data)\n",
    "                train_output = output.index_select(0, map_edge_idx)  # 选择idx的行\n",
    "                train_output_ = train_output.cpu()\n",
    "                loss = self.loss_fn(train_output, self.label[batch_edges])\n",
    "                self.optimizer.zero_grad()  # 权重清零\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                end_time = time.time()\n",
    "                acc_train = f1_score(data.y.cpu(), torch.argmax(train_output_, dim=-1), average=\"weighted\")\n",
    "\n",
    "                print('batch: {:03d}'.format(batch + 1),\n",
    "                      'loss_train: {:.4f}'.format(loss.item()),\n",
    "                      'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                      'time: {:.4f}s'.format(end_time - start_time))\n",
    "\n",
    "                if batch >= 179:\n",
    "                    break\n",
    "                torch.save(self.model.state_dict(), './ClusterGCNConv.pth' + f'{epoch}' + '.pth')\n",
    "\n",
    "    def evaluate(self):\n",
    "        batch_size = 500\n",
    "        predict_output = []\n",
    "        model_path = './ResGatedGraphConv2.pth'\n",
    "        if not os.path.exists(model_path):\n",
    "            raise RuntimeError('没有已保存的模型')\n",
    "        model_dict = torch.load(model_path)\n",
    "        self.model.load_state_dict(model_dict)\n",
    "        allloss = []\n",
    "        for batch in tqdm(range(int(len(self.val_idx) / batch_size))):\n",
    "            batch_edges = self.val_idx[batch_size * batch:batch_size * (batch + 1)]\n",
    "            x, edge_index, map_edge_idx = self.dataset.get_data(batch_edges)\n",
    "            data = Data(x=x, edge_index=edge_index, y=self.label[batch_edges]).to(device)\n",
    "            output = self.model(data)\n",
    "            batch_output = output.index_select(0, map_edge_idx)  # 选择idx的行\n",
    "            batch_loss = self.loss_fn(batch_output, self.label[batch_edges])\n",
    "            allloss.append(batch_loss.item())\n",
    "            batch_output = torch.argmax(batch_output, dim=-1)\n",
    "            predict_output.extend(batch_output.cpu())\n",
    "        allloss = np.array(allloss)\n",
    "        print(classification_report(self.label[self.val_idx].cpu(), predict_output, digits=4))\n",
    "        print(confusion_matrix(self.label[self.val_idx].cpu(), predict_output))\n",
    "        return predict_output, self.label[self.val_idx].cpu()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ae3b1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "batch: 001 loss_train: 5.9969 acc_train: 0.0856 time: 5.9186s\n",
      "batch: 002 loss_train: 4.0600 acc_train: 0.2980 time: 6.1450s\n",
      "batch: 003 loss_train: 3.4513 acc_train: 0.4180 time: 5.9042s\n",
      "batch: 004 loss_train: 2.2568 acc_train: 0.7147 time: 6.5370s\n",
      "batch: 005 loss_train: 2.3909 acc_train: 0.7212 time: 5.4628s\n",
      "batch: 006 loss_train: 1.8252 acc_train: 0.6933 time: 5.6928s\n",
      "batch: 007 loss_train: 1.9634 acc_train: 0.7210 time: 6.1728s\n",
      "batch: 008 loss_train: 1.3968 acc_train: 0.7502 time: 5.9159s\n",
      "batch: 009 loss_train: 1.2552 acc_train: 0.7806 time: 5.9816s\n",
      "batch: 010 loss_train: 1.0715 acc_train: 0.8080 time: 6.0314s\n",
      "batch: 011 loss_train: 1.0083 acc_train: 0.7990 time: 5.8860s\n",
      "batch: 012 loss_train: 1.5124 acc_train: 0.8073 time: 5.8352s\n",
      "batch: 013 loss_train: 0.8878 acc_train: 0.8762 time: 5.7954s\n",
      "batch: 014 loss_train: 0.6298 acc_train: 0.8768 time: 5.9676s\n",
      "batch: 015 loss_train: 0.7318 acc_train: 0.8959 time: 6.4645s\n",
      "batch: 016 loss_train: 0.5743 acc_train: 0.9039 time: 6.3321s\n",
      "batch: 017 loss_train: 0.5254 acc_train: 0.8786 time: 6.5980s\n",
      "batch: 018 loss_train: 0.5627 acc_train: 0.8708 time: 5.8372s\n",
      "batch: 019 loss_train: 0.7214 acc_train: 0.8601 time: 5.8950s\n",
      "batch: 020 loss_train: 0.5319 acc_train: 0.8810 time: 6.4914s\n",
      "batch: 021 loss_train: 0.4701 acc_train: 0.9137 time: 5.7844s\n",
      "batch: 022 loss_train: 0.4422 acc_train: 0.9107 time: 5.7237s\n",
      "batch: 023 loss_train: 0.3822 acc_train: 0.9077 time: 5.7874s\n",
      "batch: 024 loss_train: 0.6877 acc_train: 0.8586 time: 5.6819s\n",
      "batch: 025 loss_train: 0.4236 acc_train: 0.8844 time: 6.0334s\n",
      "batch: 026 loss_train: 0.6435 acc_train: 0.8743 time: 5.9447s\n",
      "batch: 027 loss_train: 0.3897 acc_train: 0.8973 time: 5.7386s\n",
      "batch: 028 loss_train: 0.5118 acc_train: 0.9031 time: 6.4147s\n",
      "batch: 029 loss_train: 0.4301 acc_train: 0.8958 time: 5.9189s\n",
      "batch: 030 loss_train: 0.3596 acc_train: 0.9081 time: 6.2046s\n",
      "batch: 031 loss_train: 0.4470 acc_train: 0.8859 time: 6.4137s\n",
      "batch: 032 loss_train: 0.4663 acc_train: 0.9120 time: 6.2355s\n",
      "batch: 033 loss_train: 0.4335 acc_train: 0.8953 time: 6.3431s\n",
      "batch: 034 loss_train: 1.1484 acc_train: 0.9054 time: 5.8253s\n",
      "batch: 035 loss_train: 0.5519 acc_train: 0.8766 time: 6.0483s\n",
      "batch: 036 loss_train: 0.4494 acc_train: 0.9227 time: 5.8233s\n",
      "batch: 037 loss_train: 0.3667 acc_train: 0.9147 time: 6.7135s\n",
      "batch: 038 loss_train: 0.5195 acc_train: 0.8579 time: 5.9079s\n",
      "batch: 039 loss_train: 0.3896 acc_train: 0.9101 time: 6.4725s\n",
      "batch: 040 loss_train: 0.3906 acc_train: 0.8956 time: 6.0493s\n",
      "batch: 041 loss_train: 0.5206 acc_train: 0.8798 time: 5.5195s\n",
      "batch: 042 loss_train: 0.4129 acc_train: 0.8839 time: 6.2066s\n",
      "batch: 043 loss_train: 0.5080 acc_train: 0.8905 time: 6.2504s\n",
      "batch: 044 loss_train: 0.3298 acc_train: 0.9070 time: 6.4147s\n",
      "batch: 045 loss_train: 0.2989 acc_train: 0.9225 time: 5.9567s\n",
      "batch: 046 loss_train: 0.5333 acc_train: 0.8867 time: 5.9925s\n",
      "batch: 047 loss_train: 0.3318 acc_train: 0.9028 time: 5.9291s\n",
      "batch: 048 loss_train: 0.3721 acc_train: 0.8752 time: 5.5106s\n",
      "batch: 049 loss_train: 0.3804 acc_train: 0.9041 time: 5.8910s\n",
      "batch: 050 loss_train: 0.4130 acc_train: 0.9070 time: 6.1061s\n",
      "batch: 051 loss_train: 0.4134 acc_train: 0.8910 time: 6.1379s\n",
      "batch: 052 loss_train: 0.3575 acc_train: 0.8737 time: 6.1240s\n",
      "batch: 053 loss_train: 0.3768 acc_train: 0.9023 time: 5.9457s\n",
      "batch: 054 loss_train: 0.2919 acc_train: 0.9024 time: 6.0373s\n",
      "batch: 055 loss_train: 0.3567 acc_train: 0.8877 time: 5.6649s\n",
      "batch: 056 loss_train: 0.3860 acc_train: 0.9018 time: 5.2616s\n",
      "batch: 057 loss_train: 0.2913 acc_train: 0.9155 time: 5.6809s\n",
      "batch: 058 loss_train: 0.3741 acc_train: 0.9073 time: 5.8382s\n",
      "batch: 059 loss_train: 0.3501 acc_train: 0.8827 time: 5.9915s\n",
      "batch: 060 loss_train: 0.2942 acc_train: 0.9303 time: 6.0234s\n",
      "batch: 061 loss_train: 0.2539 acc_train: 0.9256 time: 6.5561s\n",
      "batch: 062 loss_train: 0.3793 acc_train: 0.9044 time: 5.9258s\n",
      "batch: 063 loss_train: 0.2700 acc_train: 0.9329 time: 5.9577s\n",
      "batch: 064 loss_train: 0.3070 acc_train: 0.9138 time: 6.3172s\n",
      "batch: 065 loss_train: 0.3343 acc_train: 0.9127 time: 6.1867s\n",
      "batch: 066 loss_train: 0.2337 acc_train: 0.9290 time: 6.0373s\n",
      "batch: 067 loss_train: 0.3203 acc_train: 0.9108 time: 6.7065s\n",
      "batch: 068 loss_train: 0.2162 acc_train: 0.9441 time: 6.5283s\n",
      "batch: 069 loss_train: 0.3098 acc_train: 0.8932 time: 6.2395s\n",
      "batch: 070 loss_train: 0.3932 acc_train: 0.8987 time: 6.1728s\n",
      "batch: 071 loss_train: 0.3551 acc_train: 0.8784 time: 6.6567s\n",
      "batch: 072 loss_train: 0.2113 acc_train: 0.9512 time: 5.8571s\n",
      "batch: 073 loss_train: 0.3803 acc_train: 0.8842 time: 6.3162s\n",
      "batch: 074 loss_train: 0.3311 acc_train: 0.9085 time: 6.4705s\n",
      "batch: 075 loss_train: 0.4724 acc_train: 0.9075 time: 6.1150s\n",
      "batch: 076 loss_train: 0.3793 acc_train: 0.8865 time: 6.4785s\n",
      "batch: 077 loss_train: 0.2746 acc_train: 0.9152 time: 6.2733s\n",
      "batch: 078 loss_train: 0.3135 acc_train: 0.9269 time: 6.6736s\n",
      "batch: 079 loss_train: 0.2576 acc_train: 0.9184 time: 5.9766s\n",
      "batch: 080 loss_train: 0.3084 acc_train: 0.8971 time: 5.9238s\n",
      "batch: 081 loss_train: 0.3047 acc_train: 0.8974 time: 6.3739s\n",
      "batch: 082 loss_train: 0.4184 acc_train: 0.8908 time: 5.9686s\n",
      "batch: 083 loss_train: 0.3047 acc_train: 0.9046 time: 5.9527s\n",
      "batch: 084 loss_train: 0.3087 acc_train: 0.9231 time: 5.8113s\n",
      "batch: 085 loss_train: 0.3118 acc_train: 0.8998 time: 6.4934s\n",
      "batch: 086 loss_train: 0.3744 acc_train: 0.8953 time: 6.2514s\n",
      "batch: 087 loss_train: 0.3058 acc_train: 0.8895 time: 5.7446s\n",
      "batch: 088 loss_train: 0.2795 acc_train: 0.9186 time: 6.2494s\n",
      "batch: 089 loss_train: 0.3163 acc_train: 0.9023 time: 5.9806s\n",
      "batch: 090 loss_train: 0.4135 acc_train: 0.9204 time: 5.6271s\n",
      "batch: 091 loss_train: 0.2641 acc_train: 0.9066 time: 5.7117s\n",
      "batch: 092 loss_train: 0.3848 acc_train: 0.8898 time: 6.5074s\n",
      "batch: 093 loss_train: 0.5208 acc_train: 0.9000 time: 6.1509s\n",
      "batch: 094 loss_train: 0.2934 acc_train: 0.9434 time: 5.6112s\n",
      "batch: 095 loss_train: 0.2624 acc_train: 0.9415 time: 5.4688s\n",
      "batch: 096 loss_train: 0.3690 acc_train: 0.8760 time: 6.0812s\n",
      "batch: 097 loss_train: 0.2733 acc_train: 0.9243 time: 6.3660s\n",
      "batch: 098 loss_train: 0.3561 acc_train: 0.8987 time: 6.6268s\n",
      "batch: 099 loss_train: 0.4508 acc_train: 0.9040 time: 6.3062s\n",
      "batch: 100 loss_train: 0.2396 acc_train: 0.9213 time: 6.1768s\n",
      "batch: 101 loss_train: 0.3573 acc_train: 0.9060 time: 6.1768s\n",
      "batch: 102 loss_train: 0.2808 acc_train: 0.9161 time: 6.4685s\n",
      "batch: 103 loss_train: 0.3016 acc_train: 0.9221 time: 6.1937s\n",
      "batch: 104 loss_train: 0.2597 acc_train: 0.9391 time: 6.2365s\n",
      "batch: 105 loss_train: 0.3016 acc_train: 0.9065 time: 5.9736s\n",
      "batch: 106 loss_train: 0.2903 acc_train: 0.9079 time: 5.8501s\n",
      "batch: 107 loss_train: 0.2539 acc_train: 0.9151 time: 5.7595s\n",
      "batch: 108 loss_train: 0.2938 acc_train: 0.9169 time: 6.3351s\n",
      "batch: 109 loss_train: 0.2858 acc_train: 0.9187 time: 5.8233s\n",
      "batch: 110 loss_train: 0.1772 acc_train: 0.9378 time: 6.4088s\n",
      "batch: 111 loss_train: 0.2219 acc_train: 0.9309 time: 6.3490s\n",
      "batch: 112 loss_train: 0.2902 acc_train: 0.9151 time: 6.3769s\n",
      "batch: 113 loss_train: 0.2716 acc_train: 0.9213 time: 6.5990s\n",
      "batch: 114 loss_train: 0.3118 acc_train: 0.8931 time: 6.4914s\n",
      "batch: 115 loss_train: 0.3639 acc_train: 0.8874 time: 6.5900s\n",
      "batch: 116 loss_train: 0.4628 acc_train: 0.9038 time: 6.3002s\n",
      "batch: 117 loss_train: 0.3298 acc_train: 0.9128 time: 5.7784s\n",
      "batch: 118 loss_train: 0.3122 acc_train: 0.9121 time: 5.8432s\n",
      "batch: 119 loss_train: 0.2424 acc_train: 0.9371 time: 6.6766s\n",
      "batch: 120 loss_train: 0.2566 acc_train: 0.9083 time: 6.0115s\n",
      "batch: 121 loss_train: 0.3731 acc_train: 0.8951 time: 6.1449s\n",
      "batch: 122 loss_train: 0.4063 acc_train: 0.8895 time: 5.6978s\n",
      "batch: 123 loss_train: 0.2847 acc_train: 0.9024 time: 6.2584s\n",
      "batch: 124 loss_train: 0.2817 acc_train: 0.9041 time: 6.2335s\n",
      "batch: 125 loss_train: 0.3239 acc_train: 0.9198 time: 6.0841s\n",
      "batch: 126 loss_train: 0.3613 acc_train: 0.9064 time: 5.6898s\n",
      "batch: 127 loss_train: 0.3206 acc_train: 0.9027 time: 5.8959s\n",
      "batch: 128 loss_train: 0.3209 acc_train: 0.8957 time: 6.1389s\n",
      "batch: 129 loss_train: 0.2927 acc_train: 0.9032 time: 5.4110s\n",
      "batch: 130 loss_train: 0.3782 acc_train: 0.9287 time: 5.9975s\n",
      "batch: 131 loss_train: 0.2561 acc_train: 0.9304 time: 6.6687s\n",
      "batch: 132 loss_train: 0.4710 acc_train: 0.8987 time: 5.4309s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 133 loss_train: 0.3024 acc_train: 0.9111 time: 5.8860s\n",
      "batch: 134 loss_train: 0.3002 acc_train: 0.9114 time: 6.5412s\n",
      "batch: 135 loss_train: 0.2373 acc_train: 0.9330 time: 6.2365s\n",
      "batch: 136 loss_train: 0.2983 acc_train: 0.9136 time: 6.0652s\n",
      "batch: 137 loss_train: 0.2992 acc_train: 0.9203 time: 6.6995s\n",
      "batch: 138 loss_train: 0.2638 acc_train: 0.9099 time: 6.3122s\n",
      "batch: 139 loss_train: 0.2999 acc_train: 0.9268 time: 6.9365s\n",
      "batch: 140 loss_train: 0.3088 acc_train: 0.9205 time: 6.6378s\n",
      "batch: 141 loss_train: 0.4022 acc_train: 0.9121 time: 6.1180s\n",
      "batch: 142 loss_train: 0.3662 acc_train: 0.9211 time: 6.2624s\n",
      "batch: 143 loss_train: 0.4377 acc_train: 0.8930 time: 6.7155s\n",
      "batch: 144 loss_train: 0.3350 acc_train: 0.9263 time: 5.9577s\n",
      "batch: 145 loss_train: 0.3828 acc_train: 0.9078 time: 5.8601s\n",
      "batch: 146 loss_train: 0.2930 acc_train: 0.8993 time: 5.7894s\n",
      "batch: 147 loss_train: 0.3321 acc_train: 0.9030 time: 5.7466s\n",
      "batch: 148 loss_train: 0.2647 acc_train: 0.9232 time: 6.0344s\n",
      "batch: 149 loss_train: 0.2800 acc_train: 0.9266 time: 5.8252s\n",
      "batch: 150 loss_train: 0.3065 acc_train: 0.9156 time: 6.2475s\n",
      "batch: 151 loss_train: 0.2497 acc_train: 0.9233 time: 5.6968s\n",
      "batch: 152 loss_train: 0.2299 acc_train: 0.9213 time: 5.7605s\n",
      "batch: 153 loss_train: 0.2926 acc_train: 0.9285 time: 5.7854s\n",
      "batch: 154 loss_train: 0.2660 acc_train: 0.9182 time: 5.6908s\n",
      "batch: 155 loss_train: 0.2919 acc_train: 0.9178 time: 5.9089s\n",
      "batch: 156 loss_train: 0.3026 acc_train: 0.9084 time: 5.9189s\n",
      "batch: 157 loss_train: 0.3645 acc_train: 0.9072 time: 5.8462s\n",
      "batch: 158 loss_train: 0.2731 acc_train: 0.9166 time: 5.8531s\n",
      "batch: 159 loss_train: 0.2879 acc_train: 0.9144 time: 5.8362s\n",
      "batch: 160 loss_train: 0.3071 acc_train: 0.9165 time: 5.9716s\n",
      "batch: 161 loss_train: 0.2125 acc_train: 0.9306 time: 6.1190s\n",
      "batch: 162 loss_train: 0.3039 acc_train: 0.9149 time: 5.9457s\n",
      "batch: 163 loss_train: 0.3704 acc_train: 0.8782 time: 6.3052s\n",
      "batch: 164 loss_train: 0.2471 acc_train: 0.9253 time: 5.6390s\n",
      "batch: 165 loss_train: 0.3054 acc_train: 0.9105 time: 6.0244s\n",
      "batch: 166 loss_train: 0.3382 acc_train: 0.8899 time: 6.0573s\n",
      "batch: 167 loss_train: 0.3019 acc_train: 0.8886 time: 5.9696s\n",
      "batch: 168 loss_train: 0.2477 acc_train: 0.9057 time: 5.7705s\n",
      "batch: 169 loss_train: 0.2376 acc_train: 0.9399 time: 6.0612s\n",
      "batch: 170 loss_train: 0.3267 acc_train: 0.9214 time: 6.0911s\n",
      "batch: 171 loss_train: 0.3099 acc_train: 0.9151 time: 5.5444s\n",
      "batch: 172 loss_train: 0.3566 acc_train: 0.8878 time: 5.7814s\n",
      "batch: 173 loss_train: 0.1754 acc_train: 0.9338 time: 5.9248s\n",
      "batch: 174 loss_train: 0.2592 acc_train: 0.9115 time: 5.7834s\n",
      "batch: 175 loss_train: 0.2718 acc_train: 0.9036 time: 5.3801s\n",
      "batch: 176 loss_train: 0.2958 acc_train: 0.9205 time: 5.5265s\n",
      "batch: 177 loss_train: 0.2676 acc_train: 0.9165 time: 5.3065s\n",
      "batch: 178 loss_train: 0.2616 acc_train: 0.9278 time: 5.3827s\n",
      "batch: 179 loss_train: 0.3095 acc_train: 0.9033 time: 5.8442s\n",
      "batch: 180 loss_train: 0.2932 acc_train: 0.9091 time: 6.0463s\n",
      "epoch:  1\n",
      "batch: 001 loss_train: 0.2700 acc_train: 0.9178 time: 6.2674s\n",
      "batch: 002 loss_train: 0.2675 acc_train: 0.8983 time: 5.2129s\n",
      "batch: 003 loss_train: 0.3701 acc_train: 0.8783 time: 5.9089s\n",
      "batch: 004 loss_train: 0.3297 acc_train: 0.9065 time: 5.6370s\n",
      "batch: 005 loss_train: 0.2814 acc_train: 0.9105 time: 5.9308s\n",
      "batch: 006 loss_train: 0.4250 acc_train: 0.9183 time: 5.3901s\n",
      "batch: 007 loss_train: 0.2666 acc_train: 0.9054 time: 5.3114s\n",
      "batch: 008 loss_train: 0.2386 acc_train: 0.9186 time: 5.7536s\n",
      "batch: 009 loss_train: 0.3252 acc_train: 0.8962 time: 5.3772s\n",
      "batch: 010 loss_train: 0.2926 acc_train: 0.9268 time: 5.8243s\n",
      "batch: 011 loss_train: 0.3809 acc_train: 0.9225 time: 5.5942s\n",
      "batch: 012 loss_train: 0.2705 acc_train: 0.9057 time: 5.7257s\n",
      "batch: 013 loss_train: 0.2697 acc_train: 0.9334 time: 5.8591s\n",
      "batch: 014 loss_train: 0.2834 acc_train: 0.9142 time: 5.8641s\n",
      "batch: 015 loss_train: 0.2453 acc_train: 0.9203 time: 5.4927s\n",
      "batch: 016 loss_train: 0.2618 acc_train: 0.8943 time: 5.6739s\n",
      "batch: 017 loss_train: 0.3006 acc_train: 0.9082 time: 5.6161s\n",
      "batch: 018 loss_train: 0.3289 acc_train: 0.9199 time: 6.1270s\n",
      "batch: 019 loss_train: 0.3031 acc_train: 0.9202 time: 5.3313s\n",
      "batch: 020 loss_train: 0.2290 acc_train: 0.9209 time: 6.2724s\n",
      "batch: 021 loss_train: 0.2241 acc_train: 0.9172 time: 5.4459s\n",
      "batch: 022 loss_train: 0.2643 acc_train: 0.9140 time: 5.9527s\n",
      "batch: 023 loss_train: 0.3079 acc_train: 0.9142 time: 6.2146s\n",
      "batch: 024 loss_train: 0.2595 acc_train: 0.9189 time: 5.8711s\n",
      "batch: 025 loss_train: 0.3667 acc_train: 0.9042 time: 5.8631s\n",
      "batch: 026 loss_train: 0.1917 acc_train: 0.9327 time: 5.7436s\n",
      "batch: 027 loss_train: 0.2568 acc_train: 0.9250 time: 6.0622s\n",
      "batch: 028 loss_train: 0.2282 acc_train: 0.9116 time: 5.9577s\n",
      "batch: 029 loss_train: 0.3370 acc_train: 0.8917 time: 5.9537s\n",
      "batch: 030 loss_train: 0.2968 acc_train: 0.8878 time: 5.9866s\n",
      "batch: 031 loss_train: 0.2449 acc_train: 0.9392 time: 5.6709s\n",
      "batch: 032 loss_train: 0.2279 acc_train: 0.9255 time: 6.0344s\n",
      "batch: 033 loss_train: 0.2703 acc_train: 0.9120 time: 5.9308s\n",
      "batch: 034 loss_train: 0.1952 acc_train: 0.9406 time: 6.1967s\n",
      "batch: 035 loss_train: 0.3108 acc_train: 0.8988 time: 6.3928s\n",
      "batch: 036 loss_train: 0.2900 acc_train: 0.9025 time: 6.3351s\n",
      "batch: 037 loss_train: 0.2778 acc_train: 0.9028 time: 6.3580s\n",
      "batch: 038 loss_train: 0.2737 acc_train: 0.8917 time: 6.5800s\n",
      "batch: 039 loss_train: 0.2137 acc_train: 0.9339 time: 6.0354s\n",
      "batch: 040 loss_train: 0.3163 acc_train: 0.9221 time: 6.4715s\n",
      "batch: 041 loss_train: 0.3965 acc_train: 0.9015 time: 6.4207s\n",
      "batch: 042 loss_train: 0.2325 acc_train: 0.9361 time: 5.4777s\n",
      "batch: 043 loss_train: 0.2235 acc_train: 0.9275 time: 5.7735s\n",
      "batch: 044 loss_train: 0.2809 acc_train: 0.9180 time: 6.1957s\n",
      "batch: 045 loss_train: 0.2661 acc_train: 0.9086 time: 6.6029s\n",
      "batch: 046 loss_train: 0.2793 acc_train: 0.9312 time: 5.6629s\n",
      "batch: 047 loss_train: 0.2329 acc_train: 0.9299 time: 5.8631s\n",
      "batch: 048 loss_train: 0.2889 acc_train: 0.8982 time: 6.1260s\n",
      "batch: 049 loss_train: 0.2563 acc_train: 0.9186 time: 6.1489s\n",
      "batch: 050 loss_train: 0.2378 acc_train: 0.9337 time: 5.8103s\n",
      "batch: 051 loss_train: 0.2955 acc_train: 0.9114 time: 5.5584s\n",
      "batch: 052 loss_train: 0.3029 acc_train: 0.9208 time: 5.7107s\n",
      "batch: 053 loss_train: 0.2541 acc_train: 0.9048 time: 6.2813s\n",
      "batch: 054 loss_train: 0.3866 acc_train: 0.8993 time: 5.9139s\n",
      "batch: 055 loss_train: 0.2297 acc_train: 0.9344 time: 5.9308s\n",
      "batch: 056 loss_train: 0.3774 acc_train: 0.8951 time: 6.0105s\n",
      "batch: 057 loss_train: 0.2730 acc_train: 0.9173 time: 6.0204s\n",
      "batch: 058 loss_train: 0.3029 acc_train: 0.9007 time: 6.4446s\n",
      "batch: 059 loss_train: 0.2576 acc_train: 0.9320 time: 6.1429s\n",
      "batch: 060 loss_train: 0.2208 acc_train: 0.9111 time: 6.0951s\n",
      "batch: 061 loss_train: 0.2749 acc_train: 0.9064 time: 6.3829s\n",
      "batch: 062 loss_train: 0.2676 acc_train: 0.9138 time: 6.3699s\n",
      "batch: 063 loss_train: 0.2718 acc_train: 0.9268 time: 6.4705s\n",
      "batch: 064 loss_train: 0.3676 acc_train: 0.9073 time: 5.3483s\n",
      "batch: 065 loss_train: 0.2690 acc_train: 0.8973 time: 5.8352s\n",
      "batch: 066 loss_train: 0.3602 acc_train: 0.9090 time: 5.5295s\n",
      "batch: 067 loss_train: 0.2699 acc_train: 0.9184 time: 5.5156s\n",
      "batch: 068 loss_train: 0.3025 acc_train: 0.9018 time: 5.4877s\n",
      "batch: 069 loss_train: 0.2528 acc_train: 0.9284 time: 5.9517s\n",
      "batch: 070 loss_train: 0.2785 acc_train: 0.9148 time: 5.7356s\n",
      "batch: 071 loss_train: 0.2625 acc_train: 0.9299 time: 6.0085s\n",
      "batch: 072 loss_train: 0.3062 acc_train: 0.9321 time: 5.7595s\n",
      "batch: 073 loss_train: 0.2639 acc_train: 0.9209 time: 6.1628s\n",
      "batch: 074 loss_train: 0.3198 acc_train: 0.8982 time: 5.5434s\n",
      "batch: 075 loss_train: 0.2596 acc_train: 0.9074 time: 5.7844s\n",
      "batch: 076 loss_train: 0.2746 acc_train: 0.9233 time: 5.6659s\n",
      "batch: 077 loss_train: 0.3100 acc_train: 0.8970 time: 5.6341s\n",
      "batch: 078 loss_train: 0.2274 acc_train: 0.9363 time: 5.9736s\n",
      "batch: 079 loss_train: 0.3262 acc_train: 0.9114 time: 5.9457s\n",
      "batch: 080 loss_train: 0.3569 acc_train: 0.9076 time: 6.4665s\n",
      "batch: 081 loss_train: 0.2654 acc_train: 0.9148 time: 6.0891s\n",
      "batch: 082 loss_train: 0.1920 acc_train: 0.9335 time: 6.0961s\n",
      "batch: 083 loss_train: 0.2941 acc_train: 0.9221 time: 6.1309s\n",
      "batch: 084 loss_train: 0.2786 acc_train: 0.9275 time: 5.7924s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 085 loss_train: 0.2171 acc_train: 0.9337 time: 6.6189s\n",
      "batch: 086 loss_train: 0.3174 acc_train: 0.9282 time: 5.8591s\n",
      "batch: 087 loss_train: 0.2344 acc_train: 0.9292 time: 5.8511s\n",
      "batch: 088 loss_train: 0.2833 acc_train: 0.9317 time: 6.1280s\n",
      "batch: 089 loss_train: 0.2383 acc_train: 0.9096 time: 5.5225s\n",
      "batch: 090 loss_train: 0.2520 acc_train: 0.9349 time: 5.9866s\n",
      "batch: 091 loss_train: 0.3129 acc_train: 0.9255 time: 5.7137s\n",
      "batch: 092 loss_train: 0.2616 acc_train: 0.9455 time: 5.3244s\n",
      "batch: 093 loss_train: 0.2826 acc_train: 0.9142 time: 5.9617s\n",
      "batch: 094 loss_train: 0.2761 acc_train: 0.9119 time: 5.9329s\n",
      "batch: 095 loss_train: 0.2641 acc_train: 0.9136 time: 5.6370s\n",
      "batch: 096 loss_train: 0.2671 acc_train: 0.9144 time: 5.7954s\n",
      "batch: 097 loss_train: 0.3345 acc_train: 0.8839 time: 5.8999s\n",
      "batch: 098 loss_train: 0.3346 acc_train: 0.8979 time: 5.5544s\n",
      "batch: 099 loss_train: 0.2561 acc_train: 0.9334 time: 5.6918s\n",
      "batch: 100 loss_train: 0.2716 acc_train: 0.9127 time: 5.5066s\n",
      "batch: 101 loss_train: 0.2108 acc_train: 0.9354 time: 5.7137s\n",
      "batch: 102 loss_train: 0.4160 acc_train: 0.9305 time: 5.5713s\n",
      "batch: 103 loss_train: 0.2964 acc_train: 0.9014 time: 5.6092s\n",
      "batch: 104 loss_train: 0.3229 acc_train: 0.9089 time: 5.7436s\n",
      "batch: 105 loss_train: 0.2799 acc_train: 0.8905 time: 5.8820s\n",
      "batch: 106 loss_train: 0.2697 acc_train: 0.8995 time: 5.6171s\n",
      "batch: 107 loss_train: 0.2842 acc_train: 0.9220 time: 5.9009s\n",
      "batch: 108 loss_train: 0.2404 acc_train: 0.9258 time: 5.5375s\n",
      "batch: 109 loss_train: 54.1961 acc_train: 0.9277 time: 5.7217s\n",
      "batch: 110 loss_train: 0.3733 acc_train: 0.8990 time: 5.7924s\n",
      "batch: 111 loss_train: 0.7800 acc_train: 0.8797 time: 6.0324s\n",
      "batch: 112 loss_train: 1.0302 acc_train: 0.7429 time: 7.2014s\n",
      "batch: 113 loss_train: 1.5683 acc_train: 0.6388 time: 6.1280s\n",
      "batch: 114 loss_train: 1.5669 acc_train: 0.7161 time: 6.5601s\n",
      "batch: 115 loss_train: 1.5397 acc_train: 0.6595 time: 5.8014s\n",
      "batch: 116 loss_train: 1.3796 acc_train: 0.7635 time: 6.3789s\n",
      "batch: 117 loss_train: 0.9898 acc_train: 0.7817 time: 6.1130s\n",
      "batch: 118 loss_train: 0.7862 acc_train: 0.8272 time: 5.1899s\n",
      "batch: 119 loss_train: 0.7992 acc_train: 0.8043 time: 6.0642s\n",
      "batch: 120 loss_train: 0.6557 acc_train: 0.8048 time: 6.0603s\n",
      "batch: 121 loss_train: 0.6670 acc_train: 0.8188 time: 5.6305s\n",
      "batch: 122 loss_train: 0.5926 acc_train: 0.8254 time: 5.1621s\n",
      "batch: 123 loss_train: 0.6462 acc_train: 0.8365 time: 5.4966s\n",
      "batch: 124 loss_train: 0.5728 acc_train: 0.8449 time: 5.7157s\n",
      "batch: 125 loss_train: 0.5585 acc_train: 0.8423 time: 4.8086s\n",
      "batch: 126 loss_train: 0.5604 acc_train: 0.8233 time: 5.6022s\n",
      "batch: 127 loss_train: 0.5036 acc_train: 0.8356 time: 5.7117s\n",
      "batch: 128 loss_train: 0.4377 acc_train: 0.8584 time: 5.4230s\n",
      "batch: 129 loss_train: 0.4082 acc_train: 0.8578 time: 5.5873s\n",
      "batch: 130 loss_train: 0.4123 acc_train: 0.9004 time: 5.3523s\n",
      "batch: 131 loss_train: 0.3379 acc_train: 0.8984 time: 5.5663s\n",
      "batch: 132 loss_train: 0.3564 acc_train: 0.9237 time: 5.2865s\n",
      "batch: 133 loss_train: 0.4462 acc_train: 0.8852 time: 5.6400s\n",
      "batch: 134 loss_train: 0.3702 acc_train: 0.8961 time: 5.4309s\n",
      "batch: 135 loss_train: 0.3588 acc_train: 0.9054 time: 5.4160s\n",
      "batch: 136 loss_train: 0.2706 acc_train: 0.9294 time: 5.6649s\n",
      "batch: 137 loss_train: 0.2028 acc_train: 0.9505 time: 5.5415s\n",
      "batch: 138 loss_train: 0.2993 acc_train: 0.9122 time: 5.4917s\n",
      "batch: 139 loss_train: 0.3493 acc_train: 0.9015 time: 5.7167s\n",
      "batch: 140 loss_train: 0.3880 acc_train: 0.8855 time: 5.6171s\n",
      "batch: 141 loss_train: 0.3491 acc_train: 0.8913 time: 5.5096s\n",
      "batch: 142 loss_train: 0.4410 acc_train: 0.8737 time: 5.9836s\n",
      "batch: 143 loss_train: 0.3619 acc_train: 0.8869 time: 5.7526s\n",
      "batch: 144 loss_train: 0.3146 acc_train: 0.8960 time: 5.6012s\n",
      "batch: 145 loss_train: 0.2676 acc_train: 0.9312 time: 5.1720s\n",
      "batch: 146 loss_train: 0.2967 acc_train: 0.8973 time: 5.6500s\n",
      "batch: 147 loss_train: 0.2608 acc_train: 0.9191 time: 5.8810s\n",
      "batch: 148 loss_train: 0.3779 acc_train: 0.8958 time: 5.2447s\n",
      "batch: 149 loss_train: 0.2899 acc_train: 0.9189 time: 5.3204s\n",
      "batch: 150 loss_train: 0.2346 acc_train: 0.9342 time: 5.2298s\n",
      "batch: 151 loss_train: 0.2549 acc_train: 0.9125 time: 5.4767s\n",
      "batch: 152 loss_train: 0.3074 acc_train: 0.9118 time: 5.4528s\n",
      "batch: 153 loss_train: 0.2848 acc_train: 0.8884 time: 5.3483s\n",
      "batch: 154 loss_train: 0.3441 acc_train: 0.9001 time: 5.8900s\n",
      "batch: 155 loss_train: 0.2775 acc_train: 0.9115 time: 5.5554s\n",
      "batch: 156 loss_train: 0.2429 acc_train: 0.9364 time: 5.7735s\n",
      "batch: 157 loss_train: 0.2542 acc_train: 0.9233 time: 5.4797s\n",
      "batch: 158 loss_train: 0.2398 acc_train: 0.9175 time: 5.5644s\n",
      "batch: 159 loss_train: 0.3233 acc_train: 0.8974 time: 5.5245s\n",
      "batch: 160 loss_train: 0.3061 acc_train: 0.8881 time: 5.6161s\n",
      "batch: 161 loss_train: 0.3626 acc_train: 0.9188 time: 5.7814s\n",
      "batch: 162 loss_train: 0.3121 acc_train: 0.9150 time: 5.6271s\n",
      "batch: 163 loss_train: 0.3504 acc_train: 0.9092 time: 5.6510s\n",
      "batch: 164 loss_train: 0.2744 acc_train: 0.9107 time: 5.6171s\n",
      "batch: 165 loss_train: 0.3066 acc_train: 0.9089 time: 5.5952s\n",
      "batch: 166 loss_train: 0.2961 acc_train: 0.9207 time: 5.5733s\n",
      "batch: 167 loss_train: 0.2378 acc_train: 0.9262 time: 5.8661s\n",
      "batch: 168 loss_train: 0.2767 acc_train: 0.9285 time: 5.8013s\n",
      "batch: 169 loss_train: 0.2202 acc_train: 0.9427 time: 6.0224s\n",
      "batch: 170 loss_train: 0.3043 acc_train: 0.9042 time: 5.7476s\n",
      "batch: 171 loss_train: 0.2633 acc_train: 0.9354 time: 5.9198s\n",
      "batch: 172 loss_train: 0.2782 acc_train: 0.9160 time: 5.7595s\n",
      "batch: 173 loss_train: 0.2394 acc_train: 0.9199 time: 5.7585s\n",
      "batch: 174 loss_train: 0.2281 acc_train: 0.9301 time: 5.5355s\n",
      "batch: 175 loss_train: 0.3071 acc_train: 0.8967 time: 6.1329s\n",
      "batch: 176 loss_train: 0.2973 acc_train: 0.9143 time: 5.7705s\n",
      "batch: 177 loss_train: 0.2656 acc_train: 0.9164 time: 6.0772s\n",
      "batch: 178 loss_train: 0.2869 acc_train: 0.9196 time: 5.5116s\n",
      "batch: 179 loss_train: 0.3023 acc_train: 0.9166 time: 5.6002s\n",
      "batch: 180 loss_train: 0.2932 acc_train: 0.9137 time: 6.2783s\n",
      "epoch:  2\n",
      "batch: 001 loss_train: 0.2619 acc_train: 0.9238 time: 5.9169s\n",
      "batch: 002 loss_train: 0.2157 acc_train: 0.9333 time: 5.9557s\n",
      "batch: 003 loss_train: 0.2353 acc_train: 0.9288 time: 5.4867s\n",
      "batch: 004 loss_train: 0.3098 acc_train: 0.9128 time: 6.1628s\n",
      "batch: 005 loss_train: 0.2487 acc_train: 0.9229 time: 6.0354s\n",
      "batch: 006 loss_train: 0.2989 acc_train: 0.9124 time: 5.9726s\n",
      "batch: 007 loss_train: 0.2904 acc_train: 0.9001 time: 5.6839s\n",
      "batch: 008 loss_train: 0.4142 acc_train: 0.8869 time: 5.6151s\n",
      "batch: 009 loss_train: 0.3824 acc_train: 0.8935 time: 5.5205s\n",
      "batch: 010 loss_train: 0.3066 acc_train: 0.8887 time: 5.4727s\n",
      "batch: 011 loss_train: 0.3013 acc_train: 0.8955 time: 5.6819s\n",
      "batch: 012 loss_train: 0.1896 acc_train: 0.9488 time: 5.8880s\n",
      "batch: 013 loss_train: 0.2219 acc_train: 0.9301 time: 5.4040s\n",
      "batch: 014 loss_train: 0.2200 acc_train: 0.9246 time: 5.9089s\n",
      "batch: 015 loss_train: 0.2922 acc_train: 0.9372 time: 5.9736s\n",
      "batch: 016 loss_train: 0.3117 acc_train: 0.8932 time: 5.6928s\n",
      "batch: 017 loss_train: 0.3107 acc_train: 0.8954 time: 5.9507s\n",
      "batch: 018 loss_train: 0.2555 acc_train: 0.9179 time: 5.7934s\n",
      "batch: 019 loss_train: 0.2451 acc_train: 0.9250 time: 5.6440s\n",
      "batch: 020 loss_train: 0.2497 acc_train: 0.9314 time: 5.4927s\n",
      "batch: 021 loss_train: 0.2681 acc_train: 0.9046 time: 6.2056s\n",
      "batch: 022 loss_train: 0.2404 acc_train: 0.9265 time: 5.4249s\n",
      "batch: 023 loss_train: 0.2632 acc_train: 0.9117 time: 5.5026s\n",
      "batch: 024 loss_train: 0.3180 acc_train: 0.9166 time: 5.9766s\n",
      "batch: 025 loss_train: 0.3244 acc_train: 0.9037 time: 5.5733s\n",
      "batch: 026 loss_train: 0.2971 acc_train: 0.8955 time: 5.8531s\n",
      "batch: 027 loss_train: 0.2633 acc_train: 0.9166 time: 5.8153s\n",
      "batch: 028 loss_train: 0.3083 acc_train: 0.9038 time: 5.3065s\n",
      "batch: 029 loss_train: 0.2914 acc_train: 0.9073 time: 5.7466s\n",
      "batch: 030 loss_train: 0.3448 acc_train: 0.8993 time: 5.9965s\n",
      "batch: 031 loss_train: 0.2940 acc_train: 0.9150 time: 6.0244s\n",
      "batch: 032 loss_train: 0.2960 acc_train: 0.9102 time: 6.0881s\n",
      "batch: 033 loss_train: 0.1888 acc_train: 0.9412 time: 5.8342s\n",
      "batch: 034 loss_train: 0.2209 acc_train: 0.9429 time: 5.9278s\n",
      "batch: 035 loss_train: 0.2735 acc_train: 0.9084 time: 6.3430s\n",
      "batch: 036 loss_train: 0.2335 acc_train: 0.9289 time: 5.6032s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 037 loss_train: 0.2831 acc_train: 0.9212 time: 5.5046s\n",
      "batch: 038 loss_train: 0.2493 acc_train: 0.9311 time: 6.2255s\n",
      "batch: 039 loss_train: 0.2646 acc_train: 0.9125 time: 5.6619s\n",
      "batch: 040 loss_train: 0.2423 acc_train: 0.9250 time: 6.0802s\n",
      "batch: 041 loss_train: 0.3434 acc_train: 0.8806 time: 5.7605s\n",
      "batch: 042 loss_train: 0.3273 acc_train: 0.9074 time: 5.7217s\n",
      "batch: 043 loss_train: 0.2725 acc_train: 0.9135 time: 5.5893s\n",
      "batch: 044 loss_train: 0.1934 acc_train: 0.9417 time: 5.6500s\n",
      "batch: 045 loss_train: 0.3050 acc_train: 0.9214 time: 5.6679s\n",
      "batch: 046 loss_train: 0.2484 acc_train: 0.9068 time: 6.0154s\n",
      "batch: 047 loss_train: 0.2392 acc_train: 0.9126 time: 5.8482s\n",
      "batch: 048 loss_train: 0.2309 acc_train: 0.9280 time: 5.7834s\n",
      "batch: 049 loss_train: 0.3672 acc_train: 0.8861 time: 6.0981s\n",
      "batch: 050 loss_train: 0.2207 acc_train: 0.9174 time: 5.7157s\n",
      "batch: 051 loss_train: 0.3136 acc_train: 0.9064 time: 5.7287s\n",
      "batch: 052 loss_train: 0.2660 acc_train: 0.9057 time: 5.9846s\n",
      "batch: 053 loss_train: 0.2775 acc_train: 0.9205 time: 6.3460s\n",
      "batch: 054 loss_train: 0.2449 acc_train: 0.9358 time: 5.8024s\n",
      "batch: 055 loss_train: 0.2361 acc_train: 0.9307 time: 5.5335s\n",
      "batch: 056 loss_train: 0.2681 acc_train: 0.9055 time: 5.4409s\n",
      "batch: 057 loss_train: 0.2808 acc_train: 0.8965 time: 5.7894s\n",
      "batch: 058 loss_train: 0.2300 acc_train: 0.9116 time: 5.9176s\n",
      "batch: 059 loss_train: 0.2213 acc_train: 0.9137 time: 5.9856s\n",
      "batch: 060 loss_train: 0.2695 acc_train: 0.9131 time: 5.7177s\n",
      "batch: 061 loss_train: 0.2337 acc_train: 0.9004 time: 5.5982s\n",
      "batch: 062 loss_train: 0.2528 acc_train: 0.9340 time: 5.6739s\n",
      "batch: 063 loss_train: 0.2132 acc_train: 0.9175 time: 5.9895s\n",
      "batch: 064 loss_train: 0.3802 acc_train: 0.8805 time: 5.7934s\n",
      "batch: 065 loss_train: 0.2502 acc_train: 0.9319 time: 5.1631s\n",
      "batch: 066 loss_train: 0.2094 acc_train: 0.9253 time: 6.0911s\n",
      "batch: 067 loss_train: 0.2752 acc_train: 0.9037 time: 5.5325s\n",
      "batch: 068 loss_train: 0.2502 acc_train: 0.9223 time: 5.5365s\n",
      "batch: 069 loss_train: 0.3228 acc_train: 0.8998 time: 5.7117s\n",
      "batch: 070 loss_train: 0.2166 acc_train: 0.9292 time: 5.4598s\n",
      "batch: 071 loss_train: 0.2029 acc_train: 0.9399 time: 6.3231s\n",
      "batch: 072 loss_train: 0.2426 acc_train: 0.9226 time: 5.5265s\n",
      "batch: 073 loss_train: 0.2979 acc_train: 0.9155 time: 6.1877s\n",
      "batch: 074 loss_train: 0.2452 acc_train: 0.9187 time: 5.2377s\n",
      "batch: 075 loss_train: 0.2843 acc_train: 0.9151 time: 5.1272s\n",
      "batch: 076 loss_train: 0.2441 acc_train: 0.9107 time: 5.8661s\n",
      "batch: 077 loss_train: 0.3096 acc_train: 0.8931 time: 5.5355s\n",
      "batch: 078 loss_train: 0.3364 acc_train: 0.8909 time: 5.9647s\n",
      "batch: 079 loss_train: 0.3275 acc_train: 0.8804 time: 6.0035s\n",
      "batch: 080 loss_train: 0.2690 acc_train: 0.9228 time: 6.3719s\n",
      "batch: 081 loss_train: 0.2432 acc_train: 0.9245 time: 5.5783s\n",
      "batch: 082 loss_train: 0.2545 acc_train: 0.9454 time: 5.9915s\n",
      "batch: 083 loss_train: 0.2992 acc_train: 0.9017 time: 5.7336s\n",
      "batch: 084 loss_train: 0.2794 acc_train: 0.9173 time: 5.7287s\n",
      "batch: 085 loss_train: 0.2014 acc_train: 0.9420 time: 5.8850s\n",
      "batch: 086 loss_train: 0.2784 acc_train: 0.9108 time: 5.7008s\n",
      "batch: 087 loss_train: 0.2874 acc_train: 0.8968 time: 5.1073s\n",
      "batch: 088 loss_train: 0.2625 acc_train: 0.9138 time: 5.5564s\n",
      "batch: 089 loss_train: 0.2577 acc_train: 0.9146 time: 5.8402s\n",
      "batch: 090 loss_train: 0.2721 acc_train: 0.9000 time: 5.6390s\n",
      "batch: 091 loss_train: 0.2629 acc_train: 0.9211 time: 5.8233s\n",
      "batch: 092 loss_train: 0.3372 acc_train: 0.9060 time: 5.4230s\n",
      "batch: 093 loss_train: 0.2386 acc_train: 0.9316 time: 6.2066s\n",
      "batch: 094 loss_train: 0.2056 acc_train: 0.9364 time: 5.5464s\n",
      "batch: 095 loss_train: 0.3138 acc_train: 0.9071 time: 5.9198s\n",
      "batch: 096 loss_train: 0.2401 acc_train: 0.9229 time: 5.8840s\n",
      "batch: 097 loss_train: 0.3261 acc_train: 0.8826 time: 6.6199s\n",
      "batch: 098 loss_train: 0.3271 acc_train: 0.9014 time: 5.3104s\n",
      "batch: 099 loss_train: 0.2227 acc_train: 0.9334 time: 6.2674s\n",
      "batch: 100 loss_train: 0.2466 acc_train: 0.9198 time: 6.5542s\n",
      "batch: 101 loss_train: 0.2278 acc_train: 0.9245 time: 6.0085s\n",
      "batch: 102 loss_train: 0.3109 acc_train: 0.9075 time: 6.0344s\n",
      "batch: 103 loss_train: 0.2763 acc_train: 0.9134 time: 6.1280s\n",
      "batch: 104 loss_train: 0.2649 acc_train: 0.9044 time: 5.9348s\n",
      "batch: 105 loss_train: 0.1735 acc_train: 0.9396 time: 5.6151s\n",
      "batch: 106 loss_train: 0.2371 acc_train: 0.9184 time: 6.2226s\n",
      "batch: 107 loss_train: 0.2826 acc_train: 0.9128 time: 5.7197s\n",
      "batch: 108 loss_train: 0.3152 acc_train: 0.8936 time: 5.9228s\n",
      "batch: 109 loss_train: 0.2560 acc_train: 0.8989 time: 5.8043s\n",
      "batch: 110 loss_train: 0.2819 acc_train: 0.8964 time: 6.0841s\n",
      "batch: 111 loss_train: 0.2147 acc_train: 0.9129 time: 6.0622s\n",
      "batch: 112 loss_train: 0.2182 acc_train: 0.9207 time: 5.9029s\n",
      "batch: 113 loss_train: 0.3310 acc_train: 0.9038 time: 5.9308s\n",
      "batch: 114 loss_train: 0.2096 acc_train: 0.9390 time: 5.6739s\n",
      "batch: 115 loss_train: 0.2266 acc_train: 0.9261 time: 6.0622s\n",
      "batch: 116 loss_train: 0.2395 acc_train: 0.9195 time: 6.4068s\n",
      "batch: 117 loss_train: 0.2035 acc_train: 0.9358 time: 5.7705s\n",
      "batch: 118 loss_train: 0.2496 acc_train: 0.9225 time: 5.7486s\n",
      "batch: 119 loss_train: 0.2795 acc_train: 0.9010 time: 5.8103s\n",
      "batch: 120 loss_train: 0.2954 acc_train: 0.9117 time: 6.4496s\n",
      "batch: 121 loss_train: 0.3113 acc_train: 0.9017 time: 5.5405s\n",
      "batch: 122 loss_train: 0.3006 acc_train: 0.9179 time: 6.3440s\n",
      "batch: 123 loss_train: 0.2716 acc_train: 0.9031 time: 5.7745s\n",
      "batch: 124 loss_train: 0.2183 acc_train: 0.9344 time: 6.0493s\n",
      "batch: 125 loss_train: 0.2803 acc_train: 0.9156 time: 6.3401s\n",
      "batch: 126 loss_train: 0.2377 acc_train: 0.9250 time: 6.1240s\n",
      "batch: 127 loss_train: 0.2605 acc_train: 0.9106 time: 5.3732s\n",
      "batch: 128 loss_train: 0.2466 acc_train: 0.9334 time: 5.5474s\n",
      "batch: 129 loss_train: 0.2214 acc_train: 0.9279 time: 5.9517s\n",
      "batch: 130 loss_train: 0.2148 acc_train: 0.9330 time: 5.3582s\n",
      "batch: 131 loss_train: 0.2503 acc_train: 0.9089 time: 5.6221s\n",
      "batch: 132 loss_train: 0.2904 acc_train: 0.9060 time: 6.1190s\n",
      "batch: 133 loss_train: 0.2672 acc_train: 0.9216 time: 6.2783s\n",
      "batch: 134 loss_train: 1.0882 acc_train: 0.9172 time: 5.8452s\n",
      "batch: 135 loss_train: 0.2473 acc_train: 0.9085 time: 5.6112s\n",
      "batch: 136 loss_train: 0.4123 acc_train: 0.8963 time: 5.5076s\n",
      "batch: 137 loss_train: 0.2411 acc_train: 0.9195 time: 5.7715s\n",
      "batch: 138 loss_train: 0.2313 acc_train: 0.9209 time: 6.3789s\n",
      "batch: 139 loss_train: 0.2233 acc_train: 0.9371 time: 6.1877s\n",
      "batch: 140 loss_train: 0.2709 acc_train: 0.9039 time: 5.6868s\n",
      "batch: 141 loss_train: 14.3379 acc_train: 0.8979 time: 6.4964s\n",
      "batch: 142 loss_train: 0.2720 acc_train: 0.9246 time: 5.9218s\n",
      "batch: 143 loss_train: 0.3760 acc_train: 0.8717 time: 6.1459s\n",
      "batch: 144 loss_train: 0.5232 acc_train: 0.8654 time: 6.1558s\n",
      "batch: 145 loss_train: 0.4781 acc_train: 0.8657 time: 6.2723s\n",
      "batch: 146 loss_train: 0.4613 acc_train: 0.8683 time: 5.8730s\n",
      "batch: 147 loss_train: 0.5186 acc_train: 0.8761 time: 5.7625s\n",
      "batch: 148 loss_train: 0.5057 acc_train: 0.8812 time: 6.1021s\n",
      "batch: 149 loss_train: 0.5422 acc_train: 0.8869 time: 5.9637s\n",
      "batch: 150 loss_train: 0.4309 acc_train: 0.8970 time: 6.1449s\n",
      "batch: 151 loss_train: 0.4136 acc_train: 0.8909 time: 5.5245s\n",
      "batch: 152 loss_train: 0.9126 acc_train: 0.9036 time: 5.6998s\n",
      "batch: 153 loss_train: 0.3118 acc_train: 0.9010 time: 5.4349s\n",
      "batch: 154 loss_train: 0.2965 acc_train: 0.8880 time: 5.3373s\n",
      "batch: 155 loss_train: 0.6359 acc_train: 0.9356 time: 6.0324s\n",
      "batch: 156 loss_train: 0.5364 acc_train: 0.8291 time: 6.2823s\n",
      "batch: 157 loss_train: 0.7527 acc_train: 0.8944 time: 5.0834s\n",
      "batch: 158 loss_train: 0.3025 acc_train: 0.8854 time: 5.4210s\n",
      "batch: 159 loss_train: 0.3794 acc_train: 0.8972 time: 5.0884s\n",
      "batch: 160 loss_train: 0.4270 acc_train: 0.8948 time: 5.7645s\n",
      "batch: 161 loss_train: 0.3269 acc_train: 0.8908 time: 5.2836s\n",
      "batch: 162 loss_train: 0.3668 acc_train: 0.9082 time: 5.8740s\n",
      "batch: 163 loss_train: 0.3276 acc_train: 0.9108 time: 6.0881s\n",
      "batch: 164 loss_train: 0.4190 acc_train: 0.8927 time: 6.2564s\n",
      "batch: 165 loss_train: 0.2595 acc_train: 0.9088 time: 5.7466s\n",
      "batch: 166 loss_train: 0.2858 acc_train: 0.9126 time: 5.6948s\n",
      "batch: 167 loss_train: 0.3529 acc_train: 0.8994 time: 5.7197s\n",
      "batch: 168 loss_train: 0.2426 acc_train: 0.9132 time: 5.7077s\n",
      "batch: 169 loss_train: 0.2840 acc_train: 0.9086 time: 6.0941s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 170 loss_train: 0.2482 acc_train: 0.9231 time: 6.0961s\n",
      "batch: 171 loss_train: 0.2036 acc_train: 0.9199 time: 5.9507s\n",
      "batch: 172 loss_train: 0.3164 acc_train: 0.9088 time: 5.6809s\n",
      "batch: 173 loss_train: 0.3485 acc_train: 0.8998 time: 5.9796s\n",
      "batch: 174 loss_train: 0.2192 acc_train: 0.9304 time: 6.7493s\n",
      "batch: 175 loss_train: 0.3677 acc_train: 0.9082 time: 6.7010s\n",
      "batch: 176 loss_train: 0.4205 acc_train: 0.8883 time: 6.0411s\n",
      "batch: 177 loss_train: 0.2960 acc_train: 0.9160 time: 7.2392s\n",
      "batch: 178 loss_train: 0.3818 acc_train: 0.8867 time: 6.0847s\n",
      "batch: 179 loss_train: 0.3023 acc_train: 0.9052 time: 6.3301s\n",
      "batch: 180 loss_train: 0.3046 acc_train: 0.9017 time: 6.4292s\n"
     ]
    }
   ],
   "source": [
    "data_name = 'CSE-CIC'\n",
    "path = \"datasets/\" + data_name + \"/\"\n",
    "num_class = data_class[data_name]\n",
    "binary = False\n",
    "# ------------------------------------------------------------------------\n",
    "edge_feat, label, adj, adj_lists = load_gat(path, binary)\n",
    "in_features = len(edge_feat[-1])\n",
    "num_edges = len(edge_feat)\n",
    "# 分割数据集\n",
    "alltrainidx = np.arange(num_edges)\n",
    "label2 = np.array(label.cpu())\n",
    "train_val, test = train_test_split(alltrainidx, test_size=test_size[data_name], stratify=label2)\n",
    "\n",
    "# 加载模型\n",
    "model = MyGNN(in_features, num_class).to(device)\n",
    "dataset = Dataset(edge_feat=edge_feat, adj=adj, adj_lists=adj_lists)\n",
    "# 开始训练\n",
    "gnn = GNNTrain(model=model,\n",
    "               train_idx=train_val, val_idx=test,\n",
    "               dataset=dataset, label=label,\n",
    "               epochs=3)\n",
    "gnn.train()\n",
    "# 测试\n",
    "# pred, label = gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85837110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [15:06<00:00,  6.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9597    0.9935    0.9763     52898\n",
      "           1     0.9891    0.9952    0.9921      5460\n",
      "           2     0.9451    0.9515    0.9483       742\n",
      "           3     0.9908    0.9988    0.9948      9798\n",
      "           4     0.8774    0.3357    0.4856       277\n",
      "           5     0.9489    0.9261    0.9374      4114\n",
      "           6     0.0000    0.0000    0.0000      1711\n",
      "\n",
      "    accuracy                         0.9651     75000\n",
      "   macro avg     0.8159    0.7430    0.7621     75000\n",
      "weighted avg     0.9430    0.9651    0.9534     75000\n",
      "\n",
      "[[52554    54    31    84    11   162     2]\n",
      " [   26  5434     0     0     0     0     0]\n",
      " [   33     3   706     0     0     0     0]\n",
      " [   10     1     1  9786     0     0     0]\n",
      " [  148     0     0     1    93    35     0]\n",
      " [  297     1     0     4     2  3810     0]\n",
      " [ 1691     1     9     2     0     8     0]]\n"
     ]
    }
   ],
   "source": [
    "pred, label = gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "169d62fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [14:19<00:00,  5.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9294    0.9807    0.9543     52898\n",
      "           1     0.9454    0.9989    0.9714      5460\n",
      "           2     0.9282    0.9232    0.9257       742\n",
      "           3     0.9525    0.9960    0.9738      9798\n",
      "           4     0.7025    0.4007    0.5103       277\n",
      "           5     0.8988    0.4925    0.6363      4114\n",
      "           6     0.0000    0.0000    0.0000      1711\n",
      "\n",
      "    accuracy                         0.9321     75000\n",
      "   macro avg     0.7653    0.6846    0.7103     75000\n",
      "weighted avg     0.9098    0.9321    0.9170     75000\n",
      "\n",
      "[[51875   270    40   451    47   197    18]\n",
      " [    5  5454     1     0     0     0     0]\n",
      " [   21    36   685     0     0     0     0]\n",
      " [   35     4     0  9759     0     0     0]\n",
      " [  163     2     1     0   111     0     0]\n",
      " [ 2085     0     0     3     0  2026     0]\n",
      " [ 1633     3    11    33     0    31     0]]\n"
     ]
    }
   ],
   "source": [
    "pred, label = gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7f138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lightgbm_catboost] *",
   "language": "python",
   "name": "conda-env-lightgbm_catboost-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
